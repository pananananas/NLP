{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification, pipeline\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import plotly.graph_objects as go\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl(input_file, tokenizer, sentiment_mapping):\n",
    "    def clean_word(word):\n",
    "        return word.strip()\n",
    "\n",
    "    def split_text_into_tokens(text):\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for item in data:\n",
    "        text = item['text']\n",
    "        labels = item.get('label', [])  # Use .get() to handle missing 'label' fields\n",
    "\n",
    "        tokens = split_text_into_tokens(text)\n",
    "        token_offsets = []\n",
    "        current_pos = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_offsets.append((start, end))\n",
    "            current_pos = end\n",
    "\n",
    "        token_labels = [\"O\"] * len(tokens)\n",
    "\n",
    "        for start, end, sentiment in labels:\n",
    "            sentiment_standard = sentiment_mapping.get(sentiment, \"O\")\n",
    "            if sentiment_standard == \"O\":\n",
    "                continue\n",
    "\n",
    "            for i, (token_start, token_end) in enumerate(token_offsets):\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    if token_start == start:\n",
    "                        token_labels[i] = f\"B-{sentiment_standard}\"\n",
    "                    else:\n",
    "                        token_labels[i] = f\"I-{sentiment_standard}\"\n",
    "\n",
    "        cleaned_tokens = [clean_word(token) for token in tokens]\n",
    "        cleaned_tokens, token_labels = zip(*[\n",
    "            (token, label) for token, label in zip(cleaned_tokens, token_labels) if token\n",
    "        ])\n",
    "\n",
    "        processed_data.append({\n",
    "            \"tokens\": list(cleaned_tokens),\n",
    "            \"labels\": list(token_labels)\n",
    "        })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sdadas/polish-gpt2-medium\", use_fast=True, add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "sentiment_mapping = {\n",
    "    'Negative': 'Negative',\n",
    "    'Neutral': 'Neutral',\n",
    "    'Positive': 'Positive'\n",
    "}\n",
    "processed_data = process_jsonl(\"patryk.jsonl\", tokenizer, sentiment_mapping)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example:\n",
      "{'tokens': ['Lakier', 'roweru', 'bardzo', 'kiepskiej', 'jakości', ',', 'robią', 'się', 'odpryski', 'nie', 'wiadomo', 'od', 'czego', 'rower', 'ładny', 'wygodny', 'ale', 'po', '3', 'miesiącach', 'użytkowania', 'widoczne', 'odpryski', 'lakieru', 'czego', 'za', 'taką', 'cenę', 'nie', 'powinno', 'być', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', 'Oczywiście', 'producent', 'twierdzi', 'że', 'są', 'to', 'wady', 'mechaniczne', ',', 'dziecko', 'ma', 'w', 'lepszym', 'stanie', 'lakier', 'na', 'rowerze', 'ale', 'nie', 'z', 'tej', 'firmy', 'ODRADZAM', 'ZAKUP', 'Z', 'TEGO', 'POWODU', 'SZKODA', 'TYLE', 'KASY', 'I', 'NERWÓW', 'chyba', 'ze', 'rower', 'będzie', 'stał', 'nieużywany', 'za', 'szybą', '.', 'Na', 'zakończenie', 'powiem', 'tak', 'porównując', 'lakier', 'zwykły', 'do', 'paznokci', 'a', 'hybrydę', 'wiadomo', 'w', 'tańszym', 'zwykłym', 'lakierze', 'robią', 'się', 'odpryski', 'a', 'lepszym', 'nie', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], 'labels': ['O', 'O', 'O', 'B-Negative', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'O', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "print(\"Dataset example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID Mapping:\n",
      "{'O': 0, 'B-Negative': 1, 'I-Negative': 2, 'B-Positive': 3, 'I-Positive': 4, 'B-Neutral': 5, 'I-Neutral': 6}\n",
      "\n",
      "ID to Label Mapping:\n",
      "{0: 'O', 1: 'B-Negative', 2: 'I-Negative', 3: 'B-Positive', 4: 'I-Positive', 5: 'B-Neutral', 6: 'I-Neutral'}\n"
     ]
    }
   ],
   "source": [
    "label_list = [\"O\", \"B-Negative\", \"I-Negative\", \"B-Positive\", \"I-Positive\", \"B-Neutral\", \"I-Neutral\"]\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Label to ID Mapping:\")\n",
    "print(label_to_id)\n",
    "\n",
    "print(\"\\nID to Label Mapping:\")\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e1985ad8114553bf4eec7bed7ebcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset Example:\n",
      "{'labels': [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0], 'input_ids': [573, 301, 523, 29345, 702, 11265, 775, 5059, 2402, 5166, 309, 672, 368, 452, 304, 3151, 357, 1386, 5557, 24161, 31670, 478, 291, 719, 10542, 15268, 13048, 672, 368, 452, 46987, 383, 1386, 313, 2594, 6908, 304, 3438, 739, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 13731, 7083, 15271, 6905, 337, 543, 339, 18822, 42797, 2402, 2846, 438, 264, 16163, 1619, 25509, 293, 22660, 478, 304, 268, 725, 3390, 18373, 4069, 3454, 6300, 556, 6197, 57, 52, 556, 14652, 8076, 44938, 9622, 57, 17763, 8337, 7248, 16778, 6266, 431, 15068, 61, 598, 656, 4745, 59, 11390, 1491, 498, 5557, 626, 2174, 34416, 3506, 313, 48395, 2077, 992, 12506, 5481, 395, 50721, 25509, 13320, 306, 36366], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [[0, 1], [1, 3], [3, 6], [0, 6], [0, 6], [0, 4], [4, 9], [0, 7], [0, 1], [0, 5], [0, 3], [0, 3], [3, 5], [5, 8], [0, 3], [0, 7], [0, 2], [0, 5], [0, 5], [0, 5], [0, 7], [0, 3], [0, 2], [0, 1], [0, 10], [0, 11], [0, 8], [0, 3], [3, 5], [5, 8], [0, 5], [5, 7], [0, 5], [0, 2], [0, 4], [0, 4], [0, 3], [0, 7], [0, 3], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 10], [0, 9], [0, 8], [0, 2], [0, 2], [0, 2], [0, 4], [0, 11], [0, 1], [0, 7], [0, 2], [0, 1], [0, 7], [0, 6], [0, 6], [0, 2], [0, 7], [0, 3], [0, 3], [0, 1], [0, 3], [0, 5], [0, 2], [2, 4], [4, 6], [6, 8], [0, 1], [1, 3], [3, 4], [4, 5], [0, 1], [0, 2], [2, 4], [0, 3], [3, 5], [5, 6], [0, 2], [2, 4], [4, 6], [0, 2], [2, 4], [0, 1], [1, 3], [3, 4], [0, 1], [0, 1], [1, 3], [3, 4], [4, 6], [0, 5], [0, 2], [0, 5], [0, 6], [0, 4], [0, 6], [6, 10], [0, 2], [0, 5], [0, 1], [0, 2], [0, 11], [0, 6], [0, 3], [0, 10], [0, 6], [0, 6], [0, 2], [0, 8]]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and aligns the labels with the tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): Dictionary containing 'tokens' and 'labels'.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized inputs with aligned labels.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id.get(label[word_idx], 0))\n",
    "            else:\n",
    "                if label[word_idx].startswith(\"B-\"):\n",
    "                    label_ids.append(label_to_id.get(label[word_idx].replace(\"B-\", \"I-\"), 0))\n",
    "                else:\n",
    "                    label_ids.append(label_to_id.get(label[word_idx], 0))\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=['tokens', 'labels']\n",
    ")\n",
    "\n",
    "print(\"Tokenized Dataset Example:\")\n",
    "print(tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training samples: 240\n",
      "Number of evaluation samples: 60\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']\n",
    "\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at sdadas/polish-gpt2-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForTokenClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (wpe): Embedding(2048, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"sdadas/polish-gpt2-medium\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "foundation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.h.12\n",
      "transformer.h.12.ln_1\n",
      "transformer.h.12.attn\n",
      "transformer.h.12.attn.c_attn\n",
      "transformer.h.12.attn.c_proj\n",
      "transformer.h.12.attn.attn_dropout\n",
      "transformer.h.12.attn.resid_dropout\n",
      "transformer.h.12.ln_2\n",
      "transformer.h.12.mlp\n",
      "transformer.h.12.mlp.c_fc\n",
      "transformer.h.12.mlp.c_proj\n",
      "transformer.h.12.mlp.act\n",
      "transformer.h.12.mlp.dropout\n",
      "transformer.h.13\n",
      "transformer.h.13.ln_1\n",
      "transformer.h.13.attn\n",
      "transformer.h.13.attn.c_attn\n",
      "transformer.h.13.attn.c_proj\n",
      "transformer.h.13.attn.attn_dropout\n",
      "transformer.h.13.attn.resid_dropout\n",
      "transformer.h.13.ln_2\n",
      "transformer.h.13.mlp\n",
      "transformer.h.13.mlp.c_fc\n",
      "transformer.h.13.mlp.c_proj\n",
      "transformer.h.13.mlp.act\n",
      "transformer.h.13.mlp.dropout\n",
      "transformer.h.14\n",
      "transformer.h.14.ln_1\n",
      "transformer.h.14.attn\n",
      "transformer.h.14.attn.c_attn\n",
      "transformer.h.14.attn.c_proj\n",
      "transformer.h.14.attn.attn_dropout\n",
      "transformer.h.14.attn.resid_dropout\n",
      "transformer.h.14.ln_2\n",
      "transformer.h.14.mlp\n",
      "transformer.h.14.mlp.c_fc\n",
      "transformer.h.14.mlp.c_proj\n",
      "transformer.h.14.mlp.act\n",
      "transformer.h.14.mlp.dropout\n",
      "transformer.h.15\n",
      "transformer.h.15.ln_1\n",
      "transformer.h.15.attn\n",
      "transformer.h.15.attn.c_attn\n",
      "transformer.h.15.attn.c_proj\n",
      "transformer.h.15.attn.attn_dropout\n",
      "transformer.h.15.attn.resid_dropout\n",
      "transformer.h.15.ln_2\n",
      "transformer.h.15.mlp\n",
      "transformer.h.15.mlp.c_fc\n",
      "transformer.h.15.mlp.c_proj\n",
      "transformer.h.15.mlp.act\n",
      "transformer.h.15.mlp.dropout\n",
      "transformer.h.16\n",
      "transformer.h.16.ln_1\n",
      "transformer.h.16.attn\n",
      "transformer.h.16.attn.c_attn\n",
      "transformer.h.16.attn.c_proj\n",
      "transformer.h.16.attn.attn_dropout\n",
      "transformer.h.16.attn.resid_dropout\n",
      "transformer.h.16.ln_2\n",
      "transformer.h.16.mlp\n",
      "transformer.h.16.mlp.c_fc\n",
      "transformer.h.16.mlp.c_proj\n",
      "transformer.h.16.mlp.act\n",
      "transformer.h.16.mlp.dropout\n",
      "transformer.h.17\n",
      "transformer.h.17.ln_1\n",
      "transformer.h.17.attn\n",
      "transformer.h.17.attn.c_attn\n",
      "transformer.h.17.attn.c_proj\n",
      "transformer.h.17.attn.attn_dropout\n",
      "transformer.h.17.attn.resid_dropout\n",
      "transformer.h.17.ln_2\n",
      "transformer.h.17.mlp\n",
      "transformer.h.17.mlp.c_fc\n",
      "transformer.h.17.mlp.c_proj\n",
      "transformer.h.17.mlp.act\n",
      "transformer.h.17.mlp.dropout\n",
      "transformer.h.18\n",
      "transformer.h.18.ln_1\n",
      "transformer.h.18.attn\n",
      "transformer.h.18.attn.c_attn\n",
      "transformer.h.18.attn.c_proj\n",
      "transformer.h.18.attn.attn_dropout\n",
      "transformer.h.18.attn.resid_dropout\n",
      "transformer.h.18.ln_2\n",
      "transformer.h.18.mlp\n",
      "transformer.h.18.mlp.c_fc\n",
      "transformer.h.18.mlp.c_proj\n",
      "transformer.h.18.mlp.act\n",
      "transformer.h.18.mlp.dropout\n",
      "transformer.h.19\n",
      "transformer.h.19.ln_1\n",
      "transformer.h.19.attn\n",
      "transformer.h.19.attn.c_attn\n",
      "transformer.h.19.attn.c_proj\n",
      "transformer.h.19.attn.attn_dropout\n",
      "transformer.h.19.attn.resid_dropout\n",
      "transformer.h.19.ln_2\n",
      "transformer.h.19.mlp\n",
      "transformer.h.19.mlp.c_fc\n",
      "transformer.h.19.mlp.c_proj\n",
      "transformer.h.19.mlp.act\n",
      "transformer.h.19.mlp.dropout\n",
      "transformer.h.20\n",
      "transformer.h.20.ln_1\n",
      "transformer.h.20.attn\n",
      "transformer.h.20.attn.c_attn\n",
      "transformer.h.20.attn.c_proj\n",
      "transformer.h.20.attn.attn_dropout\n",
      "transformer.h.20.attn.resid_dropout\n",
      "transformer.h.20.ln_2\n",
      "transformer.h.20.mlp\n",
      "transformer.h.20.mlp.c_fc\n",
      "transformer.h.20.mlp.c_proj\n",
      "transformer.h.20.mlp.act\n",
      "transformer.h.20.mlp.dropout\n",
      "transformer.h.21\n",
      "transformer.h.21.ln_1\n",
      "transformer.h.21.attn\n",
      "transformer.h.21.attn.c_attn\n",
      "transformer.h.21.attn.c_proj\n",
      "transformer.h.21.attn.attn_dropout\n",
      "transformer.h.21.attn.resid_dropout\n",
      "transformer.h.21.ln_2\n",
      "transformer.h.21.mlp\n",
      "transformer.h.21.mlp.c_fc\n",
      "transformer.h.21.mlp.c_proj\n",
      "transformer.h.21.mlp.act\n",
      "transformer.h.21.mlp.dropout\n",
      "transformer.h.22\n",
      "transformer.h.22.ln_1\n",
      "transformer.h.22.attn\n",
      "transformer.h.22.attn.c_attn\n",
      "transformer.h.22.attn.c_proj\n",
      "transformer.h.22.attn.attn_dropout\n",
      "transformer.h.22.attn.resid_dropout\n",
      "transformer.h.22.ln_2\n",
      "transformer.h.22.mlp\n",
      "transformer.h.22.mlp.c_fc\n",
      "transformer.h.22.mlp.c_proj\n",
      "transformer.h.22.mlp.act\n",
      "transformer.h.22.mlp.dropout\n",
      "transformer.h.23\n",
      "transformer.h.23.ln_1\n",
      "transformer.h.23.attn\n",
      "transformer.h.23.attn.c_attn\n",
      "transformer.h.23.attn.c_proj\n",
      "transformer.h.23.attn.attn_dropout\n",
      "transformer.h.23.attn.resid_dropout\n",
      "transformer.h.23.ln_2\n",
      "transformer.h.23.mlp\n",
      "transformer.h.23.mlp.c_fc\n",
      "transformer.h.23.mlp.c_proj\n",
      "transformer.h.23.mlp.act\n",
      "transformer.h.23.mlp.dropout\n",
      "transformer.ln_f\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in foundation_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<TaskType.SEQ_CLS: 'SEQ_CLS'>, <TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, <TaskType.TOKEN_CLS: 'TOKEN_CLS'>, <TaskType.QUESTION_ANS: 'QUESTION_ANS'>, <TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(TaskType))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,          # Correct task type for token-level tasks\n",
    "    r=64,                                  # Rank of LoRA; adjust as needed\n",
    "    lora_alpha=32,                         # Scaling factor; adjust as needed\n",
    "    lora_dropout=0.05,                     # Dropout probability\n",
    "    # target_modules=[\"classifier\"]           # Correct target module(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,298,631 || all params: 363,143,182 || trainable%: 1.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config id_to_label: {0: 'O', 1: 'B-Negative', 2: 'I-Negative', 3: 'B-Positive', 4: 'I-Positive', 5: 'B-Neutral', 6: 'I-Neutral'}\n",
      "Model config label2id: {'O': 0, 'B-Negative': 1, 'I-Negative': 2, 'B-Positive': 3, 'I-Positive': 4, 'B-Neutral': 5, 'I-Neutral': 6}\n"
     ]
    }
   ],
   "source": [
    "peft_model.config.id2label = id_to_label\n",
    "peft_model.config.label2id = label_to_id\n",
    "print(\"Model config id_to_label:\", peft_model.config.id2label)\n",
    "print(\"Model config label2id:\", peft_model.config.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForTokenClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForTokenClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(51200, 1024)\n",
       "        (wpe): Embedding(2048, 1024)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=3072, nx=1024)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "              (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "              (act): FastGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels_all = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions_all = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(pred_seq, label_seq) if label != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results_all = metric.compute(predictions=true_predictions_all, references=true_labels_all)\n",
    "\n",
    "    true_labels_without_O = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100 and id_to_label[label] != \"O\"]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions_without_O = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(pred_seq, label_seq) if label != -100 and id_to_label[label] != \"O\"]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results_without_O = metric.compute(predictions=true_predictions_without_O, references=true_labels_without_O)\n",
    "    \n",
    "    return {\n",
    "        \"precision_all\": results_all.get(\"overall_precision\", 0.0),\n",
    "        \"recall_all\": results_all.get(\"overall_recall\", 0.0),\n",
    "        \"f1_all\": results_all.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy_all\": results_all.get(\"overall_accuracy\", 0.0),\n",
    "        \"precision_without_O\": results_without_O.get(\"overall_precision\", 0.0),\n",
    "        \"recall_without_O\": results_without_O.get(\"overall_recall\", 0.0),\n",
    "        \"f1_without_O\": results_without_O.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy_without_O\": results_without_O.get(\"overall_accuracy\", 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_all\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d16adfd9a3f4fed828e2c43a7645eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1581, 'grad_norm': 16.687063217163086, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6b95ef743e40eca5ce30372ebd51c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87e1c710b6c42f18c53d760e9c8754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "{'eval_loss': 1.537061333656311, 'eval_precision_all': 0.013524936601859678, 'eval_recall_all': 0.125, 'eval_f1_all': 0.024408848207475208, 'eval_accuracy_all': 0.4380733944954128, 'eval_precision_without_O': 0.14414414414414414, 'eval_recall_without_O': 0.125, 'eval_f1_without_O': 0.13389121338912136, 'eval_accuracy_without_O': 0.11538461538461539, 'eval_runtime': 2.1998, 'eval_samples_per_second': 27.275, 'eval_steps_per_second': 1.818, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GemmaForTokenClassification', 'Gemma2ForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LlamaForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MistralForTokenClassification', 'MixtralForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NemotronForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PersimmonForTokenClassification', 'PhiForTokenClassification', 'Phi3ForTokenClassification', 'QDQBertForTokenClassification', 'Qwen2ForTokenClassification', 'Qwen2MoeForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'StableLmForTokenClassification', 'Starcoder2ForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Nie jestem zadowolony z zakupu. Słuchawki są niewygodne i głośność jest irytująca.\n",
      "Inference Results:\n",
      "[{'entity_group': 'Neutral', 'score': 0.30901104, 'word': '. Słu', 'start': 30, 'end': 35}, {'entity_group': 'Neutral', 'score': 0.27608386, 'word': 'wki', 'start': 38, 'end': 41}, {'entity_group': 'Neutral', 'score': 0.4730853, 'word': ' głoś', 'start': 57, 'end': 62}, {'entity_group': 'Neutral', 'score': 0.47635025, 'word': ' jest', 'start': 66, 'end': 71}]\n",
      "\n",
      "Text: Zaakceptowałem ofertę i kupiłem nowy telefon, który działa bez zarzutu.\n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.3555447, 'word': 'akcep', 'start': 2, 'end': 7}, {'entity_group': 'Negative', 'score': 0.39310208, 'word': ' kupiłem', 'start': 23, 'end': 31}, {'entity_group': 'Neutral', 'score': 0.37750885, 'word': ' nowy telefon', 'start': 31, 'end': 44}, {'entity_group': 'Neutral', 'score': 0.3029127, 'word': ' który', 'start': 45, 'end': 51}, {'entity_group': 'Neutral', 'score': 0.40529865, 'word': ' zarzutu', 'start': 62, 'end': 70}]\n",
      "\n",
      "Text: Pisanie opinii o produkcie było dla mnie bardzo łatwe i szybkie. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Positive', 'score': 0.2658695, 'word': ' produkcie', 'start': 16, 'end': 26}, {'entity_group': 'Positive', 'score': 0.33899868, 'word': ' szybkie', 'start': 55, 'end': 63}]\n",
      "\n",
      "Text: One są wszystkie, luzacki, nudne, wporzadku, groźny, mieszane, fajny, zły, nie dobry,  dobra, pozytywne, piękne, smutne. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Neutral', 'score': 0.6135285, 'word': ' lu', 'start': 17, 'end': 20}, {'entity_group': 'Negative', 'score': 0.26294255, 'word': ' wpo', 'start': 33, 'end': 37}, {'entity_group': 'Neutral', 'score': 0.2515309, 'word': 'rzad', 'start': 37, 'end': 41}, {'entity_group': 'Neutral', 'score': 0.27522328, 'word': ' mieszane', 'start': 52, 'end': 61}, {'entity_group': 'Negative', 'score': 0.24181698, 'word': ' fajny', 'start': 62, 'end': 68}, {'entity_group': 'Negative', 'score': 0.33951414, 'word': ' zły', 'start': 69, 'end': 73}, {'entity_group': 'Negative', 'score': 0.3028855, 'word': ' dobry', 'start': 78, 'end': 84}, {'entity_group': 'Neutral', 'score': 0.25786057, 'word': ' ', 'start': 85, 'end': 86}, {'entity_group': 'Neutral', 'score': 0.2727464, 'word': ' piękne', 'start': 104, 'end': 111}, {'entity_group': 'Negative', 'score': 0.28739095, 'word': ' smutne', 'start': 112, 'end': 119}]\n",
      "\n",
      "Text: Całe to jebane zycie to jeden wielki dramat. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.22945945, 'word': ' je', 'start': 7, 'end': 10}, {'entity_group': 'Positive', 'score': 0.3801619, 'word': ' to jeden wielki', 'start': 20, 'end': 36}, {'entity_group': 'Negative', 'score': 0.23553893, 'word': ' dramat', 'start': 36, 'end': 43}, {'entity_group': 'Neutral', 'score': 0.3236446, 'word': '.', 'start': 43, 'end': 44}]\n",
      "\n",
      "Text: Chuj kurwa chuj. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.25776857, 'word': 'uj', 'start': 2, 'end': 4}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "inference_results = []\n",
    "\n",
    "example_texts = [\n",
    "    \"Nie jestem zadowolony z zakupu. Słuchawki są niewygodne i głośność jest irytująca.\",\n",
    "    \"Zaakceptowałem ofertę i kupiłem nowy telefon, który działa bez zarzutu.\",\n",
    "    \"Pisanie opinii o produkcie było dla mnie bardzo łatwe i szybkie. \",\n",
    "    \"One są wszystkie, luzacki, nudne, wporzadku, groźny, mieszane, fajny, zły, nie dobry,  dobra, pozytywne, piękne, smutne. \",\n",
    "    \"Całe to jebane zycie to jeden wielki dramat. \",\n",
    "    \"Chuj kurwa chuj. \",\n",
    "]\n",
    "\n",
    "for text in example_texts:\n",
    "    predictions = nlp(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Inference Results:\")\n",
    "    print(predictions)  # Print the entire prediction dictionary\n",
    "    break  # Remove or adjust as needed\n",
    "\n",
    "\n",
    "for text in example_texts:\n",
    "    predictions = nlp(text)\n",
    "    inference_results.append({\n",
    "        \"text\": text,\n",
    "        \"predictions\": predictions\n",
    "    })\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Inference Results:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_colors = {\n",
    "    'Negative': 'red',\n",
    "    'Neutral': 'gray',\n",
    "    'Positive': 'green'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m word_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(words)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions:\n\u001b[0;32m---> 20\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m get_sentiment(label)\n\u001b[1;32m     22\u001b[0m     score \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entity'"
     ]
    }
   ],
   "source": [
    "def get_sentiment(label):\n",
    "    \"\"\"Extract the base sentiment from the label.\"\"\"\n",
    "    if label.startswith('B-') or label.startswith('I-'):\n",
    "        return label.split('-', 1)[1]\n",
    "    return label\n",
    "\n",
    "for result in inference_results:\n",
    "    text = result['text']\n",
    "    predictions = result['predictions']\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    sentiments = []\n",
    "    scores = []\n",
    "    \n",
    "    word_sentiments = ['O'] * len(words)\n",
    "    word_scores = [0.0] * len(words)\n",
    "    \n",
    "    for pred in predictions:\n",
    "        label = pred['entity']\n",
    "        sentiment = get_sentiment(label)\n",
    "        score = pred['score']\n",
    "        word = pred['word'].replace('</w>', '').strip()\n",
    "        \n",
    "        for idx, w in enumerate(words):\n",
    "            clean_w = re.sub(r'[^\\w]', '', w)\n",
    "            if word.lower() == clean_w.lower():\n",
    "                word_sentiments[idx] = sentiment\n",
    "                word_scores[idx] = score\n",
    "                break\n",
    "    \n",
    "    colors = [sentiment_colors.get(sentiment, 'black') for sentiment in word_sentiments]\n",
    "    \n",
    "    hover_texts = [f\"Sentiment: {sentiment}<br>Score: {score:.2f}\" \n",
    "                   for sentiment, score in zip(word_sentiments, word_scores)]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    spacing = 0.5  # Adjust spacing between words\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            text=[word],\n",
    "            mode='text',\n",
    "            textfont=dict(color=colors[i], size=16),\n",
    "            hoverinfo='text',\n",
    "            hovertext=hover_texts[i],\n",
    "            showlegend=False\n",
    "        ))\n",
    "        # Increment x position\n",
    "        x += len(word) * 0.1 + spacing\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Inference Results\",\n",
    "        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        margin=dict(l=20, r=20, t=50, b=20)\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # Try 'none' if 'simple' does not work\n",
    ")\n",
    "\n",
    "inference_results = []\n",
    "\n",
    "example_texts = [\n",
    "    \"Nie jestem zadowolony z zakupu. Słuchawki są niewygodne i głośność jest irytująca.\",\n",
    "    \"Zaakceptowałem ofertę i kupiłem nowy telefon, który działa bez zarzutu.\",\n",
    "    \"Pisanie opinii o produkcie było dla mnie bardzo łatwe i szybkie. \",\n",
    "    \"One są wszystkie, luzacki, nudne, wporzadku, groźny, mieszane, fajny, zły, nie dobry, dobra, pozytywne, piękne, smutne. \",\n",
    "    \"Całe to jebane zycie to jeden wielki dramat. \",\n",
    "    \"Chuj kurwa chuj. \",\n",
    "]\n",
    "\n",
    "for text in example_texts:\n",
    "    predictions = nlp(text)\n",
    "    inference_results.append({\n",
    "        \"text\": text,\n",
    "        \"predictions\": predictions\n",
    "    })\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Inference Results:\")\n",
    "    print(predictions)\n",
    "\n",
    "    # For debugging: print first prediction's keys\n",
    "    if predictions:\n",
    "        print(\"Keys in prediction:\", predictions[0].keys())\n",
    "\n",
    "    # Proceed to handle predictions\n",
    "    words = text.split()\n",
    "    \n",
    "    sentiments = []\n",
    "    scores = []\n",
    "    \n",
    "    word_sentiments = ['O'] * len(words)\n",
    "    word_scores = [0.0] * len(words)\n",
    "    \n",
    "    for pred in predictions:\n",
    "        # Use .get() to safely access keys\n",
    "        label = pred.get('entity') or pred.get('entity_group')\n",
    "        if label is None:\n",
    "            print(\"Missing 'entity' and 'entity_group' in prediction:\", pred)\n",
    "            continue\n",
    "        sentiment = get_sentiment(label)\n",
    "        score = pred.get('score', 0.0)\n",
    "        word = pred.get('word', '').replace('</w>', '').strip()\n",
    "        \n",
    "        for idx, w in enumerate(words):\n",
    "            clean_w = re.sub(r'[^\\w]', '', w)\n",
    "            if word.lower() == clean_w.lower():\n",
    "                word_sentiments[idx] = sentiment\n",
    "                word_scores[idx] = score\n",
    "                break\n",
    "    \n",
    "    colors = [sentiment_colors.get(sentiment, 'black') for sentiment in word_sentiments]\n",
    "    \n",
    "    hover_texts = [f\"Sentiment: {sentiment}<br>Score: {score:.2f}\" \n",
    "                   for sentiment, score in zip(word_sentiments, word_scores)]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    spacing = 0.5  # Adjust spacing between words\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            text=[word],\n",
    "            mode='text',\n",
    "            textfont=dict(color=colors[i], size=16),\n",
    "            hoverinfo='text',\n",
    "            hovertext=hover_texts[i],\n",
    "            showlegend=False\n",
    "        ))\n",
    "        # Increment x position\n",
    "        x += len(word) * 0.1 + spacing\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Inference Results\",\n",
    "        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        margin=dict(l=20, r=20, t=50, b=20)\n",
    "    )\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
