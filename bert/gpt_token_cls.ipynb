{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import plotly.graph_objects as go\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import evaluate\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl(input_file, tokenizer, sentiment_mapping):\n",
    "    \"\"\"\n",
    "    Processes a JSONL file and returns a list of dictionaries with tokens and labels.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input JSONL file.\n",
    "        tokenizer (AutoTokenizer): Hugging Face tokenizer.\n",
    "        sentiment_mapping (dict): Mapping from sentiment strings to standardized labels.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries with 'tokens' and 'labels'.\n",
    "    \"\"\"\n",
    "    def clean_word(word):\n",
    "        \"\"\"Remove unnecessary spaces from tokens.\"\"\"\n",
    "        # Strip only spaces, not punctuation\n",
    "        return word.strip()\n",
    "\n",
    "    def split_text_into_tokens(text):\n",
    "        \"\"\"\n",
    "        Splits text into tokens, treating punctuation as separate tokens.\n",
    "        Example: \"Hello, world!\" -> [\"Hello\", \",\", \"world\", \"!\"]\n",
    "        \"\"\"\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "\n",
    "    # Open and read the input JSONL file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    # Iterate through each review in the data\n",
    "    for item in data:\n",
    "        text = item['text']\n",
    "        labels = item.get('label', [])  # Use .get() to handle missing 'label' fields\n",
    "\n",
    "        # Tokenize text, splitting punctuation into separate tokens\n",
    "        tokens = split_text_into_tokens(text)\n",
    "        token_offsets = []\n",
    "        current_pos = 0\n",
    "\n",
    "        # Find character positions for each token\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_offsets.append((start, end))\n",
    "            current_pos = end\n",
    "\n",
    "        # Initialize labels for each token as \"O\"\n",
    "        token_labels = [\"O\"] * len(tokens)\n",
    "\n",
    "        # Assign labels based on sentiment spans\n",
    "        for start, end, sentiment in labels:\n",
    "            # Standardize sentiment label\n",
    "            sentiment_standard = sentiment_mapping.get(sentiment, \"O\")\n",
    "            if sentiment_standard == \"O\":\n",
    "                continue  # Skip if sentiment is not recognized\n",
    "\n",
    "            for i, (token_start, token_end) in enumerate(token_offsets):\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    if token_start == start:\n",
    "                        token_labels[i] = f\"B-{sentiment_standard}\"\n",
    "                    else:\n",
    "                        token_labels[i] = f\"I-{sentiment_standard}\"\n",
    "\n",
    "        # Clean tokens and remove any that are empty after cleaning\n",
    "        cleaned_tokens = [clean_word(token) for token in tokens]\n",
    "        cleaned_tokens, token_labels = zip(*[\n",
    "            (token, label) for token, label in zip(cleaned_tokens, token_labels) if token\n",
    "        ])\n",
    "\n",
    "        # Append the processed entry\n",
    "        processed_data.append({\n",
    "            \"tokens\": list(cleaned_tokens),\n",
    "            \"labels\": list(token_labels)\n",
    "        })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sdadas/polish-gpt2-medium\", use_fast=True, add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Proceed with your code\n",
    "sentiment_mapping = {\n",
    "    'Negative': 'Negative',\n",
    "    'Neutral': 'Neutral',\n",
    "    'Positive': 'Positive'\n",
    "}\n",
    "processed_data = process_jsonl(\"patryk.jsonl\", tokenizer, sentiment_mapping)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example:\n",
      "{'tokens': ['Lakier', 'roweru', 'bardzo', 'kiepskiej', 'jakości', 'robią', 'się', 'odpryski', 'nie', 'wiadomo', 'od', 'czego', 'rower', 'ładny', 'wygodny', 'ale', 'po', '3', 'miesiącach', 'użytkowania', 'widoczne', 'odpryski', 'lakieru', 'czego', 'za', 'taką', 'cenę', 'nie', 'powinno', 'być', 'Oczywiście', 'producent', 'twierdzi', 'że', 'są', 'to', 'wady', 'mechaniczne', 'dziecko', 'ma', 'w', 'lepszym', 'stanie', 'lakier', 'na', 'rowerze', 'ale', 'nie', 'z', 'tej', 'firmy', 'ODRADZAM', 'ZAKUP', 'Z', 'TEGO', 'POWODU', 'SZKODA', 'TYLE', 'KASY', 'I', 'NERWÓW', 'chyba', 'ze', 'rower', 'będzie', 'stał', 'nieużywany', 'za', 'szybą', 'Na', 'zakończenie', 'powiem', 'tak', 'porównując', 'lakier', 'zwykły', 'do', 'paznokci', 'a', 'hybrydę', 'wiadomo', 'w', 'tańszym', 'zwykłym', 'lakierze', 'robią', 'się', 'odpryski', 'a', 'lepszym', 'nie'], 'labels': ['O', 'O', 'O', 'B-Negative', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'B-Neutral', 'O', 'O', 'O', 'O', 'O', 'B-Positive', 'O']}\n"
     ]
    }
   ],
   "source": [
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "print(\"Dataset example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID Mapping:\n",
      "{'O': 0, 'B-Negative': 1, 'I-Negative': 2, 'B-Positive': 3, 'I-Positive': 4, 'B-Neutral': 5, 'I-Neutral': 6}\n",
      "\n",
      "ID to Label Mapping:\n",
      "{0: 'O', 1: 'B-Negative', 2: 'I-Negative', 3: 'B-Positive', 4: 'I-Positive', 5: 'B-Neutral', 6: 'I-Neutral'}\n"
     ]
    }
   ],
   "source": [
    "# Define label list including \"O\" for tokens outside any entity\n",
    "label_list = [\"O\", \"B-Negative\", \"I-Negative\", \"B-Positive\", \"I-Positive\", \"B-Neutral\", \"I-Neutral\"]\n",
    "\n",
    "# Create mappings from label to ID and ID to label\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Label to ID Mapping:\")\n",
    "print(label_to_id)\n",
    "\n",
    "print(\"\\nID to Label Mapping:\")\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f15dedcf5f4f7788fa0d0324eef54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset Example:\n",
      "{'labels': [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100, -100, -100, -100, -100, -100], 'input_ids': [573, 301, 523, 29345, 702, 11265, 775, 5059, 5166, 309, 672, 368, 452, 304, 3151, 357, 1386, 5557, 24161, 31670, 478, 291, 719, 10542, 15268, 13048, 672, 368, 452, 46987, 383, 1386, 313, 2594, 6908, 304, 3438, 739, 7083, 15271, 6905, 337, 543, 339, 18822, 42797, 2846, 438, 264, 16163, 1619, 25509, 293, 22660, 478, 304, 268, 725, 3390, 18373, 4069, 3454, 6300, 556, 6197, 57, 52, 556, 14652, 8076, 44938, 9622, 57, 17763, 8337, 7248, 16778, 6266, 431, 15068, 61, 598, 656, 4745, 59, 11390, 1491, 498, 5557, 626, 2174, 34416, 3506, 313, 48395, 992, 12506, 5481, 395, 50721, 25509, 13320, 306, 36366, 330, 26718, 763, 3151, 264, 7356, 1028, 16078, 46987, 299, 5166, 309, 672, 368, 452, 330, 16163, 304, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'offset_mapping': [[0, 1], [1, 3], [3, 6], [0, 6], [0, 6], [0, 4], [4, 9], [0, 7], [0, 5], [0, 3], [0, 3], [3, 5], [5, 8], [0, 3], [0, 7], [0, 2], [0, 5], [0, 5], [0, 5], [0, 7], [0, 3], [0, 2], [0, 1], [0, 10], [0, 11], [0, 8], [0, 3], [3, 5], [5, 8], [0, 5], [5, 7], [0, 5], [0, 2], [0, 4], [0, 4], [0, 3], [0, 7], [0, 3], [0, 10], [0, 9], [0, 8], [0, 2], [0, 2], [0, 2], [0, 4], [0, 11], [0, 7], [0, 2], [0, 1], [0, 7], [0, 6], [0, 6], [0, 2], [0, 7], [0, 3], [0, 3], [0, 1], [0, 3], [0, 5], [0, 2], [2, 4], [4, 6], [6, 8], [0, 1], [1, 3], [3, 4], [4, 5], [0, 1], [0, 2], [2, 4], [0, 3], [3, 5], [5, 6], [0, 2], [2, 4], [4, 6], [0, 2], [2, 4], [0, 1], [1, 3], [3, 4], [0, 1], [0, 1], [1, 3], [3, 4], [4, 6], [0, 5], [0, 2], [0, 5], [0, 6], [0, 4], [0, 6], [6, 10], [0, 2], [0, 5], [0, 2], [0, 11], [0, 6], [0, 3], [0, 10], [0, 6], [0, 6], [0, 2], [0, 8], [0, 1], [0, 5], [5, 7], [0, 7], [0, 1], [0, 3], [3, 7], [0, 7], [0, 5], [5, 8], [0, 5], [0, 3], [0, 3], [3, 5], [5, 8], [0, 1], [0, 7], [0, 3], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and aligns the labels with the tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): Dictionary containing 'tokens' and 'labels'.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized inputs with aligned labels.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Beginning of a word\n",
    "                label_ids.append(label_to_id.get(label[word_idx], 0))\n",
    "            else:\n",
    "                # Inside a word\n",
    "                if label[word_idx].startswith(\"B-\"):\n",
    "                    label_ids.append(label_to_id.get(label[word_idx].replace(\"B-\", \"I-\"), 0))\n",
    "                else:\n",
    "                    label_ids.append(label_to_id.get(label[word_idx], 0))\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization and alignment\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=['tokens', 'labels']\n",
    ")\n",
    "\n",
    "print(\"Tokenized Dataset Example:\")\n",
    "print(tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training samples: 240\n",
      "Number of evaluation samples: 60\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and evaluation sets (e.g., 80% train, 20% test)\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the 'train' and 'test' splits\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']\n",
    "\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0e15e2d41f4bc29e209a84955ce6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2d221b3cca43dc899c80c3d917142d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at sdadas/polish-gpt2-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForTokenClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (wpe): Embedding(2048, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "foundation_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"sdadas/polish-gpt2-medium\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "foundation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.h.12\n",
      "transformer.h.12.ln_1\n",
      "transformer.h.12.attn\n",
      "transformer.h.12.attn.c_attn\n",
      "transformer.h.12.attn.c_proj\n",
      "transformer.h.12.attn.attn_dropout\n",
      "transformer.h.12.attn.resid_dropout\n",
      "transformer.h.12.ln_2\n",
      "transformer.h.12.mlp\n",
      "transformer.h.12.mlp.c_fc\n",
      "transformer.h.12.mlp.c_proj\n",
      "transformer.h.12.mlp.act\n",
      "transformer.h.12.mlp.dropout\n",
      "transformer.h.13\n",
      "transformer.h.13.ln_1\n",
      "transformer.h.13.attn\n",
      "transformer.h.13.attn.c_attn\n",
      "transformer.h.13.attn.c_proj\n",
      "transformer.h.13.attn.attn_dropout\n",
      "transformer.h.13.attn.resid_dropout\n",
      "transformer.h.13.ln_2\n",
      "transformer.h.13.mlp\n",
      "transformer.h.13.mlp.c_fc\n",
      "transformer.h.13.mlp.c_proj\n",
      "transformer.h.13.mlp.act\n",
      "transformer.h.13.mlp.dropout\n",
      "transformer.h.14\n",
      "transformer.h.14.ln_1\n",
      "transformer.h.14.attn\n",
      "transformer.h.14.attn.c_attn\n",
      "transformer.h.14.attn.c_proj\n",
      "transformer.h.14.attn.attn_dropout\n",
      "transformer.h.14.attn.resid_dropout\n",
      "transformer.h.14.ln_2\n",
      "transformer.h.14.mlp\n",
      "transformer.h.14.mlp.c_fc\n",
      "transformer.h.14.mlp.c_proj\n",
      "transformer.h.14.mlp.act\n",
      "transformer.h.14.mlp.dropout\n",
      "transformer.h.15\n",
      "transformer.h.15.ln_1\n",
      "transformer.h.15.attn\n",
      "transformer.h.15.attn.c_attn\n",
      "transformer.h.15.attn.c_proj\n",
      "transformer.h.15.attn.attn_dropout\n",
      "transformer.h.15.attn.resid_dropout\n",
      "transformer.h.15.ln_2\n",
      "transformer.h.15.mlp\n",
      "transformer.h.15.mlp.c_fc\n",
      "transformer.h.15.mlp.c_proj\n",
      "transformer.h.15.mlp.act\n",
      "transformer.h.15.mlp.dropout\n",
      "transformer.h.16\n",
      "transformer.h.16.ln_1\n",
      "transformer.h.16.attn\n",
      "transformer.h.16.attn.c_attn\n",
      "transformer.h.16.attn.c_proj\n",
      "transformer.h.16.attn.attn_dropout\n",
      "transformer.h.16.attn.resid_dropout\n",
      "transformer.h.16.ln_2\n",
      "transformer.h.16.mlp\n",
      "transformer.h.16.mlp.c_fc\n",
      "transformer.h.16.mlp.c_proj\n",
      "transformer.h.16.mlp.act\n",
      "transformer.h.16.mlp.dropout\n",
      "transformer.h.17\n",
      "transformer.h.17.ln_1\n",
      "transformer.h.17.attn\n",
      "transformer.h.17.attn.c_attn\n",
      "transformer.h.17.attn.c_proj\n",
      "transformer.h.17.attn.attn_dropout\n",
      "transformer.h.17.attn.resid_dropout\n",
      "transformer.h.17.ln_2\n",
      "transformer.h.17.mlp\n",
      "transformer.h.17.mlp.c_fc\n",
      "transformer.h.17.mlp.c_proj\n",
      "transformer.h.17.mlp.act\n",
      "transformer.h.17.mlp.dropout\n",
      "transformer.h.18\n",
      "transformer.h.18.ln_1\n",
      "transformer.h.18.attn\n",
      "transformer.h.18.attn.c_attn\n",
      "transformer.h.18.attn.c_proj\n",
      "transformer.h.18.attn.attn_dropout\n",
      "transformer.h.18.attn.resid_dropout\n",
      "transformer.h.18.ln_2\n",
      "transformer.h.18.mlp\n",
      "transformer.h.18.mlp.c_fc\n",
      "transformer.h.18.mlp.c_proj\n",
      "transformer.h.18.mlp.act\n",
      "transformer.h.18.mlp.dropout\n",
      "transformer.h.19\n",
      "transformer.h.19.ln_1\n",
      "transformer.h.19.attn\n",
      "transformer.h.19.attn.c_attn\n",
      "transformer.h.19.attn.c_proj\n",
      "transformer.h.19.attn.attn_dropout\n",
      "transformer.h.19.attn.resid_dropout\n",
      "transformer.h.19.ln_2\n",
      "transformer.h.19.mlp\n",
      "transformer.h.19.mlp.c_fc\n",
      "transformer.h.19.mlp.c_proj\n",
      "transformer.h.19.mlp.act\n",
      "transformer.h.19.mlp.dropout\n",
      "transformer.h.20\n",
      "transformer.h.20.ln_1\n",
      "transformer.h.20.attn\n",
      "transformer.h.20.attn.c_attn\n",
      "transformer.h.20.attn.c_proj\n",
      "transformer.h.20.attn.attn_dropout\n",
      "transformer.h.20.attn.resid_dropout\n",
      "transformer.h.20.ln_2\n",
      "transformer.h.20.mlp\n",
      "transformer.h.20.mlp.c_fc\n",
      "transformer.h.20.mlp.c_proj\n",
      "transformer.h.20.mlp.act\n",
      "transformer.h.20.mlp.dropout\n",
      "transformer.h.21\n",
      "transformer.h.21.ln_1\n",
      "transformer.h.21.attn\n",
      "transformer.h.21.attn.c_attn\n",
      "transformer.h.21.attn.c_proj\n",
      "transformer.h.21.attn.attn_dropout\n",
      "transformer.h.21.attn.resid_dropout\n",
      "transformer.h.21.ln_2\n",
      "transformer.h.21.mlp\n",
      "transformer.h.21.mlp.c_fc\n",
      "transformer.h.21.mlp.c_proj\n",
      "transformer.h.21.mlp.act\n",
      "transformer.h.21.mlp.dropout\n",
      "transformer.h.22\n",
      "transformer.h.22.ln_1\n",
      "transformer.h.22.attn\n",
      "transformer.h.22.attn.c_attn\n",
      "transformer.h.22.attn.c_proj\n",
      "transformer.h.22.attn.attn_dropout\n",
      "transformer.h.22.attn.resid_dropout\n",
      "transformer.h.22.ln_2\n",
      "transformer.h.22.mlp\n",
      "transformer.h.22.mlp.c_fc\n",
      "transformer.h.22.mlp.c_proj\n",
      "transformer.h.22.mlp.act\n",
      "transformer.h.22.mlp.dropout\n",
      "transformer.h.23\n",
      "transformer.h.23.ln_1\n",
      "transformer.h.23.attn\n",
      "transformer.h.23.attn.c_attn\n",
      "transformer.h.23.attn.c_proj\n",
      "transformer.h.23.attn.attn_dropout\n",
      "transformer.h.23.attn.resid_dropout\n",
      "transformer.h.23.ln_2\n",
      "transformer.h.23.mlp\n",
      "transformer.h.23.mlp.c_fc\n",
      "transformer.h.23.mlp.c_proj\n",
      "transformer.h.23.mlp.act\n",
      "transformer.h.23.mlp.dropout\n",
      "transformer.ln_f\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in foundation_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<TaskType.SEQ_CLS: 'SEQ_CLS'>, <TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, <TaskType.TOKEN_CLS: 'TOKEN_CLS'>, <TaskType.QUESTION_ANS: 'QUESTION_ANS'>, <TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(TaskType))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,          # Correct task type for token-level tasks\n",
    "    r=64,                                  # Rank of LoRA; adjust as needed\n",
    "    lora_alpha=32,                         # Scaling factor; adjust as needed\n",
    "    lora_dropout=0.05,                     # Dropout probability\n",
    "    # target_modules=[\"classifier\"]           # Correct target module(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,298,631 || all params: 363,143,182 || trainable%: 1.7345\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForTokenClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForTokenClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(51200, 1024)\n",
       "        (wpe): Embedding(2048, 1024)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=3072, nx=1024)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "              (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "              (act): FastGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(pred_seq, label_seq) if label != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828529bfaa444105a4deda51d859c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0891, 'grad_norm': 16.019893646240234, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa65bffddad94660a367e742e6b6ee87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.728405475616455, 'eval_precision': 0.006798096532970768, 'eval_recall': 0.07518796992481203, 'eval_f1': 0.012468827930174562, 'eval_accuracy': 0.2973993288590604, 'eval_runtime': 2.6065, 'eval_samples_per_second': 23.02, 'eval_steps_per_second': 1.535, 'epoch': 1.0}\n",
      "{'loss': 1.8032, 'grad_norm': 15.863136291503906, 'learning_rate': 1.7333333333333336e-05, 'epoch': 1.33}\n",
      "{'loss': 1.4943, 'grad_norm': 14.172293663024902, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7163a02a4c48529def397ef6012c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3247723579406738, 'eval_precision': 0.00641025641025641, 'eval_recall': 0.045112781954887216, 'eval_f1': 0.011225444340505146, 'eval_accuracy': 0.5398489932885906, 'eval_runtime': 2.3228, 'eval_samples_per_second': 25.831, 'eval_steps_per_second': 1.722, 'epoch': 2.0}\n",
      "{'loss': 1.2492, 'grad_norm': 12.193614959716797, 'learning_rate': 1.4666666666666666e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1d5668b02f49b28ca3b2415eebb405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0040998458862305, 'eval_precision': 0.007228915662650603, 'eval_recall': 0.022556390977443608, 'eval_f1': 0.010948905109489052, 'eval_accuracy': 0.7479026845637584, 'eval_runtime': 2.4387, 'eval_samples_per_second': 24.603, 'eval_steps_per_second': 1.64, 'epoch': 3.0}\n",
      "{'loss': 1.0222, 'grad_norm': 9.366022109985352, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}\n",
      "{'loss': 0.8577, 'grad_norm': 7.331202507019043, 'learning_rate': 1.2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f17cf3d73994548a28884487ab2492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7719802260398865, 'eval_precision': 0.008403361344537815, 'eval_recall': 0.007518796992481203, 'eval_f1': 0.007936507936507938, 'eval_accuracy': 0.8653523489932886, 'eval_runtime': 2.9089, 'eval_samples_per_second': 20.626, 'eval_steps_per_second': 1.375, 'epoch': 4.0}\n",
      "{'loss': 0.7116, 'grad_norm': 5.934869766235352, 'learning_rate': 1.0666666666666667e-05, 'epoch': 4.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059f4e7fa8224650ab6d19fb3fb9cac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6231202483177185, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8947147651006712, 'eval_runtime': 2.8262, 'eval_samples_per_second': 21.23, 'eval_steps_per_second': 1.415, 'epoch': 5.0}\n",
      "{'loss': 0.6156, 'grad_norm': 4.2790727615356445, 'learning_rate': 9.333333333333334e-06, 'epoch': 5.33}\n",
      "{'loss': 0.5502, 'grad_norm': 3.1852235794067383, 'learning_rate': 8.000000000000001e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734b938495bc4047bbb98bb79863f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.540981113910675, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9068791946308725, 'eval_runtime': 2.5041, 'eval_samples_per_second': 23.961, 'eval_steps_per_second': 1.597, 'epoch': 6.0}\n",
      "{'loss': 0.4944, 'grad_norm': 2.98234486579895, 'learning_rate': 6.666666666666667e-06, 'epoch': 6.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ceb7aefd346849b95d98ac3c60f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.502855658531189, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9085570469798657, 'eval_runtime': 2.2561, 'eval_samples_per_second': 26.594, 'eval_steps_per_second': 1.773, 'epoch': 7.0}\n",
      "{'loss': 0.5031, 'grad_norm': 2.14072847366333, 'learning_rate': 5.333333333333334e-06, 'epoch': 7.33}\n",
      "{'loss': 0.4583, 'grad_norm': 1.2190487384796143, 'learning_rate': 4.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a720cbd8ac314096b276dabbbe6710b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4872707426548004, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9089765100671141, 'eval_runtime': 2.2275, 'eval_samples_per_second': 26.936, 'eval_steps_per_second': 1.796, 'epoch': 8.0}\n",
      "{'loss': 0.438, 'grad_norm': 1.6391103267669678, 'learning_rate': 2.666666666666667e-06, 'epoch': 8.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7531b5ac5c6b407f828004d1f97134c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48108160495758057, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9093959731543624, 'eval_runtime': 2.2163, 'eval_samples_per_second': 27.072, 'eval_steps_per_second': 1.805, 'epoch': 9.0}\n",
      "{'loss': 0.4401, 'grad_norm': 1.393377661705017, 'learning_rate': 1.3333333333333334e-06, 'epoch': 9.33}\n",
      "{'loss': 0.4517, 'grad_norm': 1.0753583908081055, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02381e53c6a54aa1b7146ec509a6de2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47935137152671814, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9098154362416108, 'eval_runtime': 2.2537, 'eval_samples_per_second': 26.623, 'eval_steps_per_second': 1.775, 'epoch': 10.0}\n",
      "{'train_runtime': 404.6252, 'train_samples_per_second': 5.931, 'train_steps_per_second': 0.371, 'train_loss': 0.8785693836212158, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f06978856bf432fa53f9caa96c5f48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "{'eval_loss': 1.728405475616455, 'eval_precision': 0.006798096532970768, 'eval_recall': 0.07518796992481203, 'eval_f1': 0.012468827930174562, 'eval_accuracy': 0.2973993288590604, 'eval_runtime': 2.6141, 'eval_samples_per_second': 22.953, 'eval_steps_per_second': 1.53, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf88d9a100ac4bd385516889ea8a2b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "{'eval_loss': 1.728405475616455, 'eval_precision': 0.006798096532970768, 'eval_recall': 0.07518796992481203, 'eval_f1': 0.012468827930174562, 'eval_accuracy': 0.2973993288590604, 'eval_runtime': 2.4149, 'eval_samples_per_second': 24.846, 'eval_steps_per_second': 1.656, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GemmaForTokenClassification', 'Gemma2ForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LlamaForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MistralForTokenClassification', 'MixtralForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NemotronForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PersimmonForTokenClassification', 'PhiForTokenClassification', 'Phi3ForTokenClassification', 'QDQBertForTokenClassification', 'Qwen2ForTokenClassification', 'Qwen2MoeForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'StableLmForTokenClassification', 'Starcoder2ForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Nie jestem zadowolony z zakupu. Słuchawki są niewygodne i głośność jest irytująca.\n",
      "Inference Results:\n",
      "[{'entity_group': 'Neutral', 'score': 0.22114557, 'word': ' Nie', 'start': 0, 'end': 3}, {'entity_group': 'Positive', 'score': 0.32489192, 'word': ' jestem', 'start': 3, 'end': 10}, {'entity_group': 'Negative', 'score': 0.34837538, 'word': ' zakupu', 'start': 23, 'end': 30}, {'entity_group': 'Negative', 'score': 0.34270105, 'word': '.', 'start': 30, 'end': 31}, {'entity_group': 'Negative', 'score': 0.4033354, 'word': ' Słu', 'start': 31, 'end': 35}, {'entity_group': 'Positive', 'score': 0.2079943, 'word': 'cha', 'start': 35, 'end': 38}, {'entity_group': 'Negative', 'score': 0.6835422, 'word': 'wki', 'start': 38, 'end': 41}, {'entity_group': 'Neutral', 'score': 0.26501462, 'word': ' są', 'start': 41, 'end': 44}, {'entity_group': 'Negative', 'score': 0.48030657, 'word': ' niewygodne', 'start': 44, 'end': 55}, {'entity_group': 'Negative', 'score': 0.41461366, 'word': ' i', 'start': 55, 'end': 57}, {'entity_group': 'Negative', 'score': 0.44892105, 'word': ' głoś', 'start': 57, 'end': 62}, {'entity_group': 'Negative', 'score': 0.2646223, 'word': 'ność', 'start': 62, 'end': 66}, {'entity_group': 'Positive', 'score': 0.41368273, 'word': ' jest', 'start': 66, 'end': 71}, {'entity_group': 'Negative', 'score': 0.2849825, 'word': 'tująca', 'start': 75, 'end': 81}, {'entity_group': 'Negative', 'score': 0.5125603, 'word': '.', 'start': 81, 'end': 82}]\n",
      "\n",
      "Text: Zaakceptowałem ofertę i kupiłem nowy telefon, który działa bez zarzutu.\n",
      "Inference Results:\n",
      "[{'entity_group': 'Neutral', 'score': 0.33570468, 'word': 'akcep', 'start': 2, 'end': 7}, {'entity_group': 'Negative', 'score': 0.41716394, 'word': 'towałem', 'start': 7, 'end': 14}, {'entity_group': 'Negative', 'score': 0.4845916, 'word': ' ofertę', 'start': 14, 'end': 21}, {'entity_group': 'Negative', 'score': 0.7270439, 'word': ' kupiłem', 'start': 23, 'end': 31}, {'entity_group': 'Negative', 'score': 0.46264762, 'word': ' nowy', 'start': 31, 'end': 36}, {'entity_group': 'Negative', 'score': 0.6997681, 'word': ' telefon', 'start': 36, 'end': 44}, {'entity_group': 'Negative', 'score': 0.2571377, 'word': ',', 'start': 44, 'end': 45}, {'entity_group': 'Negative', 'score': 0.5984639, 'word': ' który', 'start': 45, 'end': 51}, {'entity_group': 'Negative', 'score': 0.46107975, 'word': ' działa', 'start': 51, 'end': 58}, {'entity_group': 'Neutral', 'score': 0.61023, 'word': ' bez', 'start': 58, 'end': 62}, {'entity_group': 'Negative', 'score': 0.6280844, 'word': ' zarzutu', 'start': 62, 'end': 70}]\n",
      "\n",
      "Text: Pisanie opinii o produkcie było dla mnie bardzo łatwe i szybkie. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.28612813, 'word': 'sanie', 'start': 2, 'end': 7}, {'entity_group': 'Negative', 'score': 0.29944134, 'word': ' opinii o produkcie', 'start': 7, 'end': 26}, {'entity_group': 'Positive', 'score': 0.5396497, 'word': ' było', 'start': 26, 'end': 31}, {'entity_group': 'Positive', 'score': 0.31754908, 'word': ' mnie', 'start': 35, 'end': 40}, {'entity_group': 'Negative', 'score': 0.33658004, 'word': ' bardzo', 'start': 40, 'end': 47}, {'entity_group': 'Negative', 'score': 0.33396837, 'word': ' łatwe', 'start': 47, 'end': 53}, {'entity_group': 'Negative', 'score': 0.3890579, 'word': ' i', 'start': 53, 'end': 55}, {'entity_group': 'Negative', 'score': 0.521255, 'word': ' szybkie', 'start': 55, 'end': 63}, {'entity_group': 'Negative', 'score': 0.46182156, 'word': '.', 'start': 63, 'end': 64}, {'entity_group': 'Neutral', 'score': 0.23577806, 'word': ' ', 'start': 64, 'end': 65}]\n",
      "\n",
      "Text: One są wszystkie, luzacki, nudne, wporzadku, groźny, mieszane, fajny, zły, nie dobry,  dobra, pozytywne, piękne, smutne. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Neutral', 'score': 0.22532034, 'word': ' One', 'start': 0, 'end': 3}, {'entity_group': 'Positive', 'score': 0.2532509, 'word': ' są', 'start': 3, 'end': 6}, {'entity_group': 'Negative', 'score': 0.2639993, 'word': ' wszystkie', 'start': 6, 'end': 16}, {'entity_group': 'Positive', 'score': 0.2227776, 'word': ',', 'start': 16, 'end': 17}, {'entity_group': 'Neutral', 'score': 0.26713547, 'word': 'za', 'start': 20, 'end': 22}, {'entity_group': 'Positive', 'score': 0.24108258, 'word': 'cki', 'start': 22, 'end': 25}, {'entity_group': 'Positive', 'score': 0.19845851, 'word': ',', 'start': 25, 'end': 26}, {'entity_group': 'Negative', 'score': 0.2597406, 'word': ' nudne', 'start': 26, 'end': 32}, {'entity_group': 'Negative', 'score': 0.19612375, 'word': ',', 'start': 32, 'end': 33}, {'entity_group': 'Neutral', 'score': 0.25421727, 'word': ' wpo', 'start': 33, 'end': 37}, {'entity_group': 'Positive', 'score': 0.40104577, 'word': 'rzad', 'start': 37, 'end': 41}, {'entity_group': 'Negative', 'score': 0.21214283, 'word': 'ku,', 'start': 41, 'end': 44}, {'entity_group': 'Neutral', 'score': 0.3086363, 'word': ' mieszane', 'start': 52, 'end': 61}, {'entity_group': 'Negative', 'score': 0.21962714, 'word': ',', 'start': 61, 'end': 62}, {'entity_group': 'Negative', 'score': 0.22697595, 'word': ' fajny', 'start': 62, 'end': 68}, {'entity_group': 'Negative', 'score': 0.21045111, 'word': ' zły', 'start': 69, 'end': 73}, {'entity_group': 'Positive', 'score': 0.32403147, 'word': ' nie', 'start': 74, 'end': 78}, {'entity_group': 'Negative', 'score': 0.25947446, 'word': ' dobry', 'start': 78, 'end': 84}, {'entity_group': 'Negative', 'score': 0.21324235, 'word': ',', 'start': 84, 'end': 85}, {'entity_group': 'Negative', 'score': 0.33831608, 'word': ' ', 'start': 85, 'end': 86}, {'entity_group': 'Positive', 'score': 0.23877609, 'word': ' dobra,', 'start': 86, 'end': 93}, {'entity_group': 'Neutral', 'score': 0.40850633, 'word': ' pozytywne', 'start': 93, 'end': 103}, {'entity_group': 'Negative', 'score': 0.25835812, 'word': ',', 'start': 103, 'end': 104}, {'entity_group': 'Neutral', 'score': 0.35054284, 'word': ' piękne', 'start': 104, 'end': 111}, {'entity_group': 'Negative', 'score': 0.26015225, 'word': ',', 'start': 111, 'end': 112}, {'entity_group': 'Neutral', 'score': 0.47019005, 'word': ' smutne', 'start': 112, 'end': 119}, {'entity_group': 'Negative', 'score': 0.21338862, 'word': '.', 'start': 119, 'end': 120}, {'entity_group': 'Negative', 'score': 0.26319054, 'word': ' ', 'start': 120, 'end': 121}]\n",
      "\n",
      "Text: Całe to jebane zycie to jeden wielki dramat. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.34091663, 'word': ' to', 'start': 4, 'end': 7}, {'entity_group': 'Negative', 'score': 0.2803268, 'word': ' je', 'start': 7, 'end': 10}, {'entity_group': 'Positive', 'score': 0.33369818, 'word': ' jeden', 'start': 23, 'end': 29}, {'entity_group': 'Neutral', 'score': 0.22905393, 'word': ' dramat', 'start': 36, 'end': 43}, {'entity_group': 'Neutral', 'score': 0.20347193, 'word': '.', 'start': 43, 'end': 44}, {'entity_group': 'Neutral', 'score': 0.27497742, 'word': ' ', 'start': 44, 'end': 45}]\n",
      "\n",
      "Text: Chuj kurwa chuj. \n",
      "Inference Results:\n",
      "[{'entity_group': 'Negative', 'score': 0.28406984, 'word': ' kurwa', 'start': 4, 'end': 10}, {'entity_group': 'Negative', 'score': 0.3256076, 'word': ' chuj', 'start': 10, 'end': 15}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "inference_results = []\n",
    "\n",
    "example_texts = [\n",
    "    \"Nie jestem zadowolony z zakupu. Słuchawki są niewygodne i głośność jest irytująca.\",\n",
    "    \"Zaakceptowałem ofertę i kupiłem nowy telefon, który działa bez zarzutu.\",\n",
    "    \"Pisanie opinii o produkcie było dla mnie bardzo łatwe i szybkie. \",\n",
    "    \"One są wszystkie, luzacki, nudne, wporzadku, groźny, mieszane, fajny, zły, nie dobry,  dobra, pozytywne, piękne, smutne. \",\n",
    "    \"Całe to jebane zycie to jeden wielki dramat. \",\n",
    "    \"Chuj kurwa chuj. \",\n",
    "]\n",
    "\n",
    "for text in example_texts:\n",
    "    predictions = nlp(text)\n",
    "    inference_results.append({\n",
    "        \"text\": text,\n",
    "        \"predictions\": predictions\n",
    "    })\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Inference Results:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_colors = {\n",
    "    'Negative': 'red',\n",
    "    'Neutral': 'gray',\n",
    "    'Positive': 'green'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(label):\n",
    "    \"\"\"Extract the base sentiment from the label.\"\"\"\n",
    "    if label.startswith('B-') or label.startswith('I-'):\n",
    "        return label.split('-', 1)[1]\n",
    "    return label\n",
    "\n",
    "for result in inference_results:\n",
    "    text = result['text']\n",
    "    predictions = result['predictions']\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    sentiments = []\n",
    "    scores = []\n",
    "    \n",
    "    word_sentiments = ['O'] * len(words)\n",
    "    word_scores = [0.0] * len(words)\n",
    "    \n",
    "    for pred in predictions:\n",
    "        label = pred['entity']\n",
    "        sentiment = get_sentiment(label)\n",
    "        score = pred['score']\n",
    "        word = pred['word'].replace('</w>', '').strip()\n",
    "        \n",
    "        for idx, w in enumerate(words):\n",
    "            clean_w = re.sub(r'[^\\w]', '', w)\n",
    "            if word.lower() == clean_w.lower():\n",
    "                word_sentiments[idx] = sentiment\n",
    "                word_scores[idx] = score\n",
    "                break\n",
    "    \n",
    "    colors = [sentiment_colors.get(sentiment, 'black') for sentiment in word_sentiments]\n",
    "    \n",
    "    hover_texts = [f\"Sentiment: {sentiment}<br>Score: {score:.2f}\" \n",
    "                   for sentiment, score in zip(word_sentiments, word_scores)]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    spacing = 0.5  # Adjust spacing between words\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            text=[word],\n",
    "            mode='text',\n",
    "            textfont=dict(color=colors[i], size=16),\n",
    "            hoverinfo='text',\n",
    "            hovertext=hover_texts[i],\n",
    "            showlegend=False\n",
    "        ))\n",
    "        # Increment x position\n",
    "        x += len(word) * 0.1 + spacing\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Inference Results\",\n",
    "        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
    "        margin=dict(l=20, r=20, t=50, b=20)\n",
    "    )\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
