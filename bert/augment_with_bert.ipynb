{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1392f7010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "mask_prob = 0.15  # Probability of masking a token\n",
    "augmentation_factor = 2  # Number of augmented examples per original example\n",
    "random_seed = 42\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and masked language model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up a pipeline for masked language modeling\n",
    "mlm_pipeline = pipeline(\"fill-mask\", model=mlm_model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text_with_word_mask(text, augmentation_factor=1, mask_prob=0.15):\n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(augmentation_factor):\n",
    "        masked_words = words[:]\n",
    "        num_words_to_mask = max(1, int(len(words) * mask_prob))\n",
    "        mask_indices = random.sample(range(len(words)), num_words_to_mask)\n",
    "\n",
    "        for idx in mask_indices:\n",
    "            masked_words[idx] = tokenizer.mask_token\n",
    "\n",
    "        masked_text = \" \".join(masked_words)\n",
    "\n",
    "        predictions = mlm_pipeline(masked_text)\n",
    "\n",
    "        # Replace the masks with predictions\n",
    "        for idx, pred in zip(mask_indices, predictions):\n",
    "            try:\n",
    "                if isinstance(pred, list) and len(pred) > 0:\n",
    "                    # Normal case: List of predictions\n",
    "                    masked_words[idx] = pred[0][\"token_str\"]\n",
    "                elif isinstance(pred, dict):\n",
    "                    # Edge case: Single dictionary as output\n",
    "                    masked_words[idx] = pred.get(\"token_str\", tokenizer.mask_token)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with prediction: {e}, Prediction Output: {pred}\")\n",
    "                masked_words[idx] = tokenizer.mask_token  # Fallback\n",
    "\n",
    "        augmented_texts.append(\" \".join(masked_words))\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "\n",
    "def augment_dataset(df, text_column=\"text\", label_column=\"label\", augmentation_factor=1, mask_prob=0.15):\n",
    "    augmented_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        original_text = row[text_column]\n",
    "        label = row[label_column]\n",
    "\n",
    "        augmented_texts = augment_text_with_word_mask(\n",
    "            original_text, augmentation_factor=augmentation_factor, mask_prob=mask_prob\n",
    "        )\n",
    "\n",
    "        for aug_text in augmented_texts:\n",
    "            augmented_rows.append({text_column: aug_text, label_column: label})\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to 'augmented_texts.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = \"all_texts.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_df = augment_dataset(df, text_column=\"text\", label_column=\"label\", augmentation_factor=augmentation_factor, mask_prob=mask_prob)\n",
    "\n",
    "# Save the augmented dataset\n",
    "augmented_df.to_csv(\"augmented_texts.csv\", index=False)\n",
    "print(\"Augmented dataset saved to 'augmented_texts_bert.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
