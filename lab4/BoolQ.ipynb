{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_tile,\n",
    "    aes,\n",
    "    theme,\n",
    "    element_text,\n",
    "    ggtitle,\n",
    "    xlab, ylab, ggsave,\n",
    "    facet_wrap\n",
    ")\n",
    "from plotnine.scales import scale_y_reverse, scale_fill_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_to_distrib(model, embed, log=False, logits=False):\n",
    "\n",
    "    vocab = pv.embed_to_distrib(model, embed, logits=logits)\n",
    "    return vocab\n",
    "\n",
    "# factual recall\n",
    "def factual_recall(text, model, tokenizer):\n",
    "    print(text)\n",
    "    base = deepcopy(text)\n",
    "    inputs = [\n",
    "        tokenizer(base, return_tensors=\"pt\").to(device),\n",
    "    ]\n",
    "    base = deepcopy(text)\n",
    "    for i in range(len(inputs)):\n",
    "        res = model(**inputs[i], output_hidden_states=True)\n",
    "        last = res.last_hidden_state if hasattr(res, 'last_hidden_state') else res.hidden_states[-1]\n",
    "        distrib = embed_to_distrib(model, last, logits=False)\n",
    "        pv.top_vals(tokenizer, distrib[0][-1], n=10)\n",
    "        \n",
    "def create_noise_intervention(seq_len): \n",
    "    class NoiseIntervention(pv.ConstantSourceIntervention, pv.LocalistRepresentationIntervention):\n",
    "        def __init__(self, embed_dim, **kwargs):\n",
    "            super().__init__()\n",
    "            self.interchange_dim = embed_dim\n",
    "            self.seq_len = kwargs.get(\"seq_len\", 1)\n",
    "            rs = np.random.RandomState(1)\n",
    "            prng = lambda *shape: rs.randn(*shape)\n",
    "            self.noise = torch.from_numpy(\n",
    "                prng(1, self.seq_len, embed_dim)).to(device)\n",
    "            self.noise_level = 0.13462981581687927\n",
    "\n",
    "        def forward(self, base, source=None, subspaces=None): \n",
    "            # source argument is ignored unlike in causal interventions, since we are adding noise without reference to any other input\n",
    "            base[..., : self.interchange_dim] += self.noise * self.noise_level\n",
    "            return base\n",
    "\n",
    "        def __str__(self):\n",
    "            return f\"NoiseIntervention(embed_dim={self.embed_dim}, seq_len={self.seq_len})\"\n",
    "\n",
    "    return NoiseIntervention\n",
    "\n",
    "\n",
    "def corrupted_config(model_type, seq_len):\n",
    "    config = pv.IntervenableConfig(\n",
    "        model_type=model_type,\n",
    "        representations=[\n",
    "            pv.RepresentationConfig(\n",
    "                0,              # layer\n",
    "                \"block_input\",  # intervention type\n",
    "            ),\n",
    "        ],\n",
    "        intervention_types=[create_noise_intervention(seq_len)],\n",
    "    )\n",
    "    return config\n",
    "\n",
    "def corrupted_recall(text, model, tokenizer, subject_pos):\n",
    "    print(text)\n",
    "    base = tokenizer(deepcopy(text), return_tensors=\"pt\").to(device)\n",
    "    seq_len = len(subject_pos)\n",
    "    config = corrupted_config(type(model), seq_len)\n",
    "    intervenable = pv.IntervenableModel(config, model)\n",
    "\n",
    "    _, counterfactual_outputs = intervenable(\n",
    "        base, unit_locations={\"base\": ([[subject_pos]])}\n",
    "    )\n",
    "    last = counterfactual_outputs.last_hidden_state if hasattr(counterfactual_outputs, 'last_hidden_state') else counterfactual_outputs.hidden_states[-1]\n",
    "    distrib = embed_to_distrib(model, last, logits=False)\n",
    "    pv.top_vals(tokenizer, distrib[0][-1], n=10)\n",
    "    \n",
    "def format_tokens(tokenizer, tokens):\n",
    "    return [tokenizer.decode(tok).replace(\"\\n\", \"\\\\n\") for tok in tokens]\n",
    "\n",
    "# restored run - corrupt input in some position, then restore the hidden state at a particular layer for some positions\n",
    "def restore_corrupted_with_interval_config(\n",
    "        layer, \n",
    "        stream=\"mlp_activation\", \n",
    "        window=10, \n",
    "        num_layers=48,\n",
    "        seq_len=1\n",
    "    ):\n",
    "    start = max(0, layer - window // 2)\n",
    "    end = min(num_layers, layer - (-window // 2))\n",
    "    config = pv.IntervenableConfig(\n",
    "        representations=[\n",
    "            pv.RepresentationConfig(\n",
    "                0,       # layer\n",
    "                \"block_input\",  # intervention type\n",
    "            ),\n",
    "        ] + [\n",
    "            pv.RepresentationConfig(\n",
    "                i,       # layer\n",
    "                stream,  # intervention type\n",
    "        ) for i in range(start, end)],\n",
    "        intervention_types=\\\n",
    "            [create_noise_intervention(seq_len)]+[pv.VanillaIntervention]*(end-start),\n",
    "        seq_len=seq_len,\n",
    "    )\n",
    "    return config\n",
    "\n",
    "# corrupt all layers and positions 0, 1, 2, 3 (\"The Space Needle\", i.e. the subject of the fact) and restore at a single position at every layer\n",
    "def restored_run(text, model, tokenizer, subject_pos, save_path):\n",
    "    base = tokenizer(deepcopy(text), return_tensors=\"pt\").to(device)\n",
    "    yes_token = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    seq_len = len(subject_pos)\n",
    "    \n",
    "    for stream in [\"block_output\", \"mlp_activation\", \"attention_output\"]:\n",
    "        data = []\n",
    "        n_layers = model.config.n_layer if hasattr(model.config, \"n_layer\") else model.config.num_hidden_layers\n",
    "        for layer_i in tqdm(range(n_layers)):\n",
    "            for pos_i in range(len(base.input_ids[0])):\n",
    "                config = restore_corrupted_with_interval_config(\n",
    "                    layer_i, stream, \n",
    "                    window=1 if stream == \"block_output\" else 10,\n",
    "                    num_layers=n_layers,\n",
    "                    seq_len=seq_len\n",
    "                )\n",
    "                n_restores = len(config.representations) - 1\n",
    "                intervenable = pv.IntervenableModel(config, model)\n",
    "                _, counterfactual_outputs = intervenable(\n",
    "                    base,\n",
    "                    [None] + [base]*n_restores,\n",
    "                    {\n",
    "                        \"sources->base\": (\n",
    "                            [None] + [[[pos_i]]]*n_restores,\n",
    "                            [[subject_pos]] + [[[pos_i]]]*n_restores,\n",
    "                        )\n",
    "                    },\n",
    "                )\n",
    "                last = counterfactual_outputs.last_hidden_state if hasattr(counterfactual_outputs, 'last_hidden_state') else counterfactual_outputs.hidden_states[-1]\n",
    "                distrib = embed_to_distrib(\n",
    "                    model, last, logits=False\n",
    "                )\n",
    "                yes_prob = distrib[0][-1][yes_token].detach().cpu().item()\n",
    "                no_prob = distrib[0][-1][no_token].detach().cpu().item()\n",
    "                data.append({\"layer\": layer_i, \"pos\": pos_i, \"yes_prob\": yes_prob, \"no_prob\": no_prob})\n",
    "        df = pd.DataFrame(data)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        df.to_csv(f\"{save_path}/pyvene_rome_{stream}.csv\")\n",
    "        \n",
    "def plot_activations(labels, breaks, colors, titles, path):\n",
    "    for stream in [\"block_output\", \"mlp_activation\", \"attention_output\"]:\n",
    "        df = pd.read_csv(f\"{path}/pyvene_rome_{stream}.csv\")\n",
    "        df[\"layer\"] = df[\"layer\"].astype(int)\n",
    "        df[\"pos\"] = df[\"pos\"].astype(int)\n",
    "        df[\"p(Yes)\"] = df[\"yes_prob\"].astype(float)\n",
    "        df[\"p(No)\"] = df[\"no_prob\"].astype(float)\n",
    "\n",
    "        df_melted = df.melt(\n",
    "            id_vars=[\"layer\", \"pos\"],\n",
    "            value_vars=[\"p(Yes)\", \"p(No)\"],\n",
    "            var_name=\"Token\",\n",
    "            value_name=\"Probability\"\n",
    "        )\n",
    "\n",
    "        # Create the plot with facets\n",
    "        plot = (\n",
    "            ggplot(df_melted, aes(x=\"layer\", y=\"pos\"))\n",
    "            + facet_wrap(\"~Token\", scales=\"free_y\")  # Side-by-side subplots for \"p(Yes)\" and \"p(No)\"\n",
    "            + geom_tile(aes(fill=\"Probability\"))\n",
    "            + scale_fill_cmap(colors[stream])\n",
    "            + xlab(titles[stream])\n",
    "            + scale_y_reverse(\n",
    "                limits=(-0.5, len(labels) - 0.5), \n",
    "                breaks=breaks, labels=labels\n",
    "            )\n",
    "            + ggtitle(f\"{stream} Activations: p(Yes) and p(No)\")\n",
    "            + theme(figure_size=(10, 5))  # Adjust figure size for side-by-side plots\n",
    "            + ylab(\"\") \n",
    "            + theme(axis_text_y=element_text(angle=90, hjust=1))\n",
    "        )\n",
    "        display(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_boolq_dataset():\n",
    "    dataset = load_dataset(\"boolq\")\n",
    "    df = pd.DataFrame({\n",
    "        'question': dataset['train']['question'],\n",
    "        'answer': dataset['train']['answer'],\n",
    "        'passage': dataset['train']['passage']\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is confectionary sugar the same as powdered sugar</td>\n",
       "      <td>True</td>\n",
       "      <td>Powdered sugar, also called confectioners' sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is elder scrolls online the same as skyrim</td>\n",
       "      <td>False</td>\n",
       "      <td>As with other games in The Elder Scrolls serie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  answer  \\\n",
       "0    do iran and afghanistan speak the same language    True   \n",
       "1  do good samaritan laws protect those who help ...    True   \n",
       "2  is windows movie maker part of windows essentials    True   \n",
       "3  is confectionary sugar the same as powdered sugar    True   \n",
       "4         is elder scrolls online the same as skyrim   False   \n",
       "\n",
       "                                             passage  \n",
       "0  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...  \n",
       "1  Good Samaritan laws offer legal protection to ...  \n",
       "2  Windows Movie Maker (formerly known as Windows...  \n",
       "3  Powdered sugar, also called confectioners' sug...  \n",
       "4  As with other games in The Elder Scrolls serie...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_boolq_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 1600)\n",
       "  (wpe): Embedding(1024, 1600)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-47): 48 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=4800, nx=1600)\n",
       "        (c_proj): Conv1D(nf=1600, nx=1600)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=6400, nx=1600)\n",
       "        (c_proj): Conv1D(nf=1600, nx=6400)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init GPT2-XL model\n",
    "config, tokenizer, gpt = pv.create_gpt2(name=\"gpt2-xl\")\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_for_model(text, tokenizer, device=device):\n",
    "    \"\"\"Prepare input text for model processing\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    return {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "def print_token_info(model, tokenizer, logits=None, input_ids=None, k=5, prefix=\"\", only_if_changed=False, last_probs=None):\n",
    "    \"\"\"Print token information - either tokenized input or top k probabilities\"\"\"\n",
    "    if logits is not None:\n",
    "        # Print top k tokens and probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_k = torch.topk(probs, k)\n",
    "        \n",
    "        if only_if_changed and last_probs is not None:\n",
    "            # Check if probabilities changed significantly\n",
    "            if torch.allclose(probs, last_probs, rtol=1e-3):\n",
    "                return probs\n",
    "        \n",
    "        print(f\"\\n{prefix}\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "            token = tokenizer.decode(idx)\n",
    "            print(f\"{i+1}. '{token}': {prob:.4f}\")\n",
    "        return probs\n",
    "    \n",
    "    elif input_ids is not None:\n",
    "        # Print tokenized input information\n",
    "        print(\"\\nTokens:\")\n",
    "        for token in input_ids['input_ids'][0]:\n",
    "            print(f\"'{tokenizer.decode(token)}'\")\n",
    "    else:\n",
    "        raise ValueError(\"Either input_ids or logits must be provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_boolq_example(question, model, tokenizer, device=device):\n",
    "    \"\"\"Process a single BoolQ example with debug information\"\"\"\n",
    "    # Format question with proper prompt\n",
    "    input_text = f\"Question: {question} Answer:\"\n",
    "    print(f\"\\nDebug: Input text: {input_text}\")\n",
    "    \n",
    "    input_ids = prepare_input_for_model(input_text, tokenizer, device)\n",
    "    print(\"\\nDebug: Tokenized input:\")\n",
    "    print_token_info(model, tokenizer, input_ids=input_ids)\n",
    "    \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(sample):\n",
    "    text = process_boolq_example(sample[\"question\"], gpt, tokenizer)\n",
    "    text_token = \" Yes\" if sample[\"answer\"] is True else \" No\"\n",
    "    return text, text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug: Input text: Question: is windows movie maker part of windows essentials Answer:\n",
      "\n",
      "Debug: Tokenized input:\n",
      "\n",
      "Tokens:\n",
      "'Question'\n",
      "':'\n",
      "' is'\n",
      "' windows'\n",
      "' movie'\n",
      "' maker'\n",
      "' part'\n",
      "' of'\n",
      "' windows'\n",
      "' essentials'\n",
      "' Answer'\n",
      "':'\n"
     ]
    }
   ],
   "source": [
    "text, text_token = prepare_sample(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT:   No\n",
      "Question: is windows movie maker part of windows essentials Answer:\n",
      "_yes                 0.13121595978736877\n",
      "_No                  0.11776792258024216\n",
      "\\n                   0.10949345678091049\n",
      "_Yes                 0.09982707351446152\n",
      "_no                  0.08984081447124481\n",
      "_Windows             0.04195360094308853\n",
      "_It                  0.03211362287402153\n",
      "_it                  0.022324759513139725\n",
      "_windows             0.021807190030813217\n",
      "_The                 0.01462388876825571\n"
     ]
    }
   ],
   "source": [
    "print('GT: ', text_token)\n",
    "factual_recall(text, gpt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question', ':', ' is', ' windows', ' movie', ' maker', ' part', ' of', ' windows', ' essentials', ' Answer', ':']\n"
     ]
    }
   ],
   "source": [
    "formated_tokens = format_tokens(tokenizer, tokenizer(text)[\"input_ids\"])\n",
    "print(formated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_tokens_from_question(question, tokenizer):\n",
    "    \"\"\"Extract subject tokens from the question\"\"\"\n",
    "    # Tokenize the question with the prompt\n",
    "    full_text = f\"Question: {question}? Answer: \"\n",
    "    tokens = tokenizer.encode(full_text)\n",
    "    token_words = [tokenizer.decode(t).strip().lower() for t in tokens]\n",
    "    \n",
    "    # Question words to look for\n",
    "    question_words = [\"what\", \"is\", \"are\", \"does\", \"do\", \"has\", \"can\", \"did\", \"will\", \"was\", \"were\"]\n",
    "    \n",
    "    # Find the position after question word and take exactly 3 tokens\n",
    "    subject_positions = []\n",
    "    for i, word in enumerate(token_words):\n",
    "        if word in question_words and i + 3 < len(token_words):\n",
    "            # Always take exactly 3 tokens as subject\n",
    "            subject_positions = list(range(i + 1, i + 4))\n",
    "            break\n",
    "    \n",
    "    # If no question word found or not enough tokens, take first 3 non-punctuation tokens\n",
    "    if not subject_positions:\n",
    "        count = 0\n",
    "        for i, word in enumerate(token_words):\n",
    "            if word not in ['question', ':', 'answer'] and count < 3:\n",
    "                subject_positions.append(i)\n",
    "                count += 1\n",
    "            if count == 3:\n",
    "                break\n",
    "    \n",
    "    return subject_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: is windows movie maker part of windows essentials Answer:\n",
      "[5, 6, 7]\n",
      "[' maker', ' part', ' of']\n",
      "Question: is windows movie maker part of windows essentials Answer:\n",
      "\\n                   0.1348111629486084\n",
      "_yes                 0.0988793894648552\n",
      "_Yes                 0.07032006233930588\n",
      "_windows             0.06847503036260605\n",
      "_no                  0.06502274423837662\n",
      "_Windows             0.05554073303937912\n",
      "_No                  0.05104365199804306\n",
      "_it                  0.01944822631776333\n",
      "_It                  0.019329600036144257\n",
      "\\n\\n                 0.018422432243824005\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "subject_positions = get_subject_tokens_from_question(text, tokenizer)\n",
    "print(subject_positions)\n",
    "# print the subject words using formated_tokens\n",
    "subject_words = [formated_tokens[pos] for pos in subject_positions]\n",
    "print(subject_words)\n",
    "\n",
    "\n",
    "corrupted_recall(text, gpt, tokenizer, subject_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'output/gpt2xl'\n",
    "# restored_run(text, gpt, tokenizer, subject_positions, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles={\n",
    "#     \"block_output\": \"single restored layer in GPT2-XL\",\n",
    "#     \"mlp_activation\": \"center of interval of 10 patched mlp layer\",\n",
    "#     \"attention_output\": \"center of interval of 10 patched attn layer\"\n",
    "# }\n",
    "\n",
    "# colors={\n",
    "#     \"block_output\": \"Purples\",\n",
    "#     \"mlp_activation\": \"Greens\",\n",
    "#     \"attention_output\": \"Reds\"\n",
    "# } \n",
    "        \n",
    "# labels = ['Is','windows','movie','maker','part','of','windows','essentials']\n",
    "# breaks = list(range(0, len(labels)))\n",
    "# plot_activations(labels, breaks, colors, titles, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, model, tokenizer, output_prefix):\n",
    "    text, text_token = prepare_sample(df.iloc[2])\n",
    "    print('GT: ', text_token)\n",
    "    factual_recall(text, model, tokenizer)\n",
    "    subject_positions = get_subject_tokens_from_question(text, tokenizer)\n",
    "    \n",
    "    path = f'{output_prefix}'\n",
    "    restored_run(text, model, tokenizer, subject_positions, path) \n",
    "\n",
    "    titles={\n",
    "        \"block_output\": \"single restored layer in GPT2-XL\",\n",
    "        \"mlp_activation\": \"center of interval of 10 patched mlp layer\",\n",
    "        \"attention_output\": \"center of interval of 10 patched attn layer\"\n",
    "    }\n",
    "\n",
    "    colors={\n",
    "        \"block_output\": \"Purples\",\n",
    "        \"mlp_activation\": \"Greens\",\n",
    "        \"attention_output\": \"Reds\"\n",
    "    }\n",
    "\n",
    "    labels = format_tokens(tokenizer, tokenizer(text)[\"input_ids\"])\n",
    "    breaks = list(range(0, len(labels)))\n",
    "    plot_activations(labels, breaks, colors, titles, path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_examples(df, model, tokenizer, num_examples=10, random_seed=42, output_prefix = 'output/boolq_example'):\n",
    "    \"\"\"Process multiple examples from the BoolQ dataset\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    selected_indices = np.random.choice(len(df), num_examples, replace=False)\n",
    "    \n",
    "    results = []\n",
    "    for idx in selected_indices:\n",
    "        example = df.iloc[idx]\n",
    "        print(f\"\\nProcessing example {idx}:\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Answer: {example['answer']}\")\n",
    "        \n",
    "        output_prefix_idx = f\"{output_prefix}_{idx}\"\n",
    "        \n",
    "        process_text(example['question'], model, tokenizer, output_prefix_idx)\n",
    "        \n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            'question': example['question'],\n",
    "            'answer': example['answer'],\n",
    "            'output_prefix': output_prefix\n",
    "        })\n",
    "    \n",
    "    # Save results summary\n",
    "    pd.DataFrame(results).to_csv(\"output/processed_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_examples(df, gpt, tokenizer, num_examples=2, random_seed=42, output_prefix = 'output/gpt2xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2-XL BoolQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = nothing to see here\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init GPT-2 BoolQ model\n",
    "config, tokenizer, gpt_boolq = pv.create_gpt2(name='utahnlp/boolq_gpt2-xl_seed-1')\n",
    "gpt_boolq.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GT: ', text_token)\n",
    "factual_recall(text, gpt_boolq, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_tokens(tokenizer, tokenizer(text)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_positions = [6,7]\n",
    "\n",
    "corrupted_recall(text, gpt_boolq, tokenizer, subject_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'output/gpt2xl_boolq'\n",
    "# restored_run(text, gpt_boolq, tokenizer, subject_positions, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles={\n",
    "#     \"block_output\": \"single restored layer in GPT2-XL BoolQ\",\n",
    "#     \"mlp_activation\": \"center of interval of 10 patched mlp layer\",\n",
    "#     \"attention_output\": \"center of interval of 10 patched attn layer\"\n",
    "# }\n",
    "\n",
    "# colors={\n",
    "#     \"block_output\": \"Purples\",\n",
    "#     \"mlp_activation\": \"Greens\",\n",
    "#     \"attention_output\": \"Reds\"\n",
    "# } \n",
    "        \n",
    "# labels = ['Is','windows','movie','maker','part','of','windows','essentials']\n",
    "# breaks = list(range(0, len(labels)))\n",
    "# plot_activations(labels, breaks, colors, titles, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_examples(df, gpt_boolq, tokenizer, num_examples=2, random_seed=42, output_prefix = 'output/gpt2xl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
