{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"sdadas/polish-gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(51200, 1024)\n",
       "  (wpe): Embedding(2048, 1024)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-23): 24 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "        (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "        (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "        (act): FastGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FF layers: 100%|██████████| 24/24 [00:22<00:00,  1.06it/s]\n",
      "Processing FF keys: 100%|██████████| 24/24 [00:22<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "E = model.wte.weight.data  # shape: (vocab_size, d_model)\n",
    "E = E.cpu()\n",
    "\n",
    "# Transpose E for easier multiplication\n",
    "E_T = E.T  # shape: (d_model, vocab_size)\n",
    "\n",
    "# Function to extract and project FF values into embedding space\n",
    "def extract_ff_values(model, E_T, tokenizer, k=10):\n",
    "    ff_neuron_tokens = []\n",
    "    for layer_idx, block in enumerate(tqdm(model.h, desc=\"Processing FF layers\")):\n",
    "        mlp = block.mlp\n",
    "        # Get the feed-forward weights\n",
    "        # c_fc: Conv1D(nf=4096, nx=1024) - FF keys (K)\n",
    "        # c_proj: Conv1D(nf=1024, nx=4096) - FF values (V)\n",
    "        K = mlp.c_fc.weight.data.T  # shape: (4096, 1024)\n",
    "        V = mlp.c_proj.weight.data  # shape: (4096, 1024)\n",
    "\n",
    "        # Project FF values into embedding space\n",
    "        V_proj = V @ E_T  # shape: (4096, vocab_size)\n",
    "\n",
    "        # For each neuron, get top-k tokens\n",
    "        layer_neuron_tokens = []\n",
    "        for neuron_idx in range(V_proj.shape[0]):\n",
    "            neuron_proj = V_proj[neuron_idx]  # shape: (vocab_size,)\n",
    "            topk = torch.topk(neuron_proj, k=k)\n",
    "            topk_indices = topk.indices  # indices of top-k tokens\n",
    "            topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "            layer_neuron_tokens.append({\n",
    "                'neuron': neuron_idx,\n",
    "                'top_tokens': topk_tokens\n",
    "            })\n",
    "        ff_neuron_tokens.append({\n",
    "            'layer': layer_idx,\n",
    "            'neurons': layer_neuron_tokens\n",
    "        })\n",
    "    return ff_neuron_tokens\n",
    "\n",
    "# Function to extract and project FF keys into embedding space\n",
    "def extract_ff_keys(model, E_T, tokenizer, k=10):\n",
    "    ff_key_tokens = []\n",
    "    for layer_idx, block in enumerate(tqdm(model.h, desc=\"Processing FF keys\")):\n",
    "        mlp = block.mlp\n",
    "        # Get the FF keys\n",
    "        K = mlp.c_fc.weight.data.T  # shape: (4096, 1024)\n",
    "\n",
    "        # Project FF keys into embedding space\n",
    "        K_proj = K @ E_T  # shape: (4096, vocab_size)\n",
    "\n",
    "        # For each neuron, get top-k tokens\n",
    "        layer_neuron_tokens = []\n",
    "        for neuron_idx in range(K_proj.shape[0]):\n",
    "            neuron_proj = K_proj[neuron_idx]  # shape: (vocab_size,)\n",
    "            topk = torch.topk(neuron_proj, k=k)\n",
    "            topk_indices = topk.indices\n",
    "            topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "            layer_neuron_tokens.append({\n",
    "                'neuron': neuron_idx,\n",
    "                'top_tokens': topk_tokens\n",
    "            })\n",
    "        ff_key_tokens.append({\n",
    "            'layer': layer_idx,\n",
    "            'neurons': layer_neuron_tokens\n",
    "        })\n",
    "    return ff_key_tokens\n",
    "\n",
    "# Extract and project FF values and keys\n",
    "ff_values = extract_ff_values(model, E_T, tokenizer, k=10)\n",
    "ff_keys = extract_ff_keys(model, E_T, tokenizer, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-forward values (Layer 3, first 5 neurons):\n",
      "Neuron 0: ['aks', ' dłuż', 'stka', 'gat', ' Jast', 'Jakub', 'lacz', ' mac', ' rondo', ' rabat']\n",
      "Neuron 1: [' komunalnych', ' podatkowej', ' spalin', 'kowicz', 'cian', ' administracyjnych', ' mieszkaniowej', 'zów', ' budżetowych', ' przemysłowej']\n",
      "Neuron 2: ['sey', 'biu', 'bridge', 'ney', ' zapytania', 'witz', 'bież', ' numerów', 'nerów', 'dis']\n",
      "Neuron 3: ['rola', ' Rei', 'remier', 'stanowi', ' Głównej', 'roe', 'dora', ' oczekując', ' Bogdana', 'stre']\n",
      "Neuron 4: [' cywilnego', 'eu', 'neta', '\\u202f', ' kredytu', 'olat', ' jesli', ' ukarany', ' warunk', ' bak']\n",
      "Neuron 5: [' Wodnej', ' Pona', 'finy', 'usse', 'niny', ' żyjesz', 'ninę', 'foni', 'owity', 'fina']\n",
      "Neuron 6: ['ancie', 'amie', 'alizacja', 'amina', 'unku', 'antem', 'antu', 'orki', 'anci', 'ensy']\n",
      "Neuron 7: ['aku', 'ange', 'uge', ' Bazy', 'ō', 'fla', 'gospodar', 'bio', '�', 'ffe']\n",
      "Neuron 8: ['źnica', ' maszynowego', 'rup', ' samochodowym', 'paskow', 'lizowała', ' kierowcą', ' motocy', 'und', ' aptece']\n",
      "Neuron 9: ['zwykopem', 'łowie', 'zyn', 'szkiewicz', 'taria', ' gatunk', 'lerowi', 'urów', 'żby', 'zym']\n",
      "Neuron 10: ['hre', 'eme', ' człowieczeństwa', 'yton', 'niczy', ' zawodowy', ' zawodowej', 'ób', 'jeste', ' zawodową']\n",
      "Neuron 11: ['ison', 'twie', 'entar', ' krzaki', ' Waldemar', ' dziedzi', ' krzaków', 'ush', 'stia', 'Pocz']\n",
      "Neuron 12: ['sce', ' Kono', ' Ap', 'kładają', ' sezon', ' je', ' Organi', ' Lew', ' ścisk', ' E']\n",
      "Neuron 13: ['czeniowe', 'est', ' Walii', 'ległość', ' średnim', ' Środkowej', 'arstwa', 'dani', 'szew', ' Zachodnich']\n",
      "Neuron 14: ['uma', 'ierze', 'im', ' GU', 'az', 'iera', 'domu', 'tim', 'RU', 'erma']\n",
      "Neuron 15: ['walność', 'walności', 'niam', 'jącymi', ' bezpłatny', ' klauzu', 'niowa', ' szybkość', 'ngen', ' orzekania']\n",
      "Neuron 16: [' Bas', ' pon', ' pracodawcy', ' bary', 'ach', ' Mount', ' basen', 'garnia', ' przetwo', 'toli']\n",
      "Neuron 17: ['akon', 'akra', ' łożu', 'oper', ' zdrowotnego', ' Beski', 'oku', 'ens', 'biu', 'brze']\n",
      "Neuron 18: ['ung', 'ulf', ' nadrobić', 'atym', 'stra', ' żło', 'dźwi', 'Link', 'Ameryka', 'up']\n",
      "Neuron 19: ['lea', 'atur', 'cin', 'ross', 'le', 'rost', 'plat', 'ary', 'byśmy', 'jech']\n",
      "Neuron 20: ['bra', 'zacyjnego', ' widocznym', ' mocna', 'mine', 'IG', 'iagno', 'miałam', ' brzmieć', ' znacie']\n",
      "Neuron 21: ['ey', 'śni', 'strzy', 'cta', 'a', 'bu', '−', 'eta', 'hi', 'ac']\n",
      "Neuron 22: ['kal', 'bież', 'PIER', 'żąd', 'PIE', ' podstawy', 'puszcza', 'płak', '...]', 'ktry']\n",
      "Neuron 23: ['CH', 'JĄ', 'DM', 'NIK', 'ŁY', 'TC', 'CE', 'ŁE', 'XI', 'HA']\n",
      "Neuron 24: [' mi', ' nam', ' klientowi', 'mato', ' Izbie', ' staremu', ' zawah', 'Nam', ' sobie', ' premierowi']\n",
      "Neuron 25: ['Uśmiech', 'dynki', 'vor', 'niczek', 'tha', 'dynek', 'dym', 'rzeć', 'Kol', 'miej']\n",
      "Neuron 26: [' natomiast', ' oczywiście', ' oczywi', ' zresztą', ' oczywiscie', 'cuję', ' mirki', ' ciekawostki', ' naturalnie', ' też']\n",
      "Neuron 27: [' granat', ' bron', ' powietrzne', ' lot', 'Fran', ' snów', 'Lew', ' swiat', ' powietrz', ' ląd']\n",
      "Neuron 28: ['życie', 'PG', 'aly', 'EC', 'szczać', ' seksualną', ' palenie', ' picie', 'owity', ' spalanie']\n",
      "Neuron 29: ['ałach', 'ase', ' porę', ' ścier', ' matko', ' wotum', ' wykonawczych', 'odor', 'ather', ' klawisz']\n",
      "Neuron 30: ['usi', ' biednego', 'lugi', 'owów', ' podatników', ' kompak', ' czekało', ' cywilnej', ' szanuję', ' wysiąść']\n",
      "Neuron 31: ['ołek', ' Senator', ' nieła', 'atę', 'atyki', ' składzie', 'olony', 'goś', 'aker', ' Pożar']\n",
      "Neuron 32: ['wella', ' staw', '�', 'ank', ' naczy', 'En', ' �', ' ząb', 'Sen', ' Office']\n",
      "Neuron 33: [' prawdzie', 'ości', ' spos', 'łgar', 'leje', '�', 'ktery', ' Łuka', 'legł', 'twier']\n",
      "Neuron 34: ['bowie', 'ktyw', 'wiec', 'wie', 'wczym', 'czeka', 'orze', 'vo', 'zwy', 'wód']\n",
      "Neuron 35: ['rost', 'ceń', 'imy', 'cena', 'tne', ' korek', 'nictwu', ' rodziciel', ' gory', ' szybę']\n",
      "Neuron 36: ['ąż', 'miar', ' szkła', 'OŚĆ', ' ważności', ' hajsu', 'chodzi', 'ersja', ' rozwodu', 'Nowa']\n",
      "Neuron 37: ['tung', ' Bruk', 'kel', 'nai', ' rynki', 'psi', 'dere', 'rencji', ' Gaz', 'Soli']\n",
      "Neuron 38: ['alizmu', 'hire', 'letki', 'siego', 'ome', 'let', 'ard', 'ością', 'alni', 'iste']\n",
      "Neuron 39: [' obrócić', 'larii', ' lewo', ' lob', 'Się', 'Ster', ' skrzyż', ' pw', ' Lena', ' sejmowej']\n",
      "Neuron 40: [' poby', 'ontent', 'roll', ' dotyku', ' wjazdu', 'ink', ' pisania', 'arskiego', 'ing', ' pobytu']\n",
      "Neuron 41: ['——', 'ison', ' dnie', 'tish', 'dyka', 'minu', 'ksel', 'mini', ' Mid', 'ght']\n",
      "Neuron 42: [' sterowanie', ' elektroni', ' baterię', ' rozwiązanie', ' elektry', ' zwro', ' praktyczne', ' praktyczny', ' wersję', ' technika']\n",
      "Neuron 43: ['atry', 'uro', 'rzš', 'prez', ' MŚ', ' dysko', 'rząd', ' kaden', ' kopali', 'grzy']\n",
      "Neuron 44: [' KOR', 'bera', 'dzista', 'sista', ' Zachodzie', ' Fron', 'arze', ' Macierewicz', ' Fal', 'szu']\n",
      "Neuron 45: ['gh', 'vas', 'dis', 'kwi', 'dium', 'dian', ' maci', 'cjum', 'dia', 'Ve']\n",
      "Neuron 46: ['jsku', 'fery', 'lika', 'fotograf', 'ździe', 'ręt', 'gis', 'mawiać', 'jazdy', 'sier']\n",
      "Neuron 47: [' zapomina', 'dalem', ' rekordy', ' kosztem', ' zdać', ' Lasów', ' zama', ' przegrać', ' Festi', ' zaciera']\n",
      "Neuron 48: [' powietrznej', ' wieczystych', ' dopy', ' wno', ' receptę', 'minutowych', ' Rehabili', 'tedy', 'szep', ' indywidu']\n",
      "Neuron 49: [' Salo', ' suk', ' Konrada', ' Ach', ' kółkach', ' czem', ' Krasi', ' boku', ' katalo', 'arian']\n",
      "Neuron 50: [' Colin', ' Nico', ' Andrze', ' Gordon', ' Kacz', ' OP', ' Doug', ' Ita', 'hit', 'simum']\n",
      "Neuron 51: [' stopnia', 'bis', 'ý', ' głębokości', 'niczymi', 'ggi', 'jki', 'niczy', 'gt', 'lety']\n",
      "Neuron 52: ['Trump', 'skowe', ' agencje', 'wiecie', 'stok', 'dle', 'ądź', ' obywatelom', 'wódz', ' bit']\n",
      "Neuron 53: ['ity', 'nity', 'typ', 'tką', 'towa', 'ŁA', 'rola', 'fikowana', 'łeć', 'łowa']\n",
      "Neuron 54: ['ąłem', ' czekałem', 'siadłem', ' oczywiscie', ' spust', 'ncji', 'owicie', ' wątpliwość', ' reka', ' dyskusje']\n",
      "Neuron 55: ['mon', 'ówki', ' dwudziesty', 'rie', 'amencie', 'ł', 'arną', ' Square', 'szyst', 'owskiego']\n",
      "Neuron 56: [' publicznego', ' obywatelskiego', ' wschodniego', ' słonecznego', ' lekarskiego', ' maszynowego', ' terytorialnego', ' emerytalnego', ' telewizyjnego', ' technicznego']\n",
      "Neuron 57: ['znaj', ' szkolne', 'uwa', 'iller', ' odkryte', ' dowolne', ' telewizora', 'Seria', ' zakazane', 'jonal']\n",
      "Neuron 58: ['wel', 'ew', 'well', ' strzy', 'eck', 'órka', 'bj', 'typ', ' zgad', 'zmy']\n",
      "Neuron 59: ['seum', 'technika', ' wnik', ' mechani', ' manewr', 'nek', 'ceum', 'dal', 'owz', ' spowal']\n",
      "Neuron 60: ['lą', ' winę', 'wątp', ' nieroz', '\\x04', ' nao', ' zasłu', ' nozd', ' odszkod', 'adów']\n",
      "Neuron 61: ['gracji', 'zań', ' pobierania', 'śników', ' pobierać', ' treści', 'nacją', ' Gmin', ' spójności', 'Py']\n",
      "Neuron 62: ['syw', 'dera', 'yne', ' mikrofon', ' archa', ' baterie', 'zara', 'derów', 'tena', 'ise']\n",
      "Neuron 63: [' bet', ' galo', ' zbiega', 'robi', ' odbi', ' przepeł', ' let', ' dziś', ' wczoraj', 'ple']\n",
      "Neuron 64: ['sku', 'cien', 'ologicznej', 'ku', 'isz', 'cu', 'życie', 'lanu', 'cian', 'kuli']\n",
      "Neuron 65: [' poda', ' wil', ' nad', ' naje', ' test', ' mamy', ' zacho', ' adi', ' A', ' li']\n",
      "Neuron 66: ['nowskich', 'nień', 'Wszystkich', 'stów', 'dowy', 'dowód', 'Komitet', 'dyka', 'stami', 'dle']\n",
      "Neuron 67: ['lut', 'lasz', 'kraj', 'sla', 'wnego', 'raża', 'zolu', 'kłada', 'lu', 'ruz']\n",
      "Neuron 68: [' Sztuk', ' wysłucha', ' konsultacjach', 'czi', ' Spring', ' ziele', ' Broad', 'dziem', 'nii', ' stanów']\n",
      "Neuron 69: [' pomiarów', 'powi', ' palenia', ' medal', ' przebywających', ' przesiad', ' odbić', ' kontu', ' ucieczki', 'zgi']\n",
      "Neuron 70: ['niną', 'ninę', 'tum', ' zdr', ' kopy', 'nko', 'cke', 'grody', 'pne', 'mand']\n",
      "Neuron 71: [' domem', ' domami', ' willi', ' sprzedaży', 'hala', ' zruj', ' łączna', ' nieruchomościami', ' Wenecji', ' helikop']\n",
      "Neuron 72: ['ozę', 'dzącego', 'ponować', 'sobie', ' złośliwy', 'pozy', 'miać', 'yer', 'dzkę', ' barwę']\n",
      "Neuron 73: ['bica', 'tnia', 'Amerykanie', 'anowa', 'nacją', 'Nowi', 'rzyna', 'Osoba', 'dago', 'ógł']\n",
      "Neuron 74: ['śnie', 'ćmi', 'y', 'Ły', 'jąc', 'wisko', 'bas', 'nia', 'yn', ' @']\n",
      "Neuron 75: ['house', 'kry', 'żynie', 'ś', ' rolnicze', 'ole', ' liceum', 'ałoby', 'san', 'rza']\n",
      "Neuron 76: ['ceae', 'iew', 'bytu', 'śników', 'biny', 'śnika', 'berga', 'dnienia', 'lesa', 'stacji']\n",
      "Neuron 77: ['Ali', ' Generalnego', 'usi', ' chodziło', ' trwać', ' koloru', 'tch', 'kołaj', ' zajmować', ' odległe']\n",
      "Neuron 78: ['luch', 'mb', ' szarości', ' kompleksów', 'wicę', 'leks', 'rzały', 'niałe', 'łow', ' licznik']\n",
      "Neuron 79: [' piersiowej', ' paznok', ' komórkowych', ' środkowym', ' teat', ' przezię', ' telefonem', 'ległej', ' Sach', 'cowego']\n",
      "Neuron 80: [' nawiąz', 'hire', 'zeli', 'żon', ' najcieka', ' kokos', ' Warren', 'fort', ' odnoszą', 'lender']\n",
      "Neuron 81: ['nienie', 'minacji', 'aryj', 'naryj', ' cięcie', ' cięcia', 'play', 'ão', ' Gą', 'nację']\n",
      "Neuron 82: ['der', 'zyk', 'danie', 'и�', ' zdrowie', 'nięcie', 'HC', 'deł', ' kola', 'dyka']\n",
      "Neuron 83: [' zdumiony', ' zadowoleni', ' zdania', ' oszołomiony', ' zdumienia', ' zadowolony', ' posiadania', ' oczekiwaniom', ' właścicielowi', ' obaw']\n",
      "Neuron 84: ['Jona', 'leja', 'talnego', 'ggy', 'talnie', 'tet', 'talnym', 'talnej', ' hon', 'talną']\n",
      "Neuron 85: ['est', ' Nathan', 'kun', 'pę', 'stal', 'ręd', 'hon', 'dore', ' Alla', ' zasadność']\n",
      "Neuron 86: ['tki', 'ales', 'lotte', ' pobud', ' awaryj', 'с', 'ск', ' wyjści', 'granicznych', 'łeś']\n",
      "Neuron 87: [' Ibi', ' Vil', ' ZD', ' Road', 'strz', ' Wilk', ' Dur', ' Arkadi', 'hh', ' Lewand']\n",
      "Neuron 88: [' rozdz', ' wdzie', 'dziesiątych', 'batą', ' popiel', ' szer', ' merytoryczną', 'ue', ' dorze', ' Konsu']\n",
      "Neuron 89: ['tarza', ' mna', 'lego', 'Des', 'von', 'CJE', 'wzię', 'tez', 'feren', 'ise']\n",
      "Neuron 90: ['taku', ' powtórzę', 'podoba', 'ë', ' propos', 'ū', ' doku', 'roda', 'ordzie', 'simum']\n",
      "Neuron 91: [' States', ' Die', 'Kal', 'tyków', 'godne', 'rią', ' Church', 'niczą', 'Cal', ' sowieckiego']\n",
      "Neuron 92: ['went', 'tywny', 'tywnych', 'aris', ' Krajowy', 'urnal', ' przelot', 'tywne', 'gger', 'ping']\n",
      "Neuron 93: ['scia', 'rda', 'gacz', ' prezenty', ' het', 'rządy', ' kult', ' Baran', 'kup', ' kartki']\n",
      "Neuron 94: ['czkowski', 'ezer', 'łaki', 'larami', 'nitor', 'mait', ' nerki', ' fl', 'lar', 'niczce']\n",
      "Neuron 95: ['raktery', 'rosław', 'Świ', 'dziej', 'gip', '�', ' powiedzia', 'Zostało', 'ostęp', ' głębokiej']\n",
      "Neuron 96: [' niecier', ' przeciwne', ' jał', ' wodnego', ' niepewna', ' wymagające', ' rozbro', ' nieprzyjem', ' przeciwnie', ' celne']\n",
      "Neuron 97: ['leckim', ' liśćmi', 'kowskim', 'rzynie', ' organami', ' wojskiem', 'ówkami', ' wełny', 'cerze', 'ląkł']\n",
      "Neuron 98: [' emigracji', 'm', ' POLI', 'osu', 'AS', ' wskazanym', 'Soli', ' komisariatu', 'obby', ' jubile']\n",
      "Neuron 99: ['terii', ' mez', 'cla', 'cien', 'cal', 'teria', 'ceum', 'sla', 'ksa', 'te']\n",
      "\n",
      "Feed-forward keys (Layer 0, first 5 neurons):\n",
      "Neuron 0: ['uel', 'ul', 'uela', 'pul', 'gul', 'ann', ' bel', 'jal', 'cht', 'oll']\n",
      "Neuron 1: [' pracę', ' praca', ' Pracy', ' straż', ' urząd', ' pracy', ' służbę', ' zawody', ' zwolnienie', ' zawod']\n",
      "Neuron 2: [' bestsel', ' science', ' perspek', ' Doug', ' magazynu', 'Micha', ' zapotrzeb', ' jacy', ' wystarczający', ' 1949']\n",
      "Neuron 3: ['ię', 'mę', 'olę', 'głość', 'rę', ' Mę', ' mę', 'wię', ' zamę', ' ciszę']\n",
      "Neuron 4: ['taka', 'zolu', ' taką', ' rozumiemy', ' uważasz', 'zumiem', 'głeś', ' uważacie', ' taka', 'FC']\n",
      "Neuron 5: [' zakłócenia', ' nadaw', 'Nak', ' kres', ' zakłóceń', ' reklamy', ' emu', ' transmisji', 'nak', 'has']\n",
      "Neuron 6: [' pota', ' flu', ' skoja', ' Edi', ' połów', 'Time', ' radzieckiej', ' Stuart', ' obligatory', ' Zamoy']\n",
      "Neuron 7: [' dziewi', ' niemiłosier', ' błogosła', ' nieska', ' nieczy', ' zabro', 'szpa', ' recy', 'księż', ' klasz']\n",
      "Neuron 8: ['nno', 'nny', ' rozpoczęło', 'nność', 'nnej', 'czątk', 'syw', 'nną', 'nnego', ' przerwa']\n",
      "Neuron 9: ['ory', 'orem', 'orą', 'ór', 'oru', 'orne', 'orami', 'olę', 'óc', 'olą']\n",
      "Neuron 10: ['gar', 'mię', 'mina', 'monia', ' wymie', 'wyż', 'mi', ' opróż', 'ymi', 'wet']\n",
      "Neuron 11: [' hamul', 'chin', ' klucza', 'arion', 'sera', 'ser', 'wice', ' atmosfery', 'myki', 'szyna']\n",
      "Neuron 12: [' dziewięćdziesiąt', ' osiemdziesiąt', ' awaryj', ' siedemdziesiąt', ' dziesiątej', ' czterdzieści', 'Dziesięć', 'reczki', 'Harry', 'tywa']\n",
      "Neuron 13: ['brak', ' wykorzystywania', ' wykorzyst', ' wykorzystywanie', ' wykorzystaniu', ' odebra', ' wykorzysty', ' wykorzystanie', 'borze', ' wykorzystania']\n",
      "Neuron 14: [' szantaż', 'ude', 'rory', 'kument', 'film', 'cking', 'cjali', 'nitor', 'dens', 'óbr']\n",
      "Neuron 15: ['laj', 'tuj', 'duj', 'zuj', 'ruj', ' czekaj', 'jmy', 'cznij', 'zaj', 'wórz']\n",
      "Neuron 16: ['pisie', 'unkach', ' poukład', 'dziesią', ' odciski', 'lacjach', 'nięte', 'dzieje', 'zumie', 'rodku']\n",
      "Neuron 17: ['Głosowanie', 'noty', ' Niepełnosprawnych', ' wstrzymało', ' zasiłków', ' AIDS', ' zaka', ' zasił', ' Social', ' Jem']\n",
      "Neuron 18: ['bior', ' Bus', 'laz', ' dostaw', ' Wis', ' osobowy', ' zbior', ' Leg', 'siad', 'granicz']\n",
      "Neuron 19: [' służbowej', 'wicz', ' miano', ' podob', 'itt', 'role', 'Nau', 'nictwo', 'niono', 'manie']\n",
      "Neuron 20: ['rat', ' wpuszcz', 'ialu', ' wyku', ' zaopat', ' sprzedaw', ' zaopatry', ' klon', ' wypuszcz', 'urat']\n",
      "Neuron 21: [' błagać', ' błaga', ' wykorzy', ' uzdol', ' MINIST', ' chary', ' umiesz', ' poprosi', 'hry', ' umieję']\n",
      "Neuron 22: [' |', 'cjo', '<', 'Q', ' 168', ' 176', ' cmen', ' pia', 'ţ', ' /']\n",
      "Neuron 23: ['rzymie', '\";', 'bowiem', '”;', ' Babi', 'bo', ' ówczesnym', ');', ';', 'cyt']\n",
      "Neuron 24: [' Barce', ' Barcel', ' Bazy', ' jedno', ' Monte', ' Akwi', 'jed', 'Mag', 'Bor', 'jedno']\n",
      "Neuron 25: [' monarchii', ' łączną', ' sumie', ' bogat', ' uzasadnionych', ' Księstwa', ' dziewięciu', ' większ', ' małej', ' miesz']\n",
      "Neuron 26: ['bła', 'BO', ' wszak', 'inąd', 'mine', 'bowiem', 'bytków', 'przecież', ' Bog', 'dźmy']\n",
      "Neuron 27: [' płynąć', ' płyną', ' żegl', ' płynęły', ' płynących', ' płynie', ' płynące', ' dopły', ' sze', 'hand']\n",
      "Neuron 28: [' Edwar', ' aw', ' bor', ' Andrew', ' Edwarda', 'worem', 'woru', 'field', 'wychw', 'vor']\n",
      "Neuron 29: ['135', '45', '05', '12', '60', '105', '10', '55', '66', '85']\n",
      "Neuron 30: ['rem', 'rusem', 'dem', 'ntem', 'mem', 'EM', 'łowem', 'chem', ' sensem', 'asem']\n",
      "Neuron 31: ['śli', 'sli', 'illi', 'eli', 'iali', ' urlo', 'wili', 'żali', 'żeli', 'łaj']\n",
      "Neuron 32: ['grad', 'ial', '+', ' zastęp', ' buduje', 'Zamiast', ' zastępuje', ' pomost', ' zastępu', 'bet']\n",
      "Neuron 33: ['czaj', ' przeze', 'czek', ' cier', ' dziec', ' karą', ' przez', 'anse', 'przez', 'dziec']\n",
      "Neuron 34: ['sort', 'rael', 'lesty', 'nianie', 'ńsk', 'nial', 'niasz', 'dae', 'mac', 'tle']\n",
      "Neuron 35: ['atach', 'arach', 'tacjach', 'das', 'ździe', 'cach', '76', ' książkach', 'dziesiątych', 'AS']\n",
      "Neuron 36: ['jo', 'rich', 'Rad', ' rad', 'sch', 'ring', 'hin', 'chan', 'rada', 'rey']\n",
      "Neuron 37: ['day', ' warianty', ' możliwa', 'rola', ' plus', ' możliwe', ' deklaruje', ' możliwy', 'ray', 'vo']\n",
      "Neuron 38: [' Grupy', 'śnięcie', ' Grupa', ' grupy', 'śnięcia', 'Grupa', ' grupa', 'Połączenie', 'czący', 'dzielił']\n",
      "Neuron 39: [' nad', ' nade', 'nad', ' przerzu', 'far', ' multi', 'Nad', ' ponad', ' transp', ' prom']\n",
      "Neuron 40: ['TK', 'hon', 'cebo', 'arian', ' AI', ' sercem', ' medycyny', ' religii', 'pł', 'eli']\n",
      "Neuron 41: [' Eng', ' E', ' ewangeli', ' Engel', ' Eden', ' Mazur', ' ES', ' Holend', ' Thom', ' Evan']\n",
      "Neuron 42: ['wiane', 'wiało', 'wiać', 'wianie', 'wiały', 'wiał', 'wiasz', 'wiałam', 'wienie', 'wiam']\n",
      "Neuron 43: ['ódz', 'stce', 'kaz', 'kazać', 'obrze', 'Białej', '–', '‒', ' łatwością', 'rzyść']\n",
      "Neuron 44: ['arze', 'ariu', ' zakładać', ' zakładają', 'owier', ' zakła', 'ariusze', 'gady', ' zakłada', 'zory']\n",
      "Neuron 45: [' gotowość', ' gotow', ' gotów', ' gotowości', ' Chę', 'war', ' pogotowiu', 'War', ' Milo', ' rado']\n",
      "Neuron 46: [' poprawek', ' swiet', 'Świet', 'ormal', 'tical', 'tykal', ' wyznaje', 'cjal', ' poprawkę', ' kocha']\n",
      "Neuron 47: [' przekonaniu', ' takiemu', 'ieniu', ' pojęciu', ' zapytaniu', ' zadaniu', ' Gos', ' fakcie', ' poczuciu', ' zdje']\n",
      "Neuron 48: ['tura', 'kwi', 'turze', 'turę', 'turą', ' gwizd', ' cyfry', 'tury', ' kontem', 'czba']\n",
      "Neuron 49: ['ple', ' kolo', ' Kolo', 'wychw', 'yme', ' kołem', ' kole', ' zespołach', ' ple', ' pale']\n",
      "Neuron 50: ['jskiej', 'jskiego', 'cowej', 'arskiej', 'arskiego', 'й', 'ńskiej', 'niowego', 'nowskiej', 'niowej']\n",
      "Neuron 51: [' przedw', ' przedwcześnie', 'ciw', ' przeciw', ' nad', 'przeciw', 'Nad', ' Nowo', ' Nad', ' późno']\n",
      "Neuron 52: [' niedźwie', ' Niedźwie', ' Gro', 'Gro', ' gró', 'dziei', 'szuki', ' Łom', ' Śle', ' Beski']\n",
      "Neuron 53: ['bó', ' Wene', ' Buł', ' litur', ' wene', ' patriar', ' Moł', ' kapłani', ' kapł', ' ozd']\n",
      "Neuron 54: ['kordat', 'y', '92', 'ye', 'ety', 'ars', ' uczciwość', 'oy', 'xit', 'Polityka']\n",
      "Neuron 55: ['Wyborcza', 'blo', 'Wyborczej', ' ante', 'logu', ' dowiem', 'roda', 'Wyborczą', 'BN', ' Blo']\n",
      "Neuron 56: ['len', 'anth', 'ksz', 'lat', 'andi', 'sztal', ' dziejów', ' Gli', 'lend', 'asu']\n",
      "Neuron 57: [' uderzy', ' Waldem', ' uderzyła', ' uderza', ' uderzyło', ' Jame', ' mroż', ' uderzyły', ' pułkow', ' spotka']\n",
      "Neuron 58: ['ward', 'Matt', 'vat', 'don', ' dane', 'waran', 'czasu', 'cor', 'Ofer', ' wspom']\n",
      "Neuron 59: ['ce', 'cena', 'ceń', 'gieł', 'CE', 'cer', 'strzeżenie', 'gło', 'cep', 'cen']\n",
      "Neuron 60: [' kolejka', 'ved', ' stac', 'imer', 'tum', 'watch', 'isen', 'atysfakcjon', ' klientem', 'sługi']\n",
      "Neuron 61: [' podro', ' *', ' przystaw', ' postra', 'przyp', ' pomo', ' podkra', ' podo', 'Kra', ' sm']\n",
      "Neuron 62: [' naro', ' wyro', 'rone', ' rodzą', ' rodzić', 'eł', 'roś', ' zro', ' powstać', ' rówie']\n",
      "Neuron 63: [' nierówności', ' zniesienia', ' upadnie', ' upadek', ' upadła', ' upadł', ' upada', ' plamę', ' rozróżnić', ' zniesienie']\n",
      "Neuron 64: [' podstawi', ' sform', ' wyhod', ' wyrabi', ' złożone', ' sfałsz', ' wyrabia', ' utworzone', ' utworzona', ' złożonego']\n",
      "Neuron 65: [' pachnie', ' piją', ' grają', ' sądzą', ' pracują', ' powiedzą', ' mogłabym', ' myślimy', ' czują', ' robię']\n",
      "Neuron 66: ['kur', 'kura', ' Kory', ' sztuce', ' bóg', ' kura', ' Kun', ' róży', ' Mah', ' sztuka']\n",
      "Neuron 67: ['scie', 'byście', 'ujecie', 'ście', 'cie', 'wiecie', ' byście', 'owaliście', 'liście', ' żebyście']\n",
      "Neuron 68: [' skalę', ' szacunków', ' Obamy', ' mali', ' rasizm', ' statystyka', ' Ukraińcy', 'dencja', 'dług', 'dać']\n",
      "Neuron 69: [' 2005', ' 2008', '2005', ' 2006', ' AWS', '2009', 'Polityka', ' 2015', ' 2004', ' 2009']\n",
      "Neuron 70: [' wypowiad', ' nazew', ' mno', ' genera', ' itp', ' termino', ' języki', ' rozchodzi', ' formu', ' Mamo']\n",
      "Neuron 71: ['onę', 'ONA', 'onek', ' Księ', 'dion', 'Księ', 'ons', 'ON', 'enos', 'onów']\n",
      "Neuron 72: ['tyk', 'Pil', ' Pia', ' zakra', ' chodzi', ' szura', ' Szu', ' sięg', ' dotyka', ' chodziło']\n",
      "Neuron 73: ['bab', ' FA', 'bac', 'bak', 'FC', 'nice', 'bala', ' Bal', ' Bag', 'Bal']\n",
      "Neuron 74: [' wojennym', 'bory', ' wojennego', ' woje', ' wojenny', ' rzeczy', ' skandalu', ' Krzyszto', ' aferę', ' czegos']\n",
      "Neuron 75: ['hausen', 'hady', ' analo', ' trybuny', ' temp', 'sym', ' wir', ' przystaw', ' fast', ' czwora']\n",
      "Neuron 76: ['od', ' OD', 'Od', ' od', ' Od', ' odp', ' odsy', ' ode', 'OD', ' odm']\n",
      "Neuron 77: ['pe', 'pos', 'polu', 'pet', 'pety', 'po', ' Lubli', 'bez', 'pot', 'pel']\n",
      "Neuron 78: ['zesa', 'uset', 'cket', ' ˛', 'obior', ' Egi', 'ntrol', 'ring', 'ót', 'ceu']\n",
      "Neuron 79: [' wartość', ' twórczość', ' jesień', ' sierść', ' cześć', ' całość', ' młodość', ' sól', ' wieść', 'gość']\n",
      "Neuron 80: [' Janka', 'She', ' Lou', 'Lou', 'Dan', ' znajdujących', ' Fri', '�', '”[', '&']\n",
      "Neuron 81: [' Berna', ' Urba', 'skar', 'chta', ' kaz', ' skar', 'zar', 'Staro', 'boar', 'stora']\n",
      "Neuron 82: [' stażem', ' dostępem', ' stopniem', ' nożem', ' otworem', ' śladem', ' rączkę', ' palcem', ' rękojeść', ' łapę']\n",
      "Neuron 83: [' wa', ' cenie', 'kali', 'cen', 'cena', 'cyd', ' cen', 'płata', 'data', 'ceń']\n",
      "Neuron 84: [' byli', ' byliśmy', ' była', 'Byli', 'były', ' Była', 'była', ' należało', ' ówczes', ' poprzedniego']\n",
      "Neuron 85: ['ok', 'K', ' przek', ' wyk', ' k', ' Kenne', 'Kate', 'Ok', ' K', 'oku']\n",
      "Neuron 86: ['Izrael', 'mund', 'adowi', ' Albanii', ' Izraelu', ' Izraela', 'tusa', 'adka', 'tus', 'adem']\n",
      "Neuron 87: ['syw', 'gent', 'skow', 'trop', 'cjone', 'bak', 'lerow', 'toli', 'cyd', 'dak']\n",
      "Neuron 88: [' trzykrotnie', ' czterokrotnie', ' dyp', ' kro', ' greckiej', ' jądra', 'gre', ' gre', ' dwukrotnie', ' Pary']\n",
      "Neuron 89: [' stron', ' strony', ' stronami', 'wiek', ' stronach', 'stron', ' stroną', ' poważniejsze', ' newsletter', 'Strona']\n",
      "Neuron 90: [' Atenach', ' wtym', ' miesiącu', ' dodatku', ' Wietnamie', ' Przemyślu', ' gryzie', ' temu', ' cyklu', ' wogóle']\n",
      "Neuron 91: ['rówki', 'onoś', 'źnier', 'eryk', 'obry', 'rówka', 'utko', 'łgar', 'hry', 'cegowi']\n",
      "Neuron 92: ['rój', 'try', ' Artem', 'ód', 'rót', 'ściu', 'rodę', 'stąpienie', 'ść', ' List']\n",
      "Neuron 93: ['leży', 'ppo', 'obrze', 'urem', 'ura', 'var', 'para', 'vara', 'Pole', 'vena']\n",
      "Neuron 94: ['imu', 'cebo', 'ad', 'środ', 'ionu', 'Ad', 'akija', ' pion', 'Mas', 'ymu']\n",
      "Neuron 95: [' GU', 'DU', ' chrztu', 'G', ' chału', ' ch', ' zadu', 'penha', 'tro', 'stro']\n",
      "Neuron 96: ['owicze', 'owiny', ' NIK', 'owic', 'Media', 'Najwyższa', ' kontrolne', 'winy', ' Inspekcji', 'granicznych']\n",
      "Neuron 97: ['macie', 'AM', ' przekazują', 'AMY', ' przynosić', 'Macie', ' przynoszą', ' wioz', ' przynosi', ' przekazu']\n",
      "Neuron 98: [' dotyczy', 'Doty', ' dotyczą', 'Kon', ' dotyczyło', 'lega', ' tyczy', ' Ds', 'chii', 'doty']\n",
      "Neuron 99: ['aktu', 'tałam', 'odź', 'Dor', 'tanii', 'tałem', 'kali', 'tał', 'zawo', 'lizowała']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Feed-forward values (Layer 3, first 5 neurons):\")\n",
    "for neuron_info in ff_values[3]['neurons'][:100]:\n",
    "    print(f\"Neuron {neuron_info['neuron']}: {neuron_info['top_tokens']}\")\n",
    "\n",
    "print(\"\\nFeed-forward keys (Layer 0, first 5 neurons):\")\n",
    "for neuron_info in ff_keys[0]['neurons'][:100]:\n",
    "    print(f\"Neuron {neuron_info['neuron']}: {neuron_info['top_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_heads(model, E, tokenizer, k=10):\n",
    "    attention_heads = []\n",
    "    n_heads = model.config.n_head  # Number of attention heads\n",
    "    head_dim = model.config.n_embd // n_heads  # Dimension per head\n",
    "\n",
    "    for layer_idx, block in enumerate(tqdm(model.h, desc=\"Processing attention layers\")):\n",
    "        attn = block.attn\n",
    "        # Get the combined QKV weights\n",
    "        Wqkv = attn.c_attn.weight.data  # shape: (model_dim, 3 * model_dim)\n",
    "        Wqkv = Wqkv.cpu()\n",
    "\n",
    "        # Split into Wq, Wk, Wv\n",
    "        Wq = Wqkv[:, :model.config.n_embd]\n",
    "        Wk = Wqkv[:, model.config.n_embd:2 * model.config.n_embd]\n",
    "        Wv = Wqkv[:, 2 * model.config.n_embd:]\n",
    "\n",
    "        # Get Wo\n",
    "        Wo = attn.c_proj.weight.data  # shape: (model_dim, model_dim)\n",
    "        Wo = Wo.cpu()\n",
    "\n",
    "        # Reshape weights to separate heads\n",
    "        Wq = Wq.view(model.config.n_embd, n_heads, head_dim)  # (1024, 16, 64)\n",
    "        Wk = Wk.view(model.config.n_embd, n_heads, head_dim)\n",
    "        Wv = Wv.view(model.config.n_embd, n_heads, head_dim)\n",
    "        Wo = Wo.view(n_heads, head_dim, model.config.n_embd)   # (16, 64, 1024)\n",
    "\n",
    "        for head_idx in range(n_heads):\n",
    "            Wq_head = Wq[:, head_idx, :]  # shape: (1024, 64)\n",
    "            Wk_head = Wk[:, head_idx, :]\n",
    "            Wv_head = Wv[:, head_idx, :]\n",
    "            Wo_head = Wo[head_idx, :, :]  # shape: (64, 1024)\n",
    "\n",
    "            # Project Wq_head and Wk_head into embedding space\n",
    "            Wq_proj = E @ Wq_head  # shape: (vocab_size, head_dim)\n",
    "            Wk_proj = E @ Wk_head  # shape: (vocab_size, head_dim)\n",
    "\n",
    "            # Compute top-k token pairs for W_QK\n",
    "            tokens_QK = []\n",
    "            for i in range(head_dim):\n",
    "                wq = Wq_proj[:, i]  # shape: (vocab_size,)\n",
    "                wk = Wk_proj[:, i]  # shape: (vocab_size,)\n",
    "\n",
    "                # Get top-k indices\n",
    "                topk_wq_values, topk_wq_indices = torch.topk(wq, k=k)\n",
    "                topk_wk_values, topk_wk_indices = torch.topk(wk, k=k)\n",
    "\n",
    "                # Compute contributions for top-k tokens only\n",
    "                for idx_q, val_q in zip(topk_wq_indices, topk_wq_values):\n",
    "                    for idx_k, val_k in zip(topk_wk_indices, topk_wk_values):\n",
    "                        contribution = val_q.item() * val_k.item()\n",
    "                        tokens_QK.append((tokenizer.decode([idx_q.item()]), tokenizer.decode([idx_k.item()]), contribution))\n",
    "            # Sort and select top-k\n",
    "            tokens_QK = sorted(tokens_QK, key=lambda x: abs(x[2]), reverse=True)[:k]\n",
    "\n",
    "            # Similarly for W_VO\n",
    "            # Project Wv_head and Wo_head into embedding space\n",
    "            Wv_proj = E @ Wv_head  # shape: (vocab_size, head_dim)\n",
    "            Wo_proj = Wo_head @ E.T  # shape: (head_dim, vocab_size)\n",
    "\n",
    "            tokens_VO = []\n",
    "            for i in range(head_dim):\n",
    "                wv = Wv_proj[:, i]  # shape: (vocab_size,)\n",
    "                wo = Wo_proj[i, :]  # shape: (vocab_size,)\n",
    "\n",
    "                # Get top-k indices\n",
    "                topk_wv_values, topk_wv_indices = torch.topk(wv, k=k)\n",
    "                topk_wo_values, topk_wo_indices = torch.topk(wo, k=k)\n",
    "\n",
    "                # Compute contributions for top-k tokens only\n",
    "                for idx_v, val_v in zip(topk_wv_indices, topk_wv_values):\n",
    "                    for idx_o, val_o in zip(topk_wo_indices, topk_wo_values):\n",
    "                        contribution = val_v.item() * val_o.item()\n",
    "                        tokens_VO.append((tokenizer.decode([idx_v.item()]), tokenizer.decode([idx_o.item()]), contribution))\n",
    "            # Sort and select top-k\n",
    "            tokens_VO = sorted(tokens_VO, key=lambda x: abs(x[2]), reverse=True)[:k]\n",
    "\n",
    "            attention_heads.append({\n",
    "                'layer': layer_idx,\n",
    "                'head': head_idx,\n",
    "                'W_QK_top_tokens': tokens_QK,\n",
    "                'W_VO_top_tokens': tokens_VO\n",
    "            })\n",
    "    return attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attention layers: 100%|██████████| 24/24 [20:36<00:00, 51.52s/it]\n"
     ]
    }
   ],
   "source": [
    "attention_data = extract_attention_heads(model, E, tokenizer, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Head (Layer 0, Head 0) W_QK top token pairs:\n",
      "('ałam', 'rzałam', 0.1122945284010024)\n",
      "('ałam', 'rzałem', 0.11090440780752608)\n",
      "('ałam', 'lałem', 0.1105061935700613)\n",
      "('ałam', 'siadłem', 0.11007357352015745)\n",
      "('ałam', 'siadłam', 0.10937657869586115)\n",
      "('ałam', 'żyłem', 0.10925697906098542)\n",
      "('ałam', 'żyłam', 0.10824086712875047)\n",
      "('ałem', 'rzałam', 0.10775755608316562)\n",
      "('ałem', 'rzałem', 0.10642359974578319)\n",
      "('ałem', 'lałem', 0.10604147433292699)\n",
      "('ałyśmy', 'rzałam', 0.10600535208626916)\n",
      "('ałem', 'siadłem', 0.10562633318621195)\n",
      "('ałem', 'siadłam', 0.10495749864958537)\n",
      "('ałem', 'żyłem', 0.10484273113111264)\n",
      "('ałyśmy', 'rzałem', 0.10469308669763322)\n",
      "('ałam', 'cisnąłem', 0.10441236781058016)\n",
      "('ałyśmy', 'lałem', 0.10431717487851522)\n",
      "('ałyśmy', 'siadłem', 0.10390878418163396)\n",
      "('ałem', 'żyłam', 0.10386767259456864)\n",
      "('ałyśmy', 'siadłam', 0.10325082530506258)\n",
      "('ałyśmy', 'żyłem', 0.10313792397687749)\n",
      "('ałam', 'rzyłem', 0.10297483796873674)\n",
      "('ałyśmy', 'żyłam', 0.10217872049056886)\n",
      "('ałam', 'chnąłem', 0.10193185582176856)\n",
      "('ałam', 'niosłem', 0.10184712031715559)\n",
      "('ałam', ' podniosłem', 0.10138218353140438)\n",
      "('ałem', 'cisnąłem', 0.10019385396897285)\n",
      "('ałam', 'owałaś', 0.09991342412622561)\n",
      "('ałam', ' wyciągnąłem', 0.09938721131330208)\n",
      "('ałam', 'wałam', 0.09929138025393414)\n",
      "('ałam', 'biegłem', 0.09882065587333955)\n",
      "('ałem', 'rzyłem', 0.09881440383227069)\n",
      "('ałam', 'wałem', 0.09876237489985229)\n",
      "('ałyśmy', 'cisnąłem', 0.09856464041058999)\n",
      "('ałam', ' otworzyłem', 0.09852774814978371)\n",
      "('ałam', 'ciłam', 0.09831139016133861)\n",
      "('ałem', 'chnąłem', 0.09781356070308167)\n",
      "('ałem', 'niosłem', 0.09773224871913566)\n",
      "('ałam', 'niosłam', 0.09745336590261555)\n",
      "('ałam', ' odpowiedziałam', 0.09740559426341111)\n",
      "('ałam', 'szyłem', 0.09730158809309497)\n",
      "('ałem', ' podniosłem', 0.09728609651137354)\n",
      "('ałyśmy', 'rzyłem', 0.09720762097973257)\n",
      "('ałam', 'owałam', 0.09685611382701964)\n",
      "('ałam', ' odpowiedziałem', 0.0968306505414196)\n",
      "('ałam', 'rzyłam', 0.09645810210208161)\n",
      "('ałam', 'chnęłam', 0.09637965514435187)\n",
      "('znałam', 'rzałam', 0.09632267304271469)\n",
      "('ałam', 'owałem', 0.09624277799337833)\n",
      "('ałyśmy', 'chnąłem', 0.09622305217408034)\n",
      "\n",
      "Attention Head (Layer 0, Head 0) W_VO top token pairs:\n",
      "(' hrabina', 'ies', 0.004754357342297955)\n",
      "(' panienka', 'ies', 0.0046424166854071025)\n",
      "('zwykopem', ' czułam', 0.004445249265383477)\n",
      "('staje', 'lem', 0.004373431606988359)\n",
      "(' zastępuje', 'lem', 0.0043588080454989575)\n",
      "(' tem', 'lem', 0.004356411213738254)\n",
      "('zwykopem', ' znalazłam', 0.004307355779912703)\n",
      "('zwykopem', ' odpowiedziałam', 0.00427392728703313)\n",
      "('Katarzyna', 'ies', 0.004259244413313101)\n",
      "('zwykopem', ' mogłam', 0.004256318156054983)\n",
      "('zwykopem', ' pomyślałam', 0.004248022617652558)\n",
      "('rywa', 'lem', 0.004140914491399944)\n",
      "(' Madame', 'ies', 0.004096488402951626)\n",
      "(' Kasia', 'ies', 0.004084817182554129)\n",
      "('zwykopem', ' musiałam', 0.0040730430090606395)\n",
      "('iłam', ' doczekał', 0.004071226614009715)\n",
      "('niosłam', 'lem', 0.004056931194727442)\n",
      "('zwykopem', ' wzięłam', 0.004053366024984628)\n",
      "(' okazuje', 'lem', 0.004038528680326725)\n",
      "(' obywate', ' czułam', 0.004025770708568355)\n",
      "('nikowa', 'lem', 0.004025370318308463)\n",
      "('zwykopem', ' on', 0.004024910664839343)\n",
      "('zwykopem', ' byłabym', 0.004018469019911208)\n",
      "(' wracają', 'lem', 0.004006137464971554)\n",
      "('zwykopem', ' próbowałam', 0.004003090275157439)\n",
      "(' dama', 'ies', 0.003992987217093902)\n",
      "('_:', ' czułam', 0.003980424101923674)\n",
      "('staje', ' poczułem', 0.003964594434388985)\n",
      "(' zastępuje', ' poczułem', 0.003951337912805525)\n",
      "(' tem', ' poczułem', 0.003949165141692912)\n",
      "('iła', ' doczekał', 0.003940173364259947)\n",
      "('staje', 'żyłem', 0.003928779056736192)\n",
      "(' udaje', 'lem', 0.003926113938731518)\n",
      "(' zastępuje', 'żyłem', 0.003915642291999172)\n",
      "(' tem', 'żyłem', 0.003913489149279636)\n",
      "('stępuje', 'lem', 0.0039078549232480775)\n",
      "(' obywate', ' znalazłam', 0.003900889622813819)\n",
      "('staje', 'biłem', 0.003898347379372308)\n",
      "(' zastępuje', 'biłem', 0.0038853123698575354)\n",
      "(' tem', 'biłem', 0.0038831759050279024)\n",
      "('staje', ' przekonałem', 0.0038774038453413817)\n",
      "(' obywate', ' odpowiedziałam', 0.0038706156292913074)\n",
      "(' hrabina', 'gus', 0.003866096229141469)\n",
      "('staje', ' usiadłem', 0.0038660820635269544)\n",
      "('staje', 'czyłem', 0.0038652577322352555)\n",
      "(' zastępuje', ' przekonałem', 0.003864438865287506)\n",
      "(' tem', ' przekonałem', 0.0038623138784303346)\n",
      "('staje', 'rzyłem', 0.0038592732018136178)\n",
      "('_:', ' znalazłam', 0.003856949686812605)\n",
      "(' hrabina', 'szyc', 0.0038552420139171817)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttention Head (Layer 0, Head 0) W_QK top token pairs:\")\n",
    "for token_pair in attention_data[0]['W_QK_top_tokens']:\n",
    "    print(token_pair)\n",
    "\n",
    "print(\"\\nAttention Head (Layer 0, Head 0) W_VO top token pairs:\")\n",
    "for token_pair in attention_data[0]['W_VO_top_tokens']:\n",
    "    print(token_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer</th>\n",
       "      <th>Head</th>\n",
       "      <th>Type</th>\n",
       "      <th>Token 1</th>\n",
       "      <th>Token 2</th>\n",
       "      <th>Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>ałam</td>\n",
       "      <td>rzałam</td>\n",
       "      <td>0.112295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>ałam</td>\n",
       "      <td>rzałem</td>\n",
       "      <td>0.110904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>ałam</td>\n",
       "      <td>lałem</td>\n",
       "      <td>0.110506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>ałam</td>\n",
       "      <td>siadłem</td>\n",
       "      <td>0.110074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>ałam</td>\n",
       "      <td>siadłam</td>\n",
       "      <td>0.109377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer  Head  Type Token 1  Token 2  Contribution\n",
       "0      0     0  W_QK    ałam   rzałam      0.112295\n",
       "1      0     0  W_QK    ałam   rzałem      0.110904\n",
       "2      0     0  W_QK    ałam    lałem      0.110506\n",
       "3      0     0  W_QK    ałam  siadłem      0.110074\n",
       "4      0     0  W_QK    ałam  siadłam      0.109377"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rows = []\n",
    "\n",
    "for layer_data in attention_data:\n",
    "    layer = layer_data['layer']\n",
    "    head = layer_data['head']\n",
    "\n",
    "    for token_q, token_k, contribution in layer_data['W_QK_top_tokens']:\n",
    "        data_rows.append({\n",
    "            'Layer': layer,\n",
    "            'Head': head,\n",
    "            'Type': 'W_QK',\n",
    "            'Token 1': token_q,\n",
    "            'Token 2': token_k,\n",
    "            'Contribution': contribution\n",
    "        })\n",
    "\n",
    "    for token_v, token_o, contribution in layer_data['W_VO_top_tokens']:\n",
    "        data_rows.append({\n",
    "            'Layer': layer,\n",
    "            'Head': head,\n",
    "            'Type': 'W_VO',\n",
    "            'Token 1': token_v,\n",
    "            'Token 2': token_o,\n",
    "            'Contribution': contribution\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_tokens = pd.DataFrame(data_rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88daafbcd30437cbe7e1bb6c2e64e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, continuous_update=False, description='Layer:', max=23), IntSlider(value=0, c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3879acd4c1134fb5ab5320a13e075bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'marker': {'color': 'grey'},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'uid': '7a0e28b9-1a95-4068-bcae-9cf6b8c7330f',\n",
       "              'x': array([0.11229453, 0.11090441, 0.11050619, 0.11007357, 0.10937658, 0.10925698,\n",
       "                          0.10824087, 0.10775756, 0.1064236 , 0.10604147, 0.10600535, 0.10562633,\n",
       "                          0.1049575 , 0.10484273, 0.10469309, 0.10441237, 0.10431717, 0.10390878,\n",
       "                          0.10386767, 0.10325083]),\n",
       "              'y': array(['ałam ➔ rzałam', 'ałam ➔ rzałem', 'ałam ➔ lałem', 'ałam ➔ siadłem',\n",
       "                          'ałam ➔ siadłam', 'ałam ➔ żyłem', 'ałam ➔ żyłam', 'ałem ➔ rzałam',\n",
       "                          'ałem ➔ rzałem', 'ałem ➔ lałem', 'ałyśmy ➔ rzałam', 'ałem ➔ siadłem',\n",
       "                          'ałem ➔ siadłam', 'ałem ➔ żyłem', 'ałyśmy ➔ rzałem', 'ałam ➔ cisnąłem',\n",
       "                          'ałyśmy ➔ lałem', 'ałyśmy ➔ siadłem', 'ałem ➔ żyłam',\n",
       "                          'ałyśmy ➔ siadłam'], dtype=object)}],\n",
       "    'layout': {'height': 600,\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Layer 0, Head 0 - Top 20 W_QK_top_tokens'},\n",
       "               'width': 1200,\n",
       "               'xaxis': {'title': {'text': 'Contribution'}},\n",
       "               'yaxis': {'categoryorder': 'total ascending', 'title': {'text': 'Token Pair'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def interactive_attention_visualization(attention_data, model):\n",
    "    # Create sliders and dropdowns\n",
    "    layer_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=model.config.n_layer - 1,\n",
    "        step=1,\n",
    "        description='Layer:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    head_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=model.config.n_head - 1,\n",
    "        step=1,\n",
    "        description='Head:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    token_type_dropdown = widgets.Dropdown(\n",
    "        options=['W_QK_top_tokens', 'W_VO_top_tokens'],\n",
    "        value='W_QK_top_tokens',\n",
    "        description='Token Type:',\n",
    "    )\n",
    "    top_n_slider = widgets.IntSlider(\n",
    "        value=20,\n",
    "        min=1,\n",
    "        max=50,\n",
    "        step=1,\n",
    "        description='Top N:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Initialize an empty FigureWidget\n",
    "    fig = go.FigureWidget(layout=go.Layout(\n",
    "        title='',\n",
    "        xaxis=dict(title='Contribution'),\n",
    "        yaxis=dict(title='Token Pair', categoryorder='total ascending'),\n",
    "        height=600,\n",
    "        width=1200\n",
    "    ))\n",
    "\n",
    "    # Function to update the plot\n",
    "    def update_plot(layer, head, token_type, top_n):\n",
    "        fig.data = []\n",
    "        fig.layout.title = f'Layer {layer}, Head {head} - Top {top_n} {token_type}'\n",
    "\n",
    "        selected_data = None\n",
    "        for data in attention_data:\n",
    "            if data['layer'] == layer and data['head'] == head:\n",
    "                selected_data = data[token_type]\n",
    "                break\n",
    "\n",
    "        if not selected_data:\n",
    "            with fig.batch_update():\n",
    "                fig.add_annotation(\n",
    "                    text=\"No data available for the selected layer and head.\",\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=20)\n",
    "                )\n",
    "            return\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        df = pd.DataFrame(selected_data, columns=['Token 1', 'Token 2', 'Contribution'])\n",
    "        df['Token Pair'] = df['Token 1'] + ' ➔ ' + df['Token 2']\n",
    "        df = df.sort_values(by='Contribution', ascending=False).head(top_n)\n",
    "\n",
    "        # Update the figure\n",
    "        with fig.batch_update():\n",
    "            fig.add_bar(\n",
    "                x=df['Contribution'],\n",
    "                y=df['Token Pair'],\n",
    "                orientation='h',\n",
    "                marker_color='grey'\n",
    "            )\n",
    "\n",
    "    controls = {\n",
    "        'layer': layer_slider,\n",
    "        'head': head_slider,\n",
    "        'token_type': token_type_dropdown,\n",
    "        'top_n': top_n_slider\n",
    "    }\n",
    "    out = widgets.interactive_output(update_plot, controls)\n",
    "\n",
    "    ui = widgets.VBox([layer_slider, head_slider, token_type_dropdown, top_n_slider])\n",
    "    display(ui, fig)\n",
    "\n",
    "    update_plot(layer_slider.value, head_slider.value, token_type_dropdown.value, top_n_slider.value)\n",
    "\n",
    "interactive_attention_visualization(attention_data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
