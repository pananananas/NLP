{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(50000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FF layers: 100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
      "Processing FF keys: 100%|██████████| 12/12 [00:07<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "E = model.get_input_embeddings().weight\n",
    "E = E.cpu()\n",
    "\n",
    "# Transpose E for easier multiplication\n",
    "E_T = E.T  # shape: (d_model, vocab_size)\n",
    "\n",
    "# Function to extract and project FF values into embedding space\n",
    "def extract_ff_values(model, E_T, tokenizer, k=10):\n",
    "    ff_neuron_tokens = []\n",
    "    for layer_idx, layer in enumerate(tqdm(model.encoder.layer, desc=\"Processing FF layers\")):\n",
    "        # Get the feed-forward weights\n",
    "        # First linear layer\n",
    "        K = layer.intermediate.dense.weight.data  # shape: (intermediate_size, hidden_size)\n",
    "        # Second linear layer (FF values)\n",
    "        V = layer.output.dense.weight.data.T  # shape: (intermediate_size, hidden_size)\n",
    "\n",
    "        # Project FF values into embedding space\n",
    "        V_proj = V @ E_T  # shape: (intermediate_size, vocab_size)\n",
    "\n",
    "        # For each neuron, get top-k tokens\n",
    "        layer_neuron_tokens = []\n",
    "        for neuron_idx in range(V_proj.shape[0]):\n",
    "            neuron_proj = V_proj[neuron_idx]  # shape: (vocab_size,)\n",
    "            topk = torch.topk(neuron_proj, k=k)\n",
    "            topk_indices = topk.indices  # indices of top-k tokens\n",
    "            topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "            layer_neuron_tokens.append({\n",
    "                'neuron': neuron_idx,\n",
    "                'top_tokens': topk_tokens\n",
    "            })\n",
    "        ff_neuron_tokens.append({\n",
    "            'layer': layer_idx,\n",
    "            'neurons': layer_neuron_tokens\n",
    "        })\n",
    "    return ff_neuron_tokens\n",
    "\n",
    "# Function to extract and project FF keys into embedding space\n",
    "def extract_ff_keys(model, E_T, tokenizer, k=10):\n",
    "    ff_key_tokens = []\n",
    "    for layer_idx, layer in enumerate(tqdm(model.encoder.layer, desc=\"Processing FF keys\")):\n",
    "        # Get the FF keys\n",
    "        K = layer.intermediate.dense.weight.data  # shape: (intermediate_size, hidden_size)\n",
    "\n",
    "        # Project FF keys into embedding space\n",
    "        K_proj = K @ E_T  # shape: (intermediate_size, vocab_size)\n",
    "\n",
    "        # For each neuron, get top-k tokens\n",
    "        layer_neuron_tokens = []\n",
    "        for neuron_idx in range(K_proj.shape[0]):\n",
    "            neuron_proj = K_proj[neuron_idx]  # shape: (vocab_size,)\n",
    "            topk = torch.topk(neuron_proj, k=k)\n",
    "            topk_indices = topk.indices\n",
    "            topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "            layer_neuron_tokens.append({\n",
    "                'neuron': neuron_idx,\n",
    "                'top_tokens': topk_tokens\n",
    "            })\n",
    "        ff_key_tokens.append({\n",
    "            'layer': layer_idx,\n",
    "            'neurons': layer_neuron_tokens\n",
    "        })\n",
    "    return ff_key_tokens\n",
    "\n",
    "# Extract and project FF values and keys\n",
    "ff_values = extract_ff_values(model, E_T, tokenizer, k=10)\n",
    "ff_keys = extract_ff_keys(model, E_T, tokenizer, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-forward values (Layer 3, first 5 neurons):\n",
      "Neuron 0: ['火', '火', '仁', '仁', 'modli', 'ロ', 'lion', 'Ά', 'ե', 'kapli']\n",
      "Neuron 1: ['ATP', 'OBO', 'schrieb', 'dieta', 'SPA', 'napisal', 'CER', 'OZ', 'złożony', 'NASA']\n",
      "Neuron 2: ['odtwarza', 'okoliczności', 'nagrywa', 'katolickiej', 'uroczystości', 'Internecie', 'wydarzenie', 'katolickich', 'podkreśla', 'ckiego']\n",
      "Neuron 3: ['ǎ', 'ǎ', '藤', '藤', 'Toy', 'Archi', '393', '士', '士', 'テ']\n",
      "Neuron 4: ['schrieb', '†', '§', 'Negrin', '‰', '‰', 'aden', 'Zigrin', 'sena', '0691']\n",
      "Neuron 5: ['osobisty', 'gust', 'sprawdz', 'zgubi', 'gubi', 'British', 'przykład', 'EKS', 'Ani', 'sympati']\n",
      "Neuron 6: ['chwy', 'WT', 'niesienie', 'wylew', 'lew', 'wykrę', 'BOT', 'boo', 'wers', 'przejęciem']\n",
      "Neuron 7: ['か', 'か', 'ư', 'ư', 'GUS', 'RUDNI', '⋅', 'RON', 'ñ', 'STOL']\n",
      "Neuron 8: ['Zar', 'mal', 'Tuli', 'molest', 'Kri', 'Kry', 'Kul', 'Lud', 'Ola', 'czą']\n",
      "Neuron 9: ['ň', 'gili', 'Wielkim', 'pewne', 'ň', 'Berg', 'prawdziwego', 'invalid', 'prawdziwym', 'Wigili']\n",
      "Neuron 10: ['tarz', 'إ', 'klientom', 'urządzenia', 'pój', 'powodzeniem', 'przyjętym', 'rozwiązaniami', 'mieści', 'znajdującym']\n",
      "Neuron 11: ['małżeństw', 'opisuje', 'metafor', 'Dawid', 'Łukasz', 'Babi', 'schrieb', 'Dz', 'wspomina', 'Adam']\n",
      "Neuron 12: ['koper', 'azy', 'wiąże', 'toy', 'związane', 'załatwi', 'baraż', 'zdi', 'oczywiście', 'terium']\n",
      "Neuron 13: ['biec', 'skorzysta', 'przystąpi', 'przystąpi', 'skorzysta', 'skorzystać', 'Tań', 'lena', 'spróbować', 'zapobiec']\n",
      "Neuron 14: ['akta', 'przedziale', 'pełną', 'cierpliwość', 'rzeczy', 'zam', 'GMT', 'oke', 'vogel', 'Już']\n",
      "Neuron 15: ['ら', 'ら', 'Ksa', 'kta', 'ま', 'テ', '지', 'テ', '하', '하']\n",
      "Neuron 16: ['urs', 'prosi', 'jem', 'poro', 'atmosfer', 'doro', 'Kontra', 'intele', 'psi', 'prosi']\n",
      "Neuron 17: ['Chal', 'rku', 'skowe', 'Strefy', 'bojowe', 'konkursowe', 'budowane', 'nikowe', '神', 'ّ']\n",
      "Neuron 18: ['Spis', 'jeżeli', 'Jeszcze', 'jeśli', 'jeszcze', 'Gdyby', 'spis', 'najszybciej', 'jakby', 'świetnie']\n",
      "Neuron 19: ['wpływa', 'niektórzy', 'rzuca', 'zwraca', 'zwrócić', 'przedstawiciel', 'jesteś', 'stanowi', 'możesz', 'kwietnia']\n",
      "Neuron 20: ['otwarci', 'fani', 'szok', 'wania', 'ziak', 'rzanie', 'cznicy', 'szał', 'zainteresowani', 'wanie']\n",
      "Neuron 21: ['róż', 'garaż', 'brąz', 'róż', 'głęboki', 'wpływ', 'zakład', 'prysznic', 'chlo', 'garaż']\n",
      "Neuron 22: ['innym', 'innych', 'innego', 'inną', 'inne', 'inaczej', 'inna', 'innej', 'druga', 'inni']\n",
      "Neuron 23: ['Handel', 'Ochrona', 'infra', 'kość', 'kulturą', 'kulturę', 'otocze', 'oto', 'WY', 'cara']\n",
      "Neuron 24: ['vito', 'Obra', 'obra', 'makro', 'kowały', 'poza', 'okupa', 'iera', 'lepi', 'phar']\n",
      "Neuron 25: ['brow', 'mla', 'rę', 'bę', 'formu', 'skom', 'bę', 'magazy', 'pio', 'Paster']\n",
      "Neuron 26: ['html', 'sny', 'koron', 'jm', 'Ö', 'kumen', 'oszczędności', 'ür', 'rion', 'aran']\n",
      "Neuron 27: ['ű', 'atutem', 'Klaus', 'opłat', 'Fau', 'towymi', 'int', 'późniejsze', 'kraju', 'utrzymania']\n",
      "Neuron 28: ['rej', 'drze', '्', 'plik', 'rej', 'dę', 'chorej', 'pij', 'sters', 'aju']\n",
      "Neuron 29: ['<s>', 'mundur', 'opiekun', 'oficer', 'oficer', 'zauważyć', 'pokazał', 'zauważył', 'pogrzeb', 'zapomina']\n",
      "Neuron 30: ['nij', 'indo', 'Bio', 'Raj', 'Hit', 'Bul', 'krop', 'So', 'kuli', 'RW']\n",
      "Neuron 31: ['obowiązuje', 'stosować', 'stosuje', 'stosowanie', 'stosowania', 'stosowany', 'stosowane', 'powtarza', 'przestrzega', 'obowiązują']\n",
      "Neuron 32: ['Sti', 'Che', 'Fot', 'ธ', '学', 'พ', 'Fro', 'ธ', 'Max', 'Hop']\n",
      "Neuron 33: ['wolny', 'pisa', 'ulo', 'gili', 'zakłó', 'humo', 'niu', 'łó', 'stwa', 'ē']\n",
      "Neuron 34: ['nasi', 'izba', 'Zem', 'Sre', 'inge', 'gela', 'Ger', 'inger', 'lec', 'napa']\n",
      "Neuron 35: ['Stra', 'UO', 'Sc', 'ZCh', 'OWY', 'Ka', 'Został', 'ALU', 'GOR', 'CZY']\n",
      "Neuron 36: ['szczególności', 'zaś', 'aw', 'Ponadto', 'Natomiast', 'więc', 'Ponieważ', 'wtedy', 'Tymczasem', 'kien']\n",
      "Neuron 37: ['starym', 'ama', 'kinema', 'wari', 'starymi', 'gry', 'skor', 'prowadzącym', 'zakończonym', 'ex']\n",
      "Neuron 38: ['Lili', 'prawić', 'sprawić', 'lubić', 'wu', 'ponosić', 'ष', 'zadowolić', 'ponow', 'lust']\n",
      "Neuron 39: ['Aja', 'Aha', 'Sof', 'Stel', 'Niko', 'pieprz', 'sla', 'ABS', 'Lut', 'đ']\n",
      "Neuron 40: ['kki', 'ckar', 'rom', 'woź', 'nume', 'gaw', 'poja', 'tym', 'wzor', 'komendan']\n",
      "Neuron 41: ['humo', 'mów', 'utrzy', 'muj', 'salon', 'Zie', 'mowi', 'hol', 'płyn', 'nością']\n",
      "Neuron 42: ['lau', 'szan', 'awan', 'rac', 'Jas', 'lich', 'ziel', 'Ziel', 'kana', 'au']\n",
      "Neuron 43: ['prowadząc', 'uwagę', 'Argentyna', 'śmy', 'futbol', '¬', '1907', 'tty', '1912', 'jedno']\n",
      "Neuron 44: ['Bran', 'podstaw', 'obligat', 'paso', 'ো', 'Bob', 'pas', 'pasa', 'Pasa', 'manipul']\n",
      "Neuron 45: ['organizm', 'UJ', 'odzie', 'roślin', 'Ę', 'odzież', 'GRU', 'JĘ', 'grzyb', 'stry']\n",
      "Neuron 46: ['CIE', 'gip', 'wywiadzie', 'Temat', 'ambasador', 'rezerw', 'olimpiadzie', 'udziela', 'WW', 'WF']\n",
      "Neuron 47: ['ARD', 'ANNA', 'SW', 'IA', 'Times', 'CCC', 'WG', 'ML', 'ELE', 'IA']\n",
      "Neuron 48: ['Oku', 'cti', 'udar', 'Kantor', 'tatu', 'vogel', 'monopoli', 'atu', 'Kazi', 'tatu']\n",
      "Neuron 49: ['kontrole', 'ี', 'okupa', 'role', 'magi', 'inka', 'anonim', 'kustosz', 'fine', 'kontrola']\n",
      "Neuron 50: ['reprezentant', 'portugal', 'Łodzi', 'banku', 'muzeum', 'Tokio', 'republika', 'parlamentu', 'hrab', 'Warszawie']\n",
      "Neuron 51: ['krzy', 'Poprzed', 'szkol', 'długo', 'szkolnictwo', 'tokrzy', 'żona', 'Skrzy', 'つ', 'od']\n",
      "Neuron 52: ['skutecznością', 'alność', 'lność', 'skuteczności', 'braku', 'dolność', 'talent', 'walność', 'zdolności', 'absen']\n",
      "Neuron 53: ['NIK', 'NIN', 'LIA', 'TUR', '367', '1940', 'NIKA', 'TIR', 'Ľ', 'Ľ']\n",
      "Neuron 54: ['ວ', 'ວ', 'stosun', 'efektow', 'cati', 'szalenie', 'ść', 'tura', 'spektakular', 'wykoń']\n",
      "Neuron 55: ['danie', 'Got', 'rób', 'ery', 'ffi', 'tygodniowo', 'OBO', 'ryncy', 'owie', 'zdu']\n",
      "Neuron 56: ['proces', 'całością', 'sekcje', 'koron', 'Emocje', 'koron', 'różnicą', 'Obsada', 'produkcje', 'reprezentacje']\n",
      "Neuron 57: ['otoczenia', 'kow', 'Yar', 'szło', 'gili', 'raca', 'ado', 'prawi', 'xt', 'łó']\n",
      "Neuron 58: ['grzyb', 'prawnicy', 'Dzierz', 'każdej', 'zagrożenia', 'syl', 'leb', 'zwy', 'ciśnie', 'podatnicy']\n",
      "Neuron 59: ['Działa', 'Pomó', 'Działa', 'Fundacji', 'inicjatyw', 'Stowarzyszenie', 'Caritas', 'inicjatyw', 'darczy', 'akcja']\n",
      "Neuron 60: ['MOPS', 'ZK', 'online', 'alkohol', 'lokal', 'wizyt', 'singel', 'MZK', 'psychologa', 'lokale']\n",
      "Neuron 61: ['goda', 'ธ', 'kancle', 'fede', 'kanclerza', 'celem', 'dale', 'Kmita', 'Sanctum', 'Ojcze']\n",
      "Neuron 62: ['cera', 'splan', 'Æ', 'ė', 'del', 'Minnes', 'Æ', 'transplan', 'Skal', 'Kalifor']\n",
      "Neuron 63: ['ojciec', 'Brat', 'Mama', 'mąż', 'Mąż', 'Ū', 'Ū', 'Ŭ', 'dzieckiem', '後']\n",
      "Neuron 64: ['ث', 'چ', 'چ', 'dura', 'metra', 'ث', 'obsa', 'metra', 'zaczynają', 'napęd']\n",
      "Neuron 65: ['Ù', 'samoist', 'Ù', 'PF', 'przyczy', 'プ', 'CZY', 'ゲ', 'CZY', 'mecha']\n",
      "Neuron 66: ['Michał', 'Świat', 'uzu', 'Urban', 'Salon', 'bliżej', 'Andrzej', 'Andrzej', 'Urban', 'Łukasz']\n",
      "Neuron 67: ['ABS', 'Nico', 'Natura', 'jem', 'stole', 'Psy', 'kończy', 'spró', 'lec', 'uste']\n",
      "Neuron 68: ['bą', 'Dro', 'olon', 'elem', 'Inter', 'EURO', 'autora', 'cien', 'Euro', 'lin']\n",
      "Neuron 69: ['Poznaniu', 'Stuttgart', 'Szczecin', 'Bordeaux', 'Londyn', 'Krakowie', 'Toronto', 'Manchester', 'Łodzi', 'Wrocławiu']\n",
      "Neuron 70: ['sika', 'prąd', 'kalkula', 'liwe', '229', 'chwy', 'liczne', 'rowo', 'reklamowe', '389']\n",
      "Neuron 71: ['część', 'części', 'Część', 'części', 'częścią', 'fragment', 'chlu', 'częściach', 'partie', 'karo']\n",
      "Neuron 72: ['nę', '会', 'bawi', '会', 'spędza', 'zatrzymania', 'menu', 'chwil', 'bawi', 'zarządzenia']\n",
      "Neuron 73: ['Podczas', 'celu', 'Cykl', 'Dzięki', 'doraz', 'Kup', 'Mimo', 'Przy', 'Ν', 'opierając']\n",
      "Neuron 74: ['polecam', 'cko', 'zapraszam', 'Raj', 'kki', 'mla', 'cko', 'Polecam', '相', 'wyłącz']\n",
      "Neuron 75: ['pozna', 'pozna', 'znajomość', 'Pozna', 'znajomości', 'poznali', 'poznać', 'znasz', 'przestrzega', 'znali']\n",
      "Neuron 76: ['ű', 'ű', 'podlaskim', 'morskim', 'Ö', 'Feb', 'Hum', 'mskim', '撲', 'sparen']\n",
      "Neuron 77: ['źnie', 'źnie', 'Pozosta', 'ubog', 'pój', 'Dzieła', 'pój', 'Zatrud', 'slog', 'uboż']\n",
      "Neuron 78: ['priv', 'torium', 'cent', 'htm', 'spekt', 'priv', 'Polakiem', 'wiecki', 'Inwali', 'cent']\n",
      "Neuron 79: ['nasi', 'nigdy', 'nikt', 'zobacz', 'celem', 'ذ', 'ى', 'ج', 'گ', 'dodatki']\n",
      "Neuron 80: ['CIN', 'CZY', '¡', 'NOW', '„', 'CAN', 'tzw', 'CZY', 'cium', 'ZIE']\n",
      "Neuron 81: ['mogło', 'zano', 'owało', 'DT', 'miało', 'było', 'ARS', 'ロ', 'gró', 'dowało']\n",
      "Neuron 82: ['STI', 'љ', 'љ', 'を', 'を', 'Р', 'Zak', '治', 'STAN', 'zan']\n",
      "Neuron 83: ['nienia', 'szczenia', 'śnienia', 'nienie', 'ęcia', 'wydłużenie', 'wania', 'wanie', 'cyj', 'niem']\n",
      "Neuron 84: ['tapi', 'ũ', 'buła', 'tuła', 'kła', 'zała', 'doł', 'Niemniej', 'ła', 'pobła']\n",
      "Neuron 85: ['Dallas', 'Fak', 'Legion', 'resort', 'wich', 'Miami', 'Kubiak', 'czycie', 'Baran', 'Tokio']\n",
      "Neuron 86: ['stwa', 'monopol', 'pornografi', 'stwem', 'lstwa', 'stwa', 'ctwo', 'nictwo', 'doping', 'tyzm']\n",
      "Neuron 87: ['małe', 'dużym', 'duże', '大', 'wielki', '大', 'duży', 'większe', 'gigant', 'wielkie']\n",
      "Neuron 88: ['przyjaciel', 'skype', 'znajomych', 'personel', 'przyjaciel', 'rodziców', 'Ambasad', 'specjalistów', 'rektor', 'ambasad']\n",
      "Neuron 89: ['modelu', 'Kontra', 'odpowiada', 'skład', 'sprawia', 'sposobów', 'skład', 'zagra', 'punktów', 'wymagania']\n",
      "Neuron 90: ['§', 'ustawy', '§', 'traktatu', 'ustawa', 'Ustawy', 'reguluje', 'Ordyna', 'Dz', 'dyrektywy']\n",
      "Neuron 91: ['za', 'Za', 'ZA', 'za', 'poza', 'zamiesz', 'libre', 'Austria', 'ZA', 'Za']\n",
      "Neuron 92: ['ALE', 'ハ', 'part', 'kontekst', 'korespon', 'aktu', 'norm', 'przepis', 'lega', 'normy']\n",
      "Neuron 93: ['niasz', 'asz', 'Twoje', 'cisz', 'jesz', 'Twój', 'lisz', 'Twojego', 'Ciebie', 'sz']\n",
      "Neuron 94: ['też', 'wieloletni', 'Moder', 'ボ', 'łu', 'ボ', 'Ever', 'Ö', 'GOR', 'NAJ']\n",
      "Neuron 95: ['Lut', 'żaden', 'zagry', 'コ', 'tkwi', 'newsgate', '行', 'Carlo', 'WŁO', 'ssy']\n",
      "Neuron 96: ['wrażenia', 'szki', 'Ameryka', 'obiad', 'Rome', 'obiad', 'sytuacji', 'budowla', 'rozmów', 'zapewni']\n",
      "Neuron 97: ['ظ', 'ظ', 'sądow', 'wszelkich', 'Bożego', 'wód', 'św', 'dowód', 'kieli', 'skup']\n",
      "Neuron 98: ['rzą', 'wzajemnego', 'jm', 'jemu', 'mę', 'sąsie', 'wza', 'kojarzą', 'bezpośredni', 'ffe']\n",
      "Neuron 99: ['Washington', 'Toronto', 'Cannes', 'Detroit', 'Seattle', 'Atlanta', 'WF', 'Vancouver', 'Tokio', 'Rangers']\n",
      "\n",
      "Feed-forward keys (Layer 0, first 5 neurons):\n",
      "Neuron 0: ['nowa', 'bibli', 'Bibli', 'mister', 'Boli', 'Beau', 'Bog', 'Beau', 'prom', 'Dialog']\n",
      "Neuron 1: ['Lat', 'rion', 'mk', 'Arma', 'mim', 'Zem', 'Mura', 'jn', 'illon', 'mk']\n",
      "Neuron 2: ['par', 'Par', 'PAR', 'para', 'para', 'par', 'Para', 'pary', 'Para', 'parę']\n",
      "Neuron 3: ['Bada', 'wiad', 'ONZ', 'Rycer', 'ÿ', 'bada', 'Sol', 'bada', 'Harcer', 'oby']\n",
      "Neuron 4: ['</s>', '¬', 'Fal', '.', 'fal', '¬', 'Marka', 'fala', 'Pru', '¨']\n",
      "Neuron 5: ['Raj', 'Mur', 'Sul', 'Dol', 'Lor', 'ham', 'reci', 'Pasa', 'nade', 'Hir']\n",
      "Neuron 6: ['Dob', 'Nic', 'Nas', 'Mon', 'Toy', 'TPN', 'NIK', 'JS', 'OLI', 'Resort']\n",
      "Neuron 7: ['inka', 'letnie', 'Lot', 'plakat', 'Turyn', 'Ple', 'gitar', 'inka', 'Salon', 'Kry']\n",
      "Neuron 8: ['dora', 'ady', 'Saj', 'You', 'tero', 'ciel', '</s>', 'piro', '.', 'hale']\n",
      "Neuron 9: ['lko', 'lki', 'lka', 'pralki', 'Nim', 'Wielko', 'upi', 'alka', 'skle', 'odu']\n",
      "Neuron 10: ['apte', 'Tot', 'tej', 'mai', 'Mai', 'lau', 'tutej', 'Pere', '¬', '</s>']\n",
      "Neuron 11: ['Moi', 'osta', 'moi', 'Domini', 'OST', 'coli', 'Anti', 'Nikodem', 'Bug', 'ÿ']\n",
      "Neuron 12: ['San', 'San', 'san', 'san', 'Sum', 'ien', 'Sanepid', 'wiz', 'nety', 'natem']\n",
      "Neuron 13: ['Astra', 'wariant', '″', 'gis', 'idio', '</s>', '‹', 'nagroda', '′', 'plast']\n",
      "Neuron 14: ['ogi', '′', 'GI', 'wu', 'wu', 'Logi', 'fy', 'Kim', 'Maja', 'nna']\n",
      "Neuron 15: ['Kry', 'koli', 'soli', 'puli', 'Kul', 'Kata', 'psu', 'menta', 'makula', 'mili']\n",
      "Neuron 16: ['Dama', 'komor', 'zab', 'bene', 'zab', 'Dul', 'bela', 'alergi', 'bene', 'bana']\n",
      "Neuron 17: ['Dro', 'Ple', 'anie', 'ÿ', 'ÿ', 'dro', '</s>', 'dium', 'ceni', 'sli']\n",
      "Neuron 18: ['</s>', '，', '.', ',', ',', 'fore', '，', 'XIX', 'ེ', '་']\n",
      "Neuron 19: ['nag', 'Nag', 'nagle', 'nig', 'wich', 'agi', '¬', 'bourne', 'pene', 'sno']\n",
      "Neuron 20: ['</s>', '′', 'kalen', 'pragn', 'nien', '′', 'bula', 'Deli', '¨', 'Kra']\n",
      "Neuron 21: ['Tera', 'Tere', 'vera', 'mera', 'Jana', 'moda', 'dra', 'Mode', 'jad', 'bana']\n",
      "Neuron 22: ['facto', 'bene', 'nista', 'Gui', 'bene', 'ssa', 'uto', 'aro', 'vele', 'reje']\n",
      "Neuron 23: ['ÿ', 'pli', 'ÿ', 'dge', 'Pr', 'pr', 'Lek', 'dani', '¬', 'wierni']\n",
      "Neuron 24: ['opa', 'zaopa', 'gip', 'Łopa', 'â', 'Egip', 'Nadal', 'Natura', 'Lea', 'koron']\n",
      "Neuron 25: ['oprac', 'ob', '</s>', 'prac', 'at', 'ˇ', 'Oprac', 'Prac', 'Praca', 'Taki']\n",
      "Neuron 26: ['metrowe', 'Wawe', 'kse', 'Atlas', 'rowe', 'Norwe', 'wawe', 'Ewe', 'ÿ', 'wawa']\n",
      "Neuron 27: ['ryn', 'ÿ', 'ÿ', 'Spra', 'html', 'lic', '.', 'wroc', 'fos', '¬']\n",
      "Neuron 28: ['am', 'm', '</s>', 'pm', 'Fal', 'AM', '.', '′', 'Pola', '，']\n",
      "Neuron 29: ['Lui', 'karo', 'San', 'tapi', 'nai', 'lau', 'LO', 'ui', 'LO', 'nai']\n",
      "Neuron 30: ['poja', 'balu', 'basa', 'wiat', 'loka', 'lokale', 'Lam', 'Kana', 'Kata', 'argu']\n",
      "Neuron 31: ['dom', 'Dom', 'dom', 'Dom', 'DOM', 'domem', 'yn', 'yn', 'domów', 'Domy']\n",
      "Neuron 32: ['ÿ', 'Tempo', 'Cinema', 'Nim', 'pta', 'tram', 'Sali', 'TRO', 'nabiera', 'Mara']\n",
      "Neuron 33: ['gorsza', 'skazana', 'Lechem', 'Mszana', 'oby', 'Ore', 'zka', 'Jur', 'Polakiem', '¨']\n",
      "Neuron 34: ['rozbro', 'Aga', 'uspoko', 'TRO', 'Doskona', 'niepoko', 'Edi', 'karto', 'Zbro', 'sygna']\n",
      "Neuron 35: ['</s>', 'Hil', 'Ku', 'muni', 'Bug', 'Mine', 'Hir', 'ÿ', 'Moi', 'Ku']\n",
      "Neuron 36: ['ÿ', 'ÿ', 'É', 'é', 'É', 'té', '</s>', 'mó', 'rali', 'é']\n",
      "Neuron 37: ['wysiad', 'balu', 'ktu', 'rar', 'rwa', 'osiad', 'cane', 'coli', 'Mach', 'wiad']\n",
      "Neuron 38: ['óz', 'wak', 'wolny', 'dowolny', '507', 'szony', 'własny', 'dwa', 'WM', 'jedyny']\n",
      "Neuron 39: ['ứ', 'ứ', 'przypadła', 'zdołała', 'szczebli', 'hara', 'zdołali', 'adep', 'zaha', 'przywiąza']\n",
      "Neuron 40: ['，', 'bali', 'Tol', '.', '¤', 'tani', '</s>', 'to', ',', 'baro']\n",
      "Neuron 41: ['</s>', 'ÿ', '¨', 'ÿ', 'zada', 'bawi', 'ate', 'obawi', '¨', 'epide']\n",
      "Neuron 42: ['Not', 'oty', 'not', 'oty', 'pyt', 'osi', 'dana', 'lau', 'soboty', 'nieuf']\n",
      "Neuron 43: ['nu', '3', 'Nato', 'nu', 'mera', 'mer', 'Nu', 'NU', 'Mer', 'nud']\n",
      "Neuron 44: ['Barba', 'best', 'Bug', 'Cos', 'pies', 'kabar', 'lip', 'Robot', 'anim', 'Zam']\n",
      "Neuron 45: ['¶', '±', '¬', '¶', '′', 'bula', '</s>', 'CK', '.', '″']\n",
      "Neuron 46: ['Bank', 'infla', 'bank', 'bank', 'kapitału', 'zadłużenia', 'portfela', 'kredytu', 'PLN', 'Finans']\n",
      "Neuron 47: ['ATA', 'Fir', 'fir', 'aty', 'atu', 'ÿ', 'tatu', 'atu', 'ata', 'aktu']\n",
      "Neuron 48: ['nicka', 'inga', 'aha', 'Podhale', 'linga', 'ambu', 'Zoo', 'agh', 'ÿ', 'gane']\n",
      "Neuron 49: ['ca', 'ca', 'Ca', '</s>', 'ÿ', 'CA', 'Log', 'GA', 'GA', '，']\n",
      "Neuron 50: ['\\\\', '.', '⋅', '.', '</s>', '_', '￼', 'aut', '‑', '⋅']\n",
      "Neuron 51: ['spe', 'kuli', 'Spe', 'ope', 'kuli', 'gine', 'Bug', 'mona', 'glu', 'piro']\n",
      "Neuron 52: ['Hero', 'Mura', 'police', 'Domini', 'ewaku', 'poko', 'Putin', 'rog', 'rach', '′']\n",
      "Neuron 53: ['Bez', 'Hon', 'bez', 'tou', 'Bez', 'yn', 'jom', '</s>', 'bez', 'Bezdom']\n",
      "Neuron 54: ['wiz', 'kawy', 'bitwy', 'Hil', 'Bot', 'Litwy', 'bowy', 'Skawy', 'bat', 'powy']\n",
      "Neuron 55: ['G', '</s>', 'g', '¬', '，', ',', '¬', '.', '，', 'ེ']\n",
      "Neuron 56: ['mne', 'mna', 'mnie', 'facto', 'ÿ', 'ÿ', '</s>', 'Ronaldo', 'mim', '§']\n",
      "Neuron 57: ['[', ']', ']', '¬', 'Jur', '¬', '[', '</s>', 'Juri', '}']\n",
      "Neuron 58: ['ˇ', 'mister', 'ˇ', 'plane', 'anim', '1905', 'Bug', 'kuch', 'wynik', '1924']\n",
      "Neuron 59: ['</s>', 'i', '.', ',', '，', '，', '¬', '.', ',', '¤']\n",
      "Neuron 60: ['Deli', 'rali', 'subishi', 'Hil', 'orę', 'lara', 'iste', 'coli', 'Dama', '་']\n",
      "Neuron 61: ['lenie', 'rzanie', 'rzeniem', 'rzania', 'rzaniem', 'lizowanie', 'dzenie', 'żeniem', 'żenie', 'dzeniem']\n",
      "Neuron 62: ['Tra', 'TRA', 'tra', 'ÿ', 'ÿ', 'fore', 'potra', '</s>', 'witra', 'Edi']\n",
      "Neuron 63: ['ÿ', 'ÿ', '</s>', '¯', 'zadb', '¤', 'nud', '¨', '¯', 'Kut']\n",
      "Neuron 64: ['uj', 'UJ', 'uja', 'uj', 'ującymi', 'uje', 'uja', 'owanymi', 'owa', '</s>']\n",
      "Neuron 65: ['</s>', '་', 'ི', 'ག', 'འ', 'བ', 'ེ', 'ܵ', 'ད', 'ུ']\n",
      "Neuron 66: ['¬', 'mble', 'md', 'Obok', '¬', 'dez', 'ĥ', 'ÿ', '^', 'Mada']\n",
      "Neuron 67: ['anty', 'anty', 'Anti', 'nki', 'Dr', 'dra', '<unk>', 'rant', 'Anty', 'Premier']\n",
      "Neuron 68: ['aktu', 'aktu', 'vana', 'Ab', 'ab', 'ALE', 'BRU', 'rali', 'gna', 'OB']\n",
      "Neuron 69: ['gili', 'agi', 'strychu', 'rpi', 'stry', 'Syn', 'Wigili', 'nasi', 'dopi', 'gi']\n",
      "Neuron 70: ['</s>', 'Tu', '.', 'sa', 'tu', 'tu', '.', 'sa', ',', '，']\n",
      "Neuron 71: ['ety', 'wich', 'olon', 'OST', 'ost', 'este', 'wich', 'Brzost', 'est', 'tomi']\n",
      "Neuron 72: ['kien', 'kien', 'Pel', 'rion', 'ikon', 'Kiel', 'malar', 'mk', 'kolek', 'zb']\n",
      "Neuron 73: ['ས', '་', 'པ', 'ད', 'ུ', '，', 'ི', '¬', '<unk>', 'ད']\n",
      "Neuron 74: ['system', 'System', '</s>', 'systemu', 'systemem', 'proce', 'systema', 'procesie', 'Systemu', 'systemach']\n",
      "Neuron 75: ['kru', 'flaga', 'atra', 'stopa', 'nari', 'korona', 'popa', 'matura', 'stypendia', 'ronda']\n",
      "Neuron 76: ['upi', 'baz', 'gon', 'baz', 'lli', 'mandat', 'pauz', 'fore', 'mete', 'klauz']\n",
      "Neuron 77: ['Ide', 'mili', 'stadi', 'Astra', 'Idea', 'postul', 'Dialog', 'Id', 'UNESCO', 'Isla']\n",
      "Neuron 78: ['suszar', 'raj', 'ZO', 'oza', 'Aqu', 'lian', 'ş', 'gine', 'Bat', 'coli']\n",
      "Neuron 79: ['y', 'ya', 'ja', '</s>', 'Ya', '.', 'ÿ', 'ya', 'ы', 'ུ']\n",
      "Neuron 80: ['kre', 'Podkre', 'gier', 'gier', 'dekre', 'podkre', 'sekre', 'wykre', 'Olgier', 'smal']\n",
      "Neuron 81: ['rzanka', 'Iza', 'Mszana', 'natka', 'PZZ', 'gratka', 'bawi', 'baz', 'Jagoda', 'Białorusi']\n",
      "Neuron 82: ['ÿ', 'opi', 'mk', 'ÿ', '‰', 'oci', '￼', '￼', '∞', '≈']\n",
      "Neuron 83: ['.', '</s>', 'niedosta', ',', 'Jagieł', '.', 'obyczaje', 'wielodzie', 'sposobie', 'nieby']\n",
      "Neuron 84: ['±', 'OD', 'okre', '<unk>', 'zam', '±', 'nud', 'O', 'Rzad', '¨']\n",
      "Neuron 85: ['dni', 'dni', 'raz', 'kart', 'udu', 'teni', 'gry', 'Akademi', 'banki', 'Komis']\n",
      "Neuron 86: ['.', 'ུ', 'ད', 'ས', 'ག', ',', 'ི', 'པ', '，', '</s>']\n",
      "Neuron 87: ['Medi', 'domi', 'Interna', 'publi', 'gili', 'Marti', 'mili', 'piro', 'medi', 'Qu']\n",
      "Neuron 88: ['ÿ', '</s>', 'ÿ', '.', 'ུ', '¨', 'པ', '<unk>', 'ེ', 'Fiat']\n",
      "Neuron 89: ['bat', 'Bat', 'bat', 'bata', 'baty', 'Rat', '</s>', 'lau', 'BAT', 'boni']\n",
      "Neuron 90: ['zakra', 'koni', 'oko', 'kol', 'fla', 'Okra', 'inaugu', 'mla', 'Il', 'Kru']\n",
      "Neuron 91: ['Ro', 'Bo', 'ro', 'Bo', 'nau', 'RO', 'RO', 'ro', 'Rot', 'pistoletu']\n",
      "Neuron 92: ['ule', 'upa', 'oka', 'ona', 'lona', 'ura', 'oby', 'oli', 'oga', 'omi']\n",
      "Neuron 93: ['nau', 'nau', 'lau', 'Lea', 'lea', 'wei', 'nai', 'Lau', 'AU', 'Beau']\n",
      "Neuron 94: ['liku', 'lic', '</s>', 'lika', 'up', 'laj', 'lica', 'lice', '¨', 'lich']\n",
      "Neuron 95: ['nr', 'ÿ', 'apro', '￼', 'abso', '∞', 'ÿ', '¬', 'fir', '￼']\n",
      "Neuron 96: ['jej', 'yi', '±', 'ich', 'Tek', '¹', 'Zer', 'teni', 'Wujek', 'བ']\n",
      "Neuron 97: ['Nim', 'nim', 'pseudonim', '±', 'spis', 'bunt', 'ceni', '±', 'ceni', 'zadane']\n",
      "Neuron 98: ['wó', 'óz', 'Wó', 'WÓ', 'facto', 'Brzegu', 'ingu', 'Mróz', 'bula', 'PG']\n",
      "Neuron 99: ['zza', 'ÿ', 'jom', 'ÿ', 'trop', 'ktu', 'wadze', 'trud', '</s>', '∞']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Print top tokens for the first 5 neurons in the first layer\n",
    "print(\"Feed-forward values (Layer 3, first 5 neurons):\")\n",
    "for neuron_info in ff_values[3]['neurons'][:100]:\n",
    "    print(f\"Neuron {neuron_info['neuron']}: {neuron_info['top_tokens']}\")\n",
    "\n",
    "print(\"\\nFeed-forward keys (Layer 0, first 5 neurons):\")\n",
    "for neuron_info in ff_keys[0]['neurons'][:100]:\n",
    "    print(f\"Neuron {neuron_info['neuron']}: {neuron_info['top_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_heads(model, E, tokenizer, k=10):\n",
    "    attention_heads = []\n",
    "    n_heads = model.config.num_attention_heads  # Number of attention heads\n",
    "    head_dim = model.config.hidden_size // n_heads  # Dimension per head\n",
    "    hidden_size = model.config.hidden_size\n",
    "\n",
    "    for layer_idx, layer in enumerate(tqdm(model.encoder.layer, desc=\"Processing attention layers\")):\n",
    "        # Get the attention weights\n",
    "        # The weights are transposed compared to GPT-2\n",
    "        Wq = layer.attention.self.query.weight.data.T  # shape: (hidden_size, hidden_size)\n",
    "        Wk = layer.attention.self.key.weight.data.T    # shape: (hidden_size, hidden_size)\n",
    "        Wv = layer.attention.self.value.weight.data.T  # shape: (hidden_size, hidden_size)\n",
    "        Wo = layer.attention.output.dense.weight.data.T  # shape: (hidden_size, hidden_size)\n",
    "\n",
    "        # Reshape weights to separate heads\n",
    "        Wq = Wq.view(hidden_size, n_heads, head_dim)  # shape: (hidden_size, n_heads, head_dim)\n",
    "        Wk = Wk.view(hidden_size, n_heads, head_dim)\n",
    "        Wv = Wv.view(hidden_size, n_heads, head_dim)\n",
    "        Wo = Wo.view(hidden_size, n_heads, head_dim)\n",
    "\n",
    "        for head_idx in range(n_heads):\n",
    "            Wq_head = Wq[:, head_idx, :]  # shape: (hidden_size, head_dim)\n",
    "            Wk_head = Wk[:, head_idx, :]\n",
    "            Wv_head = Wv[:, head_idx, :]\n",
    "            Wo_head = Wo[:, head_idx, :].T  # shape: (head_dim, hidden_size)\n",
    "\n",
    "            # Project Wq_head and Wk_head into embedding space\n",
    "            Wq_proj = E @ Wq_head  # shape: (vocab_size, head_dim)\n",
    "            Wk_proj = E @ Wk_head  # shape: (vocab_size, head_dim)\n",
    "\n",
    "            # Compute top-k token pairs for W_QK\n",
    "            tokens_QK = []\n",
    "            for i in range(head_dim):\n",
    "                wq = Wq_proj[:, i]  # shape: (vocab_size,)\n",
    "                wk = Wk_proj[:, i]  # shape: (vocab_size,)\n",
    "\n",
    "                # Get top-k indices\n",
    "                topk_wq_values, topk_wq_indices = torch.topk(wq, k=k)\n",
    "                topk_wk_values, topk_wk_indices = torch.topk(wk, k=k)\n",
    "\n",
    "                # Compute contributions for top-k tokens only\n",
    "                for idx_q, val_q in zip(topk_wq_indices, topk_wq_values):\n",
    "                    for idx_k, val_k in zip(topk_wk_indices, topk_wk_values):\n",
    "                        contribution = val_q.item() * val_k.item()\n",
    "                        tokens_QK.append((\n",
    "                            tokenizer.decode([idx_q.item()], clean_up_tokenization_spaces=True, skip_special_tokens=True),\n",
    "                            tokenizer.decode([idx_k.item()], clean_up_tokenization_spaces=True, skip_special_tokens=True),\n",
    "                            contribution\n",
    "                        ))\n",
    "            # Sort and select top-k\n",
    "            tokens_QK = sorted(tokens_QK, key=lambda x: abs(x[2]), reverse=True)[:k]\n",
    "\n",
    "            # Similarly for W_VO\n",
    "            # Project Wv_head and Wo_head into embedding space\n",
    "            Wv_proj = E @ Wv_head  # shape: (vocab_size, head_dim)\n",
    "            Wo_proj = Wo_head @ E.T  # shape: (head_dim, vocab_size)\n",
    "\n",
    "            tokens_VO = []\n",
    "            for i in range(head_dim):\n",
    "                wv = Wv_proj[:, i]  # shape: (vocab_size,)\n",
    "                wo = Wo_proj[i, :]  # shape: (vocab_size,)\n",
    "\n",
    "                # Get top-k indices\n",
    "                topk_wv_values, topk_wv_indices = torch.topk(wv, k=k)\n",
    "                topk_wo_values, topk_wo_indices = torch.topk(wo, k=k)\n",
    "\n",
    "                # Compute contributions for top-k tokens only\n",
    "                for idx_v, val_v in zip(topk_wv_indices, topk_wv_values):\n",
    "                    for idx_o, val_o in zip(topk_wo_indices, topk_wo_values):\n",
    "                        contribution = val_v.item() * val_o.item()\n",
    "                        tokens_VO.append((\n",
    "                            tokenizer.decode([idx_v.item()], clean_up_tokenization_spaces=True, skip_special_tokens=True),\n",
    "                            tokenizer.decode([idx_o.item()], clean_up_tokenization_spaces=True, skip_special_tokens=True),\n",
    "                            contribution\n",
    "                        ))\n",
    "            # Sort and select top-k\n",
    "            tokens_VO = sorted(tokens_VO, key=lambda x: abs(x[2]), reverse=True)[:k]\n",
    "\n",
    "            attention_heads.append({\n",
    "                'layer': layer_idx,\n",
    "                'head': head_idx,\n",
    "                'W_QK_top_tokens': tokens_QK,\n",
    "                'W_VO_top_tokens': tokens_VO\n",
    "            })\n",
    "    return attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attention layers: 100%|██████████| 12/12 [08:16<00:00, 41.41s/it]\n"
     ]
    }
   ],
   "source": [
    "attention_data = extract_attention_heads(model, E, tokenizer, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Head (Layer 0, Head 0) W_QK top token pairs:\n",
      "('brzo', 'ু', 17.095978677458334)\n",
      "('klasi', 'ু', 16.37561626630577)\n",
      "('brzo', 'ব', 16.241737153276517)\n",
      "('brzo', '大', 15.935528486616022)\n",
      "('brz', 'ু', 15.634091714351143)\n",
      "('klasi', 'ব', 15.55736937546294)\n",
      "('brzo', 'ব', 15.391622370302684)\n",
      "('Literatura', 'ু', 15.34665668335947)\n",
      "('klasi', '大', 15.264063229190015)\n",
      "('brzo', 'て', 15.246606470053848)\n",
      "('pobud', 'ু', 15.19352441673368)\n",
      "('brzo', 'ো', 15.117897444059054)\n",
      "('Bez', 'ু', 15.053850268948736)\n",
      "('brzo', 'て', 14.971461611688937)\n",
      "('brzo', 'ু', 14.970542413856492)\n",
      "('brzo', '大', 14.932249952551501)\n",
      "('bój', 'ু', 14.887767935601005)\n",
      "('brz', 'ব', 14.852896873901614)\n",
      "('Muze', 'ু', 14.845197760598467)\n",
      "('udar', 'ু', 14.839337460733532)\n",
      "('klasi', 'ব', 14.74307533995102)\n",
      "('Nadzor', 'ু', 14.732613028336118)\n",
      "('brzo', '國', 14.72685443512546)\n",
      "('kojarzą', 'ু', 14.664083399303308)\n",
      "('zaj', 'ু', 14.659956791414743)\n",
      "('brzo', 'ե', 14.64265591367348)\n",
      "('kaz', 'ু', 14.6311489924542)\n",
      "('klasi', 'て', 14.604169882720953)\n",
      "('Literatura', 'ব', 14.579824222719253)\n",
      "('‚', 'ু', 14.577960432488226)\n",
      "('rzą', 'ু', 14.576354949521146)\n",
      "('brz', '大', 14.572872286329357)\n",
      "('zapad', 'ু', 14.561675974741092)\n",
      "('brzo', '新', 14.5247822495312)\n",
      "('odbija', 'ু', 14.520984922829939)\n",
      "('klasi', 'ো', 14.48088418732641)\n",
      "('Bez', 'ু', 14.457031590001861)\n",
      "('pobud', 'ব', 14.434343576589242)\n",
      "('brzo', '新', 14.429619373823698)\n",
      "('strzą', 'ু', 14.411296374680433)\n",
      "('brzo', 'ম', 14.409216229008052)\n",
      "('wywiad', 'ু', 14.3782107960144)\n",
      "('Ј', 'ু', 14.352749826266518)\n",
      "('klasi', 'て', 14.340618628754328)\n",
      "('klasi', 'ু', 14.339738162577987)\n",
      "('Literatura', '大', 14.304947927575995)\n",
      "('klasi', '大', 14.30305920642968)\n",
      "('Bez', 'ব', 14.301648582156304)\n",
      "('brzo', 'ো', 14.280141555256705)\n",
      "('Kraj', 'ু', 14.252813773875914)\n",
      "\n",
      "Attention Head (Layer 0, Head 0) W_VO top token pairs:\n",
      "('ช', 'Step', 0.8622499648632242)\n",
      "('ช', 'Step', 0.8412633719103724)\n",
      "('ช', 'Glas', 0.8408987080993455)\n",
      "('ช', 'Beau', 0.8248446111667675)\n",
      "('ช', 'Glas', 0.8204317905921243)\n",
      "('ช', 'reper', 0.8139221070964879)\n",
      "('ช', 'Beau', 0.8047684397439525)\n",
      "('ช', 'reper', 0.7941117821872012)\n",
      "('ช', 'xl', 0.7685234770559823)\n",
      "('อ', 'Step', 0.7646515240421081)\n",
      "('พ', 'Step', 0.7617839177956398)\n",
      "('ช', 'xl', 0.7498181247278524)\n",
      "('อ', 'Glas', 0.7457170251264671)\n",
      "('พ', 'Glas', 0.7429204272879559)\n",
      "('다', 'Step', 0.7406673359986371)\n",
      "('เ', 'Step', 0.7348086154092925)\n",
      "('อ', 'Beau', 0.7314800982643561)\n",
      "('서', 'Step', 0.7298318373469215)\n",
      "('ն', 'Step', 0.7292885762043966)\n",
      "('พ', 'Beau', 0.7287368919370323)\n",
      "('다', 'Glas', 0.7223267397539814)\n",
      "('ól', 'doz', 0.7218443493195394)\n",
      "('อ', 'reper', 0.7217939170825218)\n",
      "('พ', 'reper', 0.71908703599982)\n",
      "('usz', 'Ostatni', 0.718452952346226)\n",
      "('พ', 'Step', 0.7174434518321959)\n",
      "('เ', 'Glas', 0.716613094319996)\n",
      "('서', 'Glas', 0.7117595525239011)\n",
      "('จ', 'Step', 0.7116333991658053)\n",
      "('ն', 'Glas', 0.7112297437543731)\n",
      "('ช', 'Vic', 0.710909612961867)\n",
      "('ส', 'Step', 0.7088018732360979)\n",
      "('다', 'Beau', 0.7085363707293766)\n",
      "('ช', 'Beau', 0.7048344161892359)\n",
      "('เ', 'Beau', 0.7029318078956521)\n",
      "('พ', 'Glas', 0.6996779314171704)\n",
      "('다', 'reper', 0.6991540079322363)\n",
      "('서', 'Beau', 0.6981709279501587)\n",
      "('ն', 'Beau', 0.6976512340746837)\n",
      "('usz', 'Poi', 0.6957074816807989)\n",
      "('จ', 'Glas', 0.6940117487784363)\n",
      "('เ', 'reper', 0.6936236601197834)\n",
      "('ช', 'Vic', 0.6936065439198558)\n",
      "('usz', 'Him', 0.6935915865783002)\n",
      "('ช', 'spu', 0.6916917023244125)\n",
      "('ส', 'Glas', 0.6912503378265455)\n",
      "('ól', 'napis', 0.6894658551702406)\n",
      "('서', 'reper', 0.6889258232370423)\n",
      "('อ', 'Step', 0.6887128015307553)\n",
      "('ն', 'reper', 0.6884130110922513)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttention Head (Layer 0, Head 0) W_QK top token pairs:\")\n",
    "for token_pair in attention_data[0]['W_QK_top_tokens']:\n",
    "    print(token_pair)\n",
    "\n",
    "print(\"\\nAttention Head (Layer 0, Head 0) W_VO top token pairs:\")\n",
    "for token_pair in attention_data[0]['W_VO_top_tokens']:\n",
    "    print(token_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer</th>\n",
       "      <th>Head</th>\n",
       "      <th>Type</th>\n",
       "      <th>Token 1</th>\n",
       "      <th>Token 2</th>\n",
       "      <th>Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>brzo</td>\n",
       "      <td>ু</td>\n",
       "      <td>17.095979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>klasi</td>\n",
       "      <td>ু</td>\n",
       "      <td>16.375616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>brzo</td>\n",
       "      <td>ব</td>\n",
       "      <td>16.241737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>brzo</td>\n",
       "      <td>大</td>\n",
       "      <td>15.935528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W_QK</td>\n",
       "      <td>brz</td>\n",
       "      <td>ু</td>\n",
       "      <td>15.634092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer  Head  Type Token 1 Token 2  Contribution\n",
       "0      0     0  W_QK    brzo       ু     17.095979\n",
       "1      0     0  W_QK   klasi       ু     16.375616\n",
       "2      0     0  W_QK    brzo       ব     16.241737\n",
       "3      0     0  W_QK    brzo       大     15.935528\n",
       "4      0     0  W_QK     brz       ু     15.634092"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect data for all layers and heads\n",
    "data_rows = []\n",
    "\n",
    "for layer_data in attention_data:\n",
    "    layer = layer_data['layer']\n",
    "    head = layer_data['head']\n",
    "\n",
    "    # W_QK_top_tokens\n",
    "    for token_q, token_k, contribution in layer_data['W_QK_top_tokens']:\n",
    "        data_rows.append({\n",
    "            'Layer': layer,\n",
    "            'Head': head,\n",
    "            'Type': 'W_QK',\n",
    "            'Token 1': token_q,\n",
    "            'Token 2': token_k,\n",
    "            'Contribution': contribution\n",
    "        })\n",
    "\n",
    "    # W_VO_top_tokens\n",
    "    for token_v, token_o, contribution in layer_data['W_VO_top_tokens']:\n",
    "        data_rows.append({\n",
    "            'Layer': layer,\n",
    "            'Head': head,\n",
    "            'Type': 'W_VO',\n",
    "            'Token 1': token_v,\n",
    "            'Token 2': token_o,\n",
    "            'Contribution': contribution\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_tokens = pd.DataFrame(data_rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfee10adb42446690cd071d811a1794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, continuous_update=False, description='Layer:', max=11), IntSlider(value=0, c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9776e98fbe246bdb97ef3edf398b5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'marker': {'color': 'grey'},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'uid': 'ec261119-9715-4aa7-9563-4fe76c0a3bc2',\n",
       "              'x': array([17.09597868, 16.37561627, 16.24173715, 15.93552849, 15.63409171,\n",
       "                          15.55736938, 15.39162237, 15.34665668, 15.26406323, 15.24660647,\n",
       "                          15.19352442, 15.11789744, 15.05385027, 14.97146161, 14.97054241,\n",
       "                          14.93224995, 14.88776794, 14.85289687, 14.84519776, 14.83933746]),\n",
       "              'y': array(['brzo ➔ ু', 'klasi ➔ ু', 'brzo ➔ ব', 'brzo ➔ 大', 'brz ➔ ু', 'klasi ➔ ব',\n",
       "                          'brzo ➔ ব', 'Literatura ➔ ু', 'klasi ➔ 大', 'brzo ➔ て', 'pobud ➔ ু',\n",
       "                          'brzo ➔ ো', 'Bez ➔ ু', 'brzo ➔ て', 'brzo ➔ ু', 'brzo ➔ 大', 'bój ➔ ু',\n",
       "                          'brz ➔ ব', 'Muze ➔ ু', 'udar ➔ ু'], dtype=object)}],\n",
       "    'layout': {'height': 600,\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Layer 0, Head 0 - Top 20 W_QK_top_tokens'},\n",
       "               'width': 1200,\n",
       "               'xaxis': {'title': {'text': 'Contribution'}},\n",
       "               'yaxis': {'categoryorder': 'total ascending', 'title': {'text': 'Token Pair'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def interactive_attention_visualization(attention_data, model):\n",
    "    # Create sliders and dropdowns\n",
    "    layer_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=model.config.num_hidden_layers - 1,\n",
    "        step=1,\n",
    "        description='Layer:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    head_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=model.config.num_attention_heads - 1,\n",
    "        step=1,\n",
    "        description='Head:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    token_type_dropdown = widgets.Dropdown(\n",
    "        options=['W_QK_top_tokens', 'W_VO_top_tokens'],\n",
    "        value='W_QK_top_tokens',\n",
    "        description='Token Type:',\n",
    "    )\n",
    "    top_n_slider = widgets.IntSlider(\n",
    "        value=20,\n",
    "        min=1,\n",
    "        max=50,\n",
    "        step=1,\n",
    "        description='Top N:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Initialize an empty FigureWidget\n",
    "    fig = go.FigureWidget(layout=go.Layout(\n",
    "        title='',\n",
    "        xaxis=dict(title='Contribution'),\n",
    "        yaxis=dict(title='Token Pair', categoryorder='total ascending'),\n",
    "        height=600,\n",
    "        width=1200\n",
    "    ))\n",
    "\n",
    "    # Function to update the plot\n",
    "    def update_plot(layer, head, token_type, top_n):\n",
    "        fig.data = []\n",
    "        fig.layout.title = f'Layer {layer}, Head {head} - Top {top_n} {token_type}'\n",
    "\n",
    "        selected_data = None\n",
    "        for data in attention_data:\n",
    "            if data['layer'] == layer and data['head'] == head:\n",
    "                selected_data = data[token_type]\n",
    "                break\n",
    "\n",
    "        if not selected_data:\n",
    "            with fig.batch_update():\n",
    "                fig.add_annotation(\n",
    "                    text=\"No data available for the selected layer and head.\",\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=20)\n",
    "                )\n",
    "            return\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        df = pd.DataFrame(selected_data, columns=['Token 1', 'Token 2', 'Contribution'])\n",
    "        df['Token Pair'] = df['Token 1'] + ' ➔ ' + df['Token 2']\n",
    "        df = df.sort_values(by='Contribution', ascending=False).head(top_n)\n",
    "\n",
    "        # Update the figure\n",
    "        with fig.batch_update():\n",
    "            fig.add_bar(\n",
    "                x=df['Contribution'],\n",
    "                y=df['Token Pair'],\n",
    "                orientation='h',\n",
    "                marker_color='grey'\n",
    "            )\n",
    "\n",
    "    controls = {\n",
    "        'layer': layer_slider,\n",
    "        'head': head_slider,\n",
    "        'token_type': token_type_dropdown,\n",
    "        'top_n': top_n_slider\n",
    "    }\n",
    "    out = widgets.interactive_output(update_plot, controls)\n",
    "\n",
    "    ui = widgets.VBox([layer_slider, head_slider, token_type_dropdown, top_n_slider])\n",
    "    display(ui, fig)\n",
    "\n",
    "    update_plot(layer_slider.value, head_slider.value, token_type_dropdown.value, top_n_slider.value)\n",
    "\n",
    "interactive_attention_visualization(attention_data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
